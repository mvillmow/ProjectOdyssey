"""CLI Wrapper for LeNet-5 Inference

Provides command-line interface for running inference with a trained LeNet-5 model.

Usage:
    mojo run examples/lenet-emnist/run_infer.mojo --checkpoint lenet5_weights --image test.png
    mojo run examples/lenet-emnist/run_infer.mojo --checkpoint lenet5_weights --test-set

Arguments:
    --checkpoint DIR   Weights directory (required)
    --image FILE       Single image file to classify (PNG/IDX format)
    --test-set         Run evaluation on EMNIST test set
    --data-dir DIR     EMNIST data directory (default: datasets/emnist)
    --top-k N          Show top-k predictions (default: 5)
"""

from model import LeNet5
from shared.data import load_idx_labels, load_idx_images, normalize_images
from shared.core import ExTensor, zeros
from shared.utils.arg_parser import ArgumentParser
from shared.training.metrics import top1_accuracy, AccuracyMetric
from collections import List


# EMNIST Balanced class labels (47 classes)
# 0-9: digits, 10-35: uppercase letters, 36-46: lowercase letters (excluding ambiguous)
fn get_class_label(class_idx: Int) -> String:
    """Convert class index to human-readable label.

    Args:
        class_idx: Class index (0-46 for EMNIST Balanced)

    Returns:
        Character label
    """
    if class_idx < 10:
        # Digits 0-9
        return String(class_idx)
    elif class_idx < 36:
        # Uppercase letters A-Z (indices 10-35)
        var letter_idx = class_idx - 10
        var letters = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
        return String(letters[letter_idx])
    else:
        # Lowercase letters (indices 36-46)
        # Only includes: a, b, d, e, f, g, h, n, q, r, t
        var lowercase_map = "abdefghnqrt"
        var lower_idx = class_idx - 36
        if lower_idx < len(lowercase_map):
            return String(lowercase_map[lower_idx])
        else:
            return "?"


struct InferConfig(Movable):
    """Inference configuration."""
    var checkpoint_dir: String
    var image_path: String
    var run_test_set: Bool
    var data_dir: String
    var top_k: Int

    fn __init__(out self):
        self.checkpoint_dir = ""
        self.image_path = ""
        self.run_test_set = False
        self.data_dir = "datasets/emnist"
        self.top_k = 5

    fn __moveinit__(out self, owned other: Self):
        self.checkpoint_dir = other.checkpoint_dir^
        self.image_path = other.image_path^
        self.run_test_set = other.run_test_set
        self.data_dir = other.data_dir^
        self.top_k = other.top_k


fn parse_args() raises -> InferConfig:
    """Parse command line arguments using enhanced argument parser."""
    var parser = ArgumentParser()
    parser.add_argument("checkpoint", "string", "")
    parser.add_argument("image", "string", "")
    parser.add_flag("test-set")
    parser.add_argument("data-dir", "string", "datasets/emnist")
    parser.add_argument("top-k", "int", "5")

    var args = parser.parse()

    var config = InferConfig()
    config.checkpoint_dir = args.get_string("checkpoint", "")
    config.image_path = args.get_string("image", "")
    config.run_test_set = args.get_bool("test-set")
    config.data_dir = args.get_string("data-dir", "datasets/emnist")
    config.top_k = args.get_int("top-k", 5)

    return config^


fn get_top_k_predictions(logits: ExTensor, k: Int) raises -> List[Tuple[Int, Float32]]:
    """Get top-k predictions from logits.

    Args:
        logits: Model output logits (1, num_classes)
        k: Number of top predictions to return

    Returns:
        List of (class_idx, score) tuples sorted by score descending
    """
    var logits_shape = logits.shape()
    var num_classes = logits_shape[1] if len(logits_shape) > 1 else logits_shape[0]

    # Extract all scores
    var scores = List[Tuple[Int, Float32]]()
    for i in range(num_classes):
        var score = logits._data.bitcast[Float32]()[i]
        scores.append((i, score))

    # Simple bubble sort for top-k (sufficient for 47 classes)
    for _ in range(min(k, len(scores))):
        for j in range(len(scores) - 1):
            if scores[j][1] < scores[j + 1][1]:
                var temp = scores[j]
                scores[j] = scores[j + 1]
                scores[j + 1] = temp

    # Return top-k
    var result = List[Tuple[Int, Float32]]()
    for i in range(min(k, len(scores))):
        result.append(scores[i])

    return result^


fn evaluate_test_set(
    mut model: LeNet5,
    test_images: ExTensor,
    test_labels: ExTensor
) raises -> Float32:
    """Evaluate model on full test set.

    Args:
        model: Loaded LeNet-5 model
        test_images: Normalized test images (N, 1, 28, 28)
        test_labels: Test labels (N,)

    Returns:
        Test accuracy (0.0 to 1.0)
    """
    var num_samples = test_images.shape()[0]
    var correct = 0

    print("Evaluating on", num_samples, "test samples...")

    var eval_batch_size = 32
    var num_batches = (num_samples + eval_batch_size - 1) // eval_batch_size

    for batch_idx in range(num_batches):
        var start_idx = batch_idx * eval_batch_size
        var end_idx = min(start_idx + eval_batch_size, num_samples)

        var batch_images = test_images.slice(start_idx, end_idx, axis=0)
        var batch_labels = test_labels.slice(start_idx, end_idx, axis=0)

        var actual_batch_size = end_idx - start_idx
        for i in range(actual_batch_size):
            var sample = batch_images.slice(i, i + 1, axis=0)
            var pred_class = model.predict(sample)
            var true_label = Int(batch_labels[i])

            if pred_class == true_label:
                correct += 1

        # Progress update
        if (batch_idx + 1) % 50 == 0:
            print("  Processed", (batch_idx + 1) * eval_batch_size, "/", num_samples, "samples")

    var accuracy = Float32(correct) / Float32(num_samples)
    return accuracy


fn main() raises:
    """Main inference entry point."""
    print("=" * 60)
    print("LeNet-5 Inference on EMNIST")
    print("=" * 60)

    # Parse arguments
    var config = parse_args()

    # Validate arguments
    if len(config.checkpoint_dir) == 0:
        print("ERROR: --checkpoint is required")
        print("\nUsage:")
        print("  mojo run run_infer.mojo --checkpoint <weights_dir> --test-set")
        print("  mojo run run_infer.mojo --checkpoint <weights_dir> --image <image_path>")
        return

    if not config.run_test_set and len(config.image_path) == 0:
        print("ERROR: Either --test-set or --image is required")
        print("\nUsage:")
        print("  mojo run run_infer.mojo --checkpoint <weights_dir> --test-set")
        print("  mojo run run_infer.mojo --checkpoint <weights_dir> --image <image_path>")
        return

    print("\nConfiguration:")
    print("  Checkpoint: ", config.checkpoint_dir)
    if config.run_test_set:
        print("  Mode: Test set evaluation")
        print("  Data Directory: ", config.data_dir)
    else:
        print("  Mode: Single image inference")
        print("  Image: ", config.image_path)
    print()

    # Initialize and load model
    print("Loading model from", config.checkpoint_dir, "...")
    var model = LeNet5(num_classes=47)
    model.load_weights(config.checkpoint_dir)
    print("  Model loaded successfully")
    print()

    if config.run_test_set:
        # Run evaluation on test set
        print("Loading EMNIST test set...")
        var test_images_path = config.data_dir + "/emnist-balanced-test-images-idx3-ubyte"
        var test_labels_path = config.data_dir + "/emnist-balanced-test-labels-idx1-ubyte"

        var test_images_raw = load_idx_images(test_images_path)
        var test_labels = load_idx_labels(test_labels_path)
        var test_images = normalize_images(test_images_raw)

        print("  Test samples: ", test_images.shape()[0])
        print()

        var accuracy = evaluate_test_set(model, test_images, test_labels)

        print()
        print("=" * 60)
        print("Test Set Results")
        print("=" * 60)
        print("  Accuracy: ", accuracy * 100.0, "%")
        print("  Correct: ", Int(accuracy * Float32(test_images.shape()[0])), "/", test_images.shape()[0])

    else:
        # Single image inference
        print("Single image inference not yet implemented.")
        print("Use --test-set for now to evaluate on the EMNIST test set.")
        print()
        print("TODO: Implement PNG/image loading for single-image inference")

    print()
    print("Inference complete!")
