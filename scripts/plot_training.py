#!/usr/bin/env python3
"""
Plot training metrics from CSV logs.

This script reads CSV files generated by CSVMetricsLogger and produces
visualization plots for training analysis.

ADR-001 Justification: Python required for:
- matplotlib visualization (not available in Mojo)
- pandas DataFrame operations
- Flexible plotting configuration

Usage:
    python scripts/plot_training.py <log_dir> [--output OUTPUT]
    python scripts/plot_training.py logs/lenet5_run1 --output training.png

Examples:
    # Basic usage
    python scripts/plot_training.py logs/lenet5_run1

    # Custom output path
    python scripts/plot_training.py logs/lenet5_run1 --output results/plot.png

    # Compare multiple runs
    python scripts/plot_training.py logs/run1 logs/run2 --compare
"""

import argparse
import sys
from pathlib import Path
from typing import Dict, List, Optional

try:
    import matplotlib.pyplot as plt
    import pandas as pd
except ImportError as e:
    print(f"Error: Required dependencies not installed: {e}")
    print("Install with: pip install matplotlib pandas")
    sys.exit(1)


def load_metrics(log_dir: Path) -> Dict[str, pd.DataFrame]:
    """Load all CSV metrics from a log directory.

    Args:
        log_dir: Path to directory containing CSV files.

    Returns:
        Dictionary mapping metric names to DataFrames.
    """
    metrics = {}
    for csv_file in log_dir.glob("*.csv"):
        name = csv_file.stem
        try:
            df = pd.read_csv(csv_file)
            metrics[name] = df
        except Exception as e:
            print(f"Warning: Could not load {csv_file}: {e}")
    return metrics


def plot_training_run(
    log_dir: str,
    output_path: Optional[str] = None,
    show_plot: bool = True,
) -> bool:
    """Plot all metrics from a training run.

    Creates a multi-panel plot with:
    - Loss curves (train/val)
    - Accuracy curves (train/val)
    - Learning rate schedule
    - Any additional metrics

    Args:
        log_dir: Directory containing CSV log files.
        output_path: Optional path to save PNG output.
        show_plot: Whether to display the plot interactively.

    Returns:
        True if plotting succeeded, False otherwise.
    """
    log_path = Path(log_dir)

    if not log_path.exists():
        print(f"Error: Log directory not found: {log_dir}")
        return False

    metrics = load_metrics(log_path)

    if not metrics:
        print(f"Error: No CSV files found in {log_dir}")
        return False

    print(f"Found {len(metrics)} metrics: {list(metrics.keys())}")

    # Determine layout based on available metrics
    # Group metrics by type
    loss_metrics = [k for k in metrics if "loss" in k.lower()]
    acc_metrics = [k for k in metrics if "acc" in k.lower()]
    lr_metrics = [k for k in metrics if "lr" in k.lower() or "learning_rate" in k.lower()]
    other_metrics = [k for k in metrics if k not in loss_metrics + acc_metrics + lr_metrics]

    # Calculate grid size
    num_panels = sum(
        [
            1 if loss_metrics else 0,
            1 if acc_metrics else 0,
            1 if lr_metrics else 0,
            len(other_metrics),
        ]
    )

    if num_panels == 0:
        print("Error: No plottable metrics found")
        return False

    # Create figure with appropriate layout
    ncols = min(2, num_panels)
    nrows = (num_panels + ncols - 1) // ncols
    fig, axes = plt.subplots(nrows, ncols, figsize=(6 * ncols, 4 * nrows))

    # Handle single subplot case
    if num_panels == 1:
        axes = [[axes]]
    elif nrows == 1:
        axes = [axes]

    # Flatten axes for easier indexing
    axes_flat = [ax for row in axes for ax in (row if isinstance(row, (list, tuple)) else [row])]
    ax_idx = 0

    # Plot loss metrics
    if loss_metrics:
        ax = axes_flat[ax_idx]
        for metric_name in loss_metrics:
            df = metrics[metric_name]
            label = metric_name.replace("_", " ").title()
            ax.plot(df["step"], df["value"], label=label)
        ax.set_xlabel("Step")
        ax.set_ylabel("Loss")
        ax.set_title("Training Loss")
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax_idx += 1

    # Plot accuracy metrics
    if acc_metrics:
        ax = axes_flat[ax_idx]
        for metric_name in acc_metrics:
            df = metrics[metric_name]
            label = metric_name.replace("_", " ").title()
            ax.plot(df["step"], df["value"], label=label)
        ax.set_xlabel("Step")
        ax.set_ylabel("Accuracy")
        ax.set_title("Training Accuracy")
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax_idx += 1

    # Plot learning rate
    if lr_metrics:
        ax = axes_flat[ax_idx]
        for metric_name in lr_metrics:
            df = metrics[metric_name]
            ax.plot(df["step"], df["value"])
        ax.set_xlabel("Step")
        ax.set_ylabel("Learning Rate")
        ax.set_title("Learning Rate Schedule")
        ax.grid(True, alpha=0.3)
        ax_idx += 1

    # Plot other metrics
    for metric_name in other_metrics:
        if ax_idx >= len(axes_flat):
            break
        ax = axes_flat[ax_idx]
        df = metrics[metric_name]
        ax.plot(df["step"], df["value"])
        ax.set_xlabel("Step")
        ax.set_ylabel(metric_name.replace("_", " ").title())
        ax.set_title(metric_name.replace("_", " ").title())
        ax.grid(True, alpha=0.3)
        ax_idx += 1

    # Hide unused axes
    for i in range(ax_idx, len(axes_flat)):
        axes_flat[i].set_visible(False)

    plt.tight_layout()

    # Save if output path specified
    if output_path:
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_file, dpi=150, bbox_inches="tight")
        print(f"Plot saved to {output_file}")
    elif show_plot:
        # Default: save to log directory
        default_output = log_path / "training_plot.png"
        plt.savefig(default_output, dpi=150, bbox_inches="tight")
        print(f"Plot saved to {default_output}")

    if show_plot:
        plt.show()

    return True


def compare_runs(
    log_dirs: List[str],
    output_path: Optional[str] = None,
    show_plot: bool = True,
) -> bool:
    """Compare metrics across multiple training runs.

    Args:
        log_dirs: List of log directory paths.
        output_path: Optional path to save PNG output.
        show_plot: Whether to display the plot.

    Returns:
        True if comparison succeeded, False otherwise.
    """
    all_metrics = {}

    for log_dir in log_dirs:
        log_path = Path(log_dir)
        run_name = log_path.name
        metrics = load_metrics(log_path)

        if metrics:
            all_metrics[run_name] = metrics

    if not all_metrics:
        print("Error: No metrics found in any run directory")
        return False

    # Find common metrics across runs
    common_metrics = set.intersection(*[set(m.keys()) for m in all_metrics.values()])

    if not common_metrics:
        print("Warning: No common metrics found across runs")
        return False

    print(f"Comparing {len(all_metrics)} runs on metrics: {list(common_metrics)}")

    # Create comparison plot
    ncols = min(2, len(common_metrics))
    nrows = (len(common_metrics) + ncols - 1) // ncols
    fig, axes = plt.subplots(nrows, ncols, figsize=(6 * ncols, 4 * nrows))

    if len(common_metrics) == 1:
        axes = [[axes]]
    elif nrows == 1:
        axes = [axes]

    axes_flat = [ax for row in axes for ax in (row if isinstance(row, (list, tuple)) else [row])]

    for idx, metric_name in enumerate(sorted(common_metrics)):
        if idx >= len(axes_flat):
            break
        ax = axes_flat[idx]

        for run_name, metrics in all_metrics.items():
            if metric_name in metrics:
                df = metrics[metric_name]
                ax.plot(df["step"], df["value"], label=run_name)

        ax.set_xlabel("Step")
        ax.set_ylabel(metric_name.replace("_", " ").title())
        ax.set_title(metric_name.replace("_", " ").title())
        ax.legend()
        ax.grid(True, alpha=0.3)

    plt.tight_layout()

    if output_path:
        plt.savefig(output_path, dpi=150, bbox_inches="tight")
        print(f"Comparison plot saved to {output_path}")

    if show_plot:
        plt.show()

    return True


def main() -> int:
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Plot training metrics from CSV logs",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )

    parser.add_argument(
        "log_dirs",
        nargs="+",
        help="Log directory or directories containing CSV files",
    )
    parser.add_argument(
        "--output",
        "-o",
        type=str,
        default=None,
        help="Output path for PNG file (default: <log_dir>/training_plot.png)",
    )
    parser.add_argument(
        "--compare",
        "-c",
        action="store_true",
        help="Compare multiple training runs",
    )
    parser.add_argument(
        "--no-show",
        action="store_true",
        help="Don't display plot interactively",
    )

    args = parser.parse_args()

    if args.compare or len(args.log_dirs) > 1:
        success = compare_runs(
            args.log_dirs,
            output_path=args.output,
            show_plot=not args.no_show,
        )
    else:
        success = plot_training_run(
            args.log_dirs[0],
            output_path=args.output,
            show_plot=not args.no_show,
        )

    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
