#!/usr/bin/env python3
"""
Generate test boilerplate for Mojo modules.

Usage:
    python scripts/generators/generate_tests.py \\
        --module shared.core.tensor \\
        --output tests/shared/core/test_tensor.mojo

    python scripts/generators/generate_tests.py \\
        --module shared.nn.linear \\
        --test-type layer \\
        --output tests/shared/nn/test_linear.mojo
"""

import argparse
from datetime import datetime
from pathlib import Path


TEST_TEMPLATES = {
    "unit": '''# test_{module_name}.mojo
"""
Unit tests for {module_path}.

Auto-generated by scripts/generators/generate_tests.py
Generated: {timestamp}
"""

from testing import assert_equal, assert_true, assert_false
from {module_path} import *


# =============================================================================
# Test Fixtures
# =============================================================================
fn create_test_data() -> ExTensor:
    """Create test data with reproducible values."""
    # Use FP-representable special values for reproducibility
    return ExTensor.from_list([0.0, 0.5, 1.0, 1.5, -0.5, -1.0])


# =============================================================================
# Basic Functionality Tests
# =============================================================================
fn test_initialization():
    """Test basic initialization."""
    # TODO: Test object creation
    pass


fn test_basic_operation():
    """Test basic operations."""
    # TODO: Test core functionality
    pass


# =============================================================================
# Edge Case Tests
# =============================================================================
fn test_empty_input():
    """Test behavior with empty inputs."""
    # TODO: Test empty case handling
    pass


fn test_single_element():
    """Test behavior with single element."""
    # TODO: Test single element case
    pass


fn test_large_input():
    """Test behavior with large inputs."""
    # TODO: Test scalability
    pass


# =============================================================================
# Error Handling Tests
# =============================================================================
fn test_invalid_input():
    """Test error handling for invalid inputs."""
    # TODO: Test error cases
    pass


# =============================================================================
# Integration Tests
# =============================================================================
fn test_integration():
    """Test integration with other components."""
    # TODO: Test component interactions
    pass


# =============================================================================
# Test Runner
# =============================================================================
fn main():
    """Run all tests."""
    print("Running tests for {module_path}")
    print("=" * 60)

    test_initialization()
    print("  test_initialization ... PASSED")

    test_basic_operation()
    print("  test_basic_operation ... PASSED")

    test_empty_input()
    print("  test_empty_input ... PASSED")

    test_single_element()
    print("  test_single_element ... PASSED")

    test_large_input()
    print("  test_large_input ... PASSED")

    test_invalid_input()
    print("  test_invalid_input ... PASSED")

    test_integration()
    print("  test_integration ... PASSED")

    print()
    print("All tests passed!")
''',
    "layer": '''# test_{module_name}.mojo
"""
Layer tests for {module_path}.

Tests forward pass, backward pass, and gradient computation.

Auto-generated by scripts/generators/generate_tests.py
Generated: {timestamp}
"""

from testing import assert_equal, assert_true, assert_almost_equal
from {module_path} import *
from shared.core import ExTensor
from shared.testing import gradient_check, LayerTester


# =============================================================================
# Test Fixtures
# =============================================================================
fn create_layer() -> {layer_type}:
    """Create layer instance for testing."""
    # TODO: Configure layer parameters
    return {layer_type}()


fn create_input() -> ExTensor:
    """Create test input tensor."""
    # Use FP-representable values for reproducibility
    return ExTensor.randn([2, 16], seed=42)


# =============================================================================
# Forward Pass Tests
# =============================================================================
fn test_forward_shape():
    """Test output shape from forward pass."""
    var layer = create_layer()
    var input = create_input()

    var output = layer.forward(input)

    # TODO: Verify expected output shape
    # assert_equal(output.shape()[0], expected_batch)
    # assert_equal(output.shape()[1], expected_features)


fn test_forward_values():
    """Test output values from forward pass."""
    var layer = create_layer()
    var input = create_input()

    var output = layer.forward(input)

    # TODO: Verify expected output values
    # With known inputs and weights, verify outputs
    pass


fn test_forward_deterministic():
    """Test that forward pass is deterministic."""
    var layer = create_layer()
    var input = create_input()

    var output1 = layer.forward(input)
    var output2 = layer.forward(input)

    # Outputs should be identical
    assert_true(output1.allclose(output2))


# =============================================================================
# Backward Pass Tests
# =============================================================================
fn test_backward_shape():
    """Test gradient shapes from backward pass."""
    var layer = create_layer()
    var input = create_input()

    # Forward
    var output = layer.forward(input)

    # Create upstream gradient
    var grad_output = ExTensor.ones_like(output)

    # Backward
    var grad_input = output.backward(grad_output)

    # Gradient should match input shape
    assert_equal(grad_input.shape(), input.shape())


fn test_backward_gradient_check():
    """Verify gradients using numerical differentiation."""
    var layer = create_layer()
    var input = create_input()

    # Use gradient checker utility
    var passed = gradient_check(
        layer,
        input,
        epsilon=1e-5,
        tolerance=1e-3
    )

    assert_true(passed, "Gradient check failed")


# =============================================================================
# Parameter Tests
# =============================================================================
fn test_parameters():
    """Test parameter collection."""
    var layer = create_layer()
    var params = layer.parameters()

    # TODO: Verify parameter count and shapes
    # assert_true(len(params) > 0, "Layer should have parameters")
    pass


fn test_parameter_gradients():
    """Test that parameters receive gradients."""
    var layer = create_layer()
    var input = create_input()

    # Forward + backward
    var output = layer.forward(input)
    var grad_output = ExTensor.ones_like(output)
    _ = output.backward(grad_output)

    # Check parameter gradients
    for param in layer.parameters():
        assert_true(param[].grad is not None, "Parameter should have gradient")


# =============================================================================
# Edge Cases
# =============================================================================
fn test_batch_size_one():
    """Test with batch size of 1."""
    var layer = create_layer()
    var input = ExTensor.randn([1, 16], seed=42)

    var output = layer.forward(input)

    assert_equal(output.shape()[0], 1)


fn test_large_batch():
    """Test with large batch size."""
    var layer = create_layer()
    var input = ExTensor.randn([256, 16], seed=42)

    var output = layer.forward(input)

    assert_equal(output.shape()[0], 256)


# =============================================================================
# DType Tests
# =============================================================================
fn test_dtypes():
    """Test layer with different dtypes."""
    # Test with float32 (default)
    var layer_f32 = create_layer()
    var input_f32 = ExTensor.randn([2, 16], dtype=DType.float32, seed=42)
    var output_f32 = layer_f32.forward(input_f32)
    assert_equal(output_f32.dtype(), DType.float32)

    # TODO: Test with other dtypes as supported
    # var layer_f16 = create_layer()
    # var input_f16 = input_f32.to(DType.float16)
    # var output_f16 = layer_f16.forward(input_f16)


# =============================================================================
# Test Runner
# =============================================================================
fn main():
    """Run all layer tests."""
    print("Running layer tests for {module_path}")
    print("=" * 60)

    test_forward_shape()
    print("  test_forward_shape ... PASSED")

    test_forward_values()
    print("  test_forward_values ... PASSED")

    test_forward_deterministic()
    print("  test_forward_deterministic ... PASSED")

    test_backward_shape()
    print("  test_backward_shape ... PASSED")

    test_backward_gradient_check()
    print("  test_backward_gradient_check ... PASSED")

    test_parameters()
    print("  test_parameters ... PASSED")

    test_parameter_gradients()
    print("  test_parameter_gradients ... PASSED")

    test_batch_size_one()
    print("  test_batch_size_one ... PASSED")

    test_large_batch()
    print("  test_large_batch ... PASSED")

    test_dtypes()
    print("  test_dtypes ... PASSED")

    print()
    print("All tests passed!")
''',
    "model": '''# test_{module_name}.mojo
"""
Model tests for {module_path}.

Tests model forward pass, backward pass, and end-to-end training.

Auto-generated by scripts/generators/generate_tests.py
Generated: {timestamp}
"""

from testing import assert_equal, assert_true, assert_almost_equal
from {module_path} import *
from shared.core import ExTensor
from shared.training import SGD, CrossEntropyLoss


# =============================================================================
# Test Fixtures
# =============================================================================
alias BATCH_SIZE = 4
alias NUM_CLASSES = 10


fn create_model() -> {model_type}:
    """Create model instance for testing."""
    return {model_type}(num_classes=NUM_CLASSES)


fn create_input() -> ExTensor:
    """Create test input tensor."""
    # TODO: Adjust shape for model input requirements
    return ExTensor.randn([BATCH_SIZE, 3, 32, 32], seed=42)


fn create_target() -> ExTensor:
    """Create test target tensor."""
    return ExTensor.randint(0, NUM_CLASSES, [BATCH_SIZE], seed=42)


# =============================================================================
# Forward Pass Tests
# =============================================================================
fn test_forward_output_shape():
    """Test model output shape."""
    var model = create_model()
    var input = create_input()

    var output = model.forward(input)

    assert_equal(output.shape()[0], BATCH_SIZE)
    assert_equal(output.shape()[1], NUM_CLASSES)


fn test_forward_output_range():
    """Test that logits are in reasonable range."""
    var model = create_model()
    var input = create_input()

    var output = model.forward(input)

    # Logits should be finite
    assert_true(output.isfinite().all())


# =============================================================================
# Backward Pass Tests
# =============================================================================
fn test_backward_gradients():
    """Test that backward pass computes gradients."""
    var model = create_model()
    var input = create_input()
    var target = create_target()

    var criterion = CrossEntropyLoss()

    # Forward
    var output = model.forward(input)
    var loss = criterion.forward(output, target)

    # Backward
    loss.backward()

    # Check all parameters have gradients
    for param in model.parameters():
        assert_true(
            param[].grad is not None,
            "All parameters should have gradients"
        )


fn test_gradient_flow():
    """Test that gradients flow to all layers."""
    var model = create_model()
    var input = create_input()
    var target = create_target()

    var criterion = CrossEntropyLoss()

    var output = model.forward(input)
    var loss = criterion.forward(output, target)
    loss.backward()

    # Check gradient magnitudes are non-zero
    for param in model.parameters():
        var grad = param[].grad.value()
        assert_true(
            grad.abs().sum() > 0,
            "Gradients should be non-zero"
        )


# =============================================================================
# Training Tests
# =============================================================================
fn test_single_training_step():
    """Test a single training iteration."""
    var model = create_model()
    var input = create_input()
    var target = create_target()

    var optimizer = SGD(model.parameters(), lr=0.01)
    var criterion = CrossEntropyLoss()

    # Get initial loss
    var output1 = model.forward(input)
    var loss1 = criterion.forward(output1, target)
    var loss1_val = loss1.item()

    # Training step
    loss1.backward()
    optimizer.step()
    optimizer.zero_grad()

    # Get loss after step
    var output2 = model.forward(input)
    var loss2 = criterion.forward(output2, target)
    var loss2_val = loss2.item()

    # Loss should change (usually decrease, but not guaranteed for single step)
    assert_true(
        abs(loss2_val - loss1_val) > 1e-6,
        "Loss should change after training step"
    )


fn test_overfitting_small_batch():
    """Test that model can overfit a small batch."""
    var model = create_model()
    var input = create_input()
    var target = create_target()

    var optimizer = SGD(model.parameters(), lr=0.1)
    var criterion = CrossEntropyLoss()

    var initial_loss: Float64 = 0.0

    # Train for multiple steps
    for step in range(100):
        var output = model.forward(input)
        var loss = criterion.forward(output, target)

        if step == 0:
            initial_loss = loss.item()

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    # Final loss
    var final_output = model.forward(input)
    var final_loss = criterion.forward(final_output, target)

    # Should achieve significant loss reduction
    assert_true(
        final_loss.item() < initial_loss * 0.5,
        "Model should be able to overfit small batch"
    )


# =============================================================================
# Parameter Tests
# =============================================================================
fn test_parameter_count():
    """Test model has expected number of parameters."""
    var model = create_model()
    var params = model.parameters()

    var total_params = 0
    for p in params:
        total_params += p[].numel()

    # TODO: Update with expected parameter count
    assert_true(total_params > 0, "Model should have parameters")
    print("  Total parameters:", total_params)


fn test_parameter_initialization():
    """Test parameters are properly initialized."""
    var model = create_model()

    for param in model.parameters():
        # Parameters should be finite
        assert_true(param[].isfinite().all())

        # Parameters should not be all zeros
        assert_true(param[].abs().sum() > 0)


# =============================================================================
# Test Runner
# =============================================================================
fn main():
    """Run all model tests."""
    print("Running model tests for {module_path}")
    print("=" * 60)

    test_forward_output_shape()
    print("  test_forward_output_shape ... PASSED")

    test_forward_output_range()
    print("  test_forward_output_range ... PASSED")

    test_backward_gradients()
    print("  test_backward_gradients ... PASSED")

    test_gradient_flow()
    print("  test_gradient_flow ... PASSED")

    test_single_training_step()
    print("  test_single_training_step ... PASSED")

    test_overfitting_small_batch()
    print("  test_overfitting_small_batch ... PASSED")

    test_parameter_count()
    print("  test_parameter_count ... PASSED")

    test_parameter_initialization()
    print("  test_parameter_initialization ... PASSED")

    print()
    print("All tests passed!")
''',
}


def extract_type_name(module_path: str) -> str:
    """Extract likely type name from module path.

    Args:
        module_path: Module path like "shared.nn.linear"

    Returns:
        Type name like "Linear"
    """
    # Get last component and convert to PascalCase
    last = module_path.split(".")[-1]
    return "".join(word.title() for word in last.split("_"))


def generate_test_code(
    module_path: str,
    test_type: str = "unit",
) -> str:
    """Generate test code.

    Args:
        module_path: Module path to test
        test_type: Type of tests (unit, layer, model)

    Returns:
        Generated test code
    """
    module_name = module_path.split(".")[-1]
    type_name = extract_type_name(module_path)

    template = TEST_TEMPLATES.get(test_type, TEST_TEMPLATES["unit"])

    return template.format(
        module_path=module_path,
        module_name=module_name,
        layer_type=type_name,
        model_type=type_name,
        timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    )


def main():
    """CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Generate test boilerplate for Mojo modules",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Generate unit tests
    python scripts/generators/generate_tests.py \\
        --module shared.core.tensor \\
        --output tests/shared/core/test_tensor.mojo

    # Generate layer tests
    python scripts/generators/generate_tests.py \\
        --module shared.nn.linear \\
        --test-type layer \\
        --output tests/shared/nn/test_linear.mojo

    # Generate model tests
    python scripts/generators/generate_tests.py \\
        --module models.lenet5 \\
        --test-type model \\
        --output tests/models/test_lenet5.mojo
        """,
    )

    parser.add_argument("--module", required=True, help="Module path to test")
    parser.add_argument(
        "--test-type",
        choices=["unit", "layer", "model"],
        default="unit",
        help="Type of tests to generate (default: unit)",
    )
    parser.add_argument("--output", "-o", required=True, help="Output file path")
    parser.add_argument("--force", "-f", action="store_true", help="Overwrite existing")

    args = parser.parse_args()

    # Generate code
    code = generate_test_code(
        module_path=args.module,
        test_type=args.test_type,
    )

    # Write output
    output_path = Path(args.output)
    if output_path.exists() and not args.force:
        print(f"Error: {output_path} already exists. Use --force to overwrite.")
        return 1

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(code)
    print(f"Generated: {output_path}")
    return 0


if __name__ == "__main__":
    exit(main())
