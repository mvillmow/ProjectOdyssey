#!/usr/bin/env python3
"""
Generate boilerplate code for custom neural network layers.

Usage:
    python scripts/generators/generate_layer.py \\
        --name AttentionLayer \\
        --inputs "query:ExTensor,key:ExTensor,value:ExTensor" \\
        --output shared/nn/attention.mojo

    python scripts/generators/generate_layer.py \\
        --name SEBlock \\
        --inputs "x:ExTensor" \\
        --params "reduction:Int=16" \\
        --output shared/nn/se_block.mojo
"""

import argparse
from datetime import datetime
from pathlib import Path

from scripts.generators.templates import to_snake_case


def parse_inputs(inputs_str: str) -> list[tuple[str, str]]:
    """Parse input specification string.

    Args:
        inputs_str: Comma-separated input specs like "query:ExTensor,key:ExTensor"

    Returns:
        List of (name, type) tuples
    """
    if not inputs_str:
        return [("input", "ExTensor")]

    inputs = []
    for spec in inputs_str.split(","):
        spec = spec.strip()
        if ":" in spec:
            name, type_name = spec.split(":", 1)
            inputs.append((name.strip(), type_name.strip()))
        else:
            inputs.append((spec, "ExTensor"))
    return inputs


def parse_params(params_str: str) -> list[tuple[str, str, str | None]]:
    """Parse parameter specification string.

    Args:
        params_str: Comma-separated param specs like "hidden_dim:Int=256,dropout:Float=0.1"

    Returns:
        List of (name, type, default) tuples
    """
    if not params_str:
        return []

    params = []
    for spec in params_str.split(","):
        spec = spec.strip()
        default = None

        if "=" in spec:
            spec, default = spec.rsplit("=", 1)
            default = default.strip()

        if ":" in spec:
            name, type_name = spec.split(":", 1)
            params.append((name.strip(), type_name.strip(), default))
        else:
            params.append((spec, "Int", default))

    return params


def generate_layer_code(
    name: str,
    inputs: list[tuple[str, str]],
    params: list[tuple[str, str, str | None]],
    has_parameters: bool = True,
) -> str:
    """Generate complete layer code.

    Args:
        name: Layer name (PascalCase)
        inputs: List of (name, type) tuples for forward() inputs
        params: List of (name, type, default) tuples for __init__ parameters
        has_parameters: Whether layer has trainable parameters

    Returns:
        Generated Mojo code
    """
    snake_name = to_snake_case(name)

    # Generate input arguments for forward
    forward_args = ", ".join(f"{n}: {t}" for n, t in inputs)

    # Generate docstring for forward inputs
    forward_docs = "\n".join(f"            {n}: {t} tensor" for n, t in inputs)

    # Generate __init__ signature
    init_params = []
    init_docs = []
    member_vars = []
    member_inits = []

    for param_name, param_type, default in params:
        if default:
            init_params.append(f"{param_name}: {param_type} = {default}")
        else:
            init_params.append(f"{param_name}: {param_type}")
        init_docs.append(f"            {param_name}: {param_type} parameter")
        member_vars.append(f"    var {param_name}: {param_type}")
        member_inits.append(f"        self.{param_name} = {param_name}")

    init_signature = ", ".join(init_params)
    if init_signature:
        init_signature = ", " + init_signature

    member_vars_str = "\n".join(member_vars) if member_vars else "    # No configuration parameters"
    member_inits_str = "\n".join(member_inits) if member_inits else "        pass"
    init_docs_str = "\n" + "\n".join(init_docs) if init_docs else ""

    # Generate parameters method
    if has_parameters:
        params_method = '''    fn parameters(self) -> List[ExTensor]:
        """Get trainable parameters.

        Returns:
            List of parameter tensors
        """
        var params = List[ExTensor]()
        # TODO: Collect trainable parameters
        # params.append(self.weight)
        # params.append(self.bias)
        return params'''
    else:
        params_method = '''    fn parameters(self) -> List[ExTensor]:
        """Get trainable parameters (empty - no trainable params).

        Returns:
            Empty list
        """
        return List[ExTensor]()'''

    code = f'''# {snake_name}.mojo
"""
{name} layer implementation.

Auto-generated by scripts/generators/generate_layer.py
Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""

from shared.nn import Module
from shared.core import ExTensor


struct {name}(Module):
    """{name} layer.

    A custom neural network layer.
    """

{member_vars_str}
    # TODO: Add trainable parameters as needed
    # var weight: ExTensor
    # var bias: ExTensor

    fn __init__(out self{init_signature}):
        """Initialize {name}.

        Args:{init_docs_str}
        """
{member_inits_str}
        # TODO: Initialize trainable parameters
        # self.weight = ExTensor.randn([out_features, in_features])
        # self.bias = ExTensor.zeros([out_features])

    fn forward(self, {forward_args}) -> ExTensor:
        """Forward pass through the layer.

        Args:
{forward_docs}

        Returns:
            Output tensor
        """
        # TODO: Implement forward computation
        {"var x = " + inputs[0][0] if inputs else "var x = input"}

        # Example computation:
        # x = matmul(x, self.weight.T())
        # x = x + self.bias

        return x

{params_method}

    fn __str__(self) -> String:
        """String representation."""
        return "{name}()"
'''
    return code


def main():
    """CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Generate boilerplate code for custom layers",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Generate attention layer
    python scripts/generators/generate_layer.py \\
        --name AttentionLayer \\
        --inputs "query:ExTensor,key:ExTensor,value:ExTensor" \\
        --params "num_heads:Int=8,hidden_dim:Int=512" \\
        --output shared/nn/attention.mojo

    # Generate squeeze-and-excitation block
    python scripts/generators/generate_layer.py \\
        --name SEBlock \\
        --inputs "x:ExTensor" \\
        --params "channels:Int,reduction:Int=16" \\
        --output shared/nn/se_block.mojo

    # Generate layer without trainable parameters
    python scripts/generators/generate_layer.py \\
        --name Reshape \\
        --inputs "x:ExTensor" \\
        --params "shape:List[Int]" \\
        --no-parameters \\
        --output shared/nn/reshape.mojo
        """,
    )

    parser.add_argument("--name", required=True, help="Layer name (PascalCase)")
    parser.add_argument(
        "--inputs",
        default="input:ExTensor",
        help="Forward input specs: name:Type,name2:Type2",
    )
    parser.add_argument(
        "--params",
        help="Init parameter specs: name:Type=default,name2:Type2",
    )
    parser.add_argument(
        "--no-parameters",
        action="store_true",
        help="Layer has no trainable parameters",
    )
    parser.add_argument("--output", "-o", required=True, help="Output file path")
    parser.add_argument("--force", "-f", action="store_true", help="Overwrite existing")

    args = parser.parse_args()

    # Parse specifications
    inputs = parse_inputs(args.inputs)
    params = parse_params(args.params) if args.params else []

    # Generate code
    code = generate_layer_code(
        name=args.name,
        inputs=inputs,
        params=params,
        has_parameters=not args.no_parameters,
    )

    # Write output
    output_path = Path(args.output)
    if output_path.exists() and not args.force:
        print(f"Error: {output_path} already exists. Use --force to overwrite.")
        return 1

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(code)
    print(f"Generated: {output_path}")
    return 0


if __name__ == "__main__":
    exit(main())
