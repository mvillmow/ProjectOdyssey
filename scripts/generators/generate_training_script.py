#!/usr/bin/env python3
"""
Generate complete training scripts.

Usage:
    python scripts/generators/generate_training_script.py \\
        --model LeNet5 \\
        --dataset MNIST \\
        --output examples/lenet5/train.mojo

    python scripts/generators/generate_training_script.py \\
        --model ResNet18 \\
        --dataset CIFAR10 \\
        --optimizer adam \\
        --epochs 100 \\
        --output examples/resnet/train.mojo
"""

import argparse
from datetime import datetime
from pathlib import Path

from scripts.generators.templates import to_snake_case


TRAINING_TEMPLATE = '''# train_{model_snake}.mojo
"""
Training script for {model} on {dataset}.

Auto-generated by scripts/generators/generate_training_script.py
Generated: {timestamp}

Usage:
    mojo run examples/{model_snake}/train.mojo
"""

from models.{model_snake} import {model}
from shared.datasets.{dataset_snake} import {dataset}Dataset
from shared.training import {optimizer_class}, {loss_class}
from shared.training.early_stopping import EarlyStopping
from shared.training.checkpoint import CheckpointManager
from shared.training.metrics import Accuracy
from shared.core import ExTensor


# =============================================================================
# Hyperparameters
# =============================================================================
alias NUM_EPOCHS = {epochs}
alias BATCH_SIZE = {batch_size}
alias LEARNING_RATE = {learning_rate}
alias WEIGHT_DECAY = {weight_decay}
alias NUM_CLASSES = {num_classes}
alias PATIENCE = {patience}


# =============================================================================
# Training Loop
# =============================================================================
fn train_epoch(
    model: {model},
    dataset: {dataset}Dataset,
    optimizer: {optimizer_class},
    criterion: {loss_class},
    batch_size: Int
) -> Float64:
    """Train for one epoch.

    Args:
        model: Model to train
        dataset: Training dataset
        optimizer: Optimizer instance
        criterion: Loss function
        batch_size: Batch size

    Returns:
        Average training loss
    """
    var total_loss: Float64 = 0.0
    var num_batches = len(dataset) // batch_size

    for batch_idx in range(num_batches):
        # Get batch indices
        var start = batch_idx * batch_size
        var end = min(start + batch_size, len(dataset))
        var indices = List[Int]()
        for i in range(start, end):
            indices.append(i)

        # Get batch data
        var batch = dataset.get_batch(indices)
        var inputs = batch[0]
        var targets = batch[1]

        # Zero gradients
        optimizer.zero_grad()

        # Forward pass
        var outputs = model.forward(inputs)

        # Compute loss
        var loss = criterion.forward(outputs, targets)

        # Backward pass
        loss.backward()

        # Update weights
        optimizer.step()

        total_loss += loss.item()

        # Progress logging
        if batch_idx % 100 == 0:
            print(
                "Batch", batch_idx, "/", num_batches,
                "| Loss:", loss.item()
            )

    return total_loss / Float64(num_batches)


fn validate(
    model: {model},
    dataset: {dataset}Dataset,
    criterion: {loss_class},
    batch_size: Int
) -> Tuple[Float64, Float64]:
    """Validate model on dataset.

    Args:
        model: Model to validate
        dataset: Validation dataset
        criterion: Loss function
        batch_size: Batch size

    Returns:
        (average_loss, accuracy) tuple
    """
    var total_loss: Float64 = 0.0
    var correct: Int = 0
    var total: Int = 0
    var num_batches = len(dataset) // batch_size

    for batch_idx in range(num_batches):
        var start = batch_idx * batch_size
        var end = min(start + batch_size, len(dataset))
        var indices = List[Int]()
        for i in range(start, end):
            indices.append(i)

        var batch = dataset.get_batch(indices)
        var inputs = batch[0]
        var targets = batch[1]

        # Forward pass (no gradient tracking)
        var outputs = model.forward(inputs)

        # Compute loss
        var loss = criterion.forward(outputs, targets)
        total_loss += loss.item()

        # Compute accuracy
        var predictions = outputs.argmax(dim=-1)
        correct += (predictions == targets).sum().item()
        total += len(indices)

    var avg_loss = total_loss / Float64(num_batches)
    var accuracy = Float64(correct) / Float64(total) * 100.0

    return (avg_loss, accuracy)


# =============================================================================
# Main Training Function
# =============================================================================
fn main():
    """Main training function."""
    print("=" * 60)
    print("{model} Training on {dataset}")
    print("=" * 60)
    print()

    # Configuration
    print("Configuration:")
    print("  Epochs:", NUM_EPOCHS)
    print("  Batch Size:", BATCH_SIZE)
    print("  Learning Rate:", LEARNING_RATE)
    print("  Weight Decay:", WEIGHT_DECAY)
    print()

    # Load datasets
    print("Loading datasets...")
    var train_dataset = {dataset}Dataset(train=True)
    var test_dataset = {dataset}Dataset(train=False)
    print("  Train samples:", len(train_dataset))
    print("  Test samples:", len(test_dataset))
    print()

    # Create model
    print("Creating model...")
    var model = {model}(num_classes=NUM_CLASSES)
    var num_params = 0
    for p in model.parameters():
        num_params += p[].numel()
    print("  Parameters:", num_params)
    print()

    # Setup training components
    var optimizer = {optimizer_class}(
        model.parameters(),
        lr=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY
    )
    var criterion = {loss_class}()
    var early_stopping = EarlyStopping(patience=PATIENCE)
    var checkpoint_mgr = CheckpointManager("checkpoints/{model_snake}")

    # Training loop
    print("Starting training...")
    print("-" * 60)

    var best_val_loss: Float64 = Float64.MAX
    var best_val_acc: Float64 = 0.0

    for epoch in range(NUM_EPOCHS):
        print()
        print("Epoch", epoch + 1, "/", NUM_EPOCHS)
        print("-" * 40)

        # Train
        var train_loss = train_epoch(
            model, train_dataset, optimizer, criterion, BATCH_SIZE
        )

        # Validate
        var val_result = validate(model, test_dataset, criterion, BATCH_SIZE)
        var val_loss = val_result[0]
        var val_acc = val_result[1]

        # Log metrics
        print("  Train Loss:", train_loss)
        print("  Val Loss:", val_loss)
        print("  Val Accuracy:", val_acc, "%")

        # Track best
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_val_acc = val_acc
            checkpoint_mgr.save(model, optimizer, epoch, {{"val_loss": val_loss, "val_acc": val_acc}})
            print("  [NEW BEST] Checkpoint saved")

        # Early stopping check
        if early_stopping.step(val_loss, epoch):
            print()
            print("Early stopping triggered at epoch", epoch + 1)
            break

    # Final summary
    print()
    print("=" * 60)
    print("Training Complete!")
    print("=" * 60)
    print("Best Validation Loss:", best_val_loss)
    print("Best Validation Accuracy:", best_val_acc, "%")
    print("Checkpoints saved to: checkpoints/{model_snake}/")
'''


OPTIMIZER_MAP = {
    "sgd": ("SGD", 0.01, 0.0001),
    "adam": ("Adam", 0.001, 0.0001),
    "adamw": ("AdamW", 0.001, 0.01),
    "lars": ("LARS", 0.1, 0.0001),
    "lamb": ("LAMB", 0.001, 0.01),
}

LOSS_MAP = {
    "crossentropy": "CrossEntropyLoss",
    "ce": "CrossEntropyLoss",
    "mse": "MSELoss",
    "bce": "BCELoss",
    "nll": "NLLLoss",
}

DATASET_CLASSES = {
    "mnist": (10, 32),
    "fashionmnist": (10, 32),
    "cifar10": (10, 64),
    "cifar100": (100, 64),
    "imagenet": (1000, 256),
    "emnist": (47, 32),
}


def generate_training_code(
    model: str,
    dataset: str,
    optimizer: str = "adam",
    loss: str = "crossentropy",
    epochs: int = 10,
    batch_size: int | None = None,
    learning_rate: float | None = None,
    weight_decay: float | None = None,
    patience: int = 5,
) -> str:
    """Generate complete training script code.

    Args:
        model: Model name (PascalCase)
        dataset: Dataset name
        optimizer: Optimizer type
        loss: Loss function type
        epochs: Number of epochs
        batch_size: Batch size (None for dataset default)
        learning_rate: Learning rate (None for optimizer default)
        weight_decay: Weight decay (None for optimizer default)
        patience: Early stopping patience

    Returns:
        Generated Mojo code
    """
    model_snake = to_snake_case(model)
    dataset_lower = dataset.lower()
    dataset_snake = to_snake_case(dataset)

    # Get optimizer info
    opt_class, default_lr, default_wd = OPTIMIZER_MAP.get(optimizer.lower(), ("Adam", 0.001, 0.0001))

    # Get loss class
    loss_class = LOSS_MAP.get(loss.lower(), "CrossEntropyLoss")

    # Get dataset info
    num_classes, default_batch = DATASET_CLASSES.get(dataset_lower, (10, 32))

    # Use defaults if not specified
    lr = learning_rate if learning_rate is not None else default_lr
    wd = weight_decay if weight_decay is not None else default_wd
    bs = batch_size if batch_size is not None else default_batch

    return TRAINING_TEMPLATE.format(
        model=model,
        model_snake=model_snake,
        dataset=dataset,
        dataset_snake=dataset_snake,
        timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        optimizer_class=opt_class,
        loss_class=loss_class,
        epochs=epochs,
        batch_size=bs,
        learning_rate=lr,
        weight_decay=wd,
        num_classes=num_classes,
        patience=patience,
    )


def main():
    """CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Generate complete training scripts",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Generate basic training script
    python scripts/generators/generate_training_script.py \\
        --model LeNet5 \\
        --dataset MNIST \\
        --output examples/lenet5/train.mojo

    # Generate with custom hyperparameters
    python scripts/generators/generate_training_script.py \\
        --model ResNet18 \\
        --dataset CIFAR10 \\
        --optimizer adamw \\
        --epochs 100 \\
        --batch-size 128 \\
        --learning-rate 0.0003 \\
        --output examples/resnet/train.mojo

    # Generate with early stopping
    python scripts/generators/generate_training_script.py \\
        --model VGG16 \\
        --dataset ImageNet \\
        --patience 10 \\
        --output examples/vgg/train.mojo
        """,
    )

    parser.add_argument("--model", required=True, help="Model name (PascalCase)")
    parser.add_argument("--dataset", required=True, help="Dataset name")
    parser.add_argument(
        "--optimizer",
        choices=["sgd", "adam", "adamw", "lars", "lamb"],
        default="adam",
        help="Optimizer type (default: adam)",
    )
    parser.add_argument(
        "--loss",
        choices=["crossentropy", "mse", "bce", "nll"],
        default="crossentropy",
        help="Loss function (default: crossentropy)",
    )
    parser.add_argument("--epochs", type=int, default=10, help="Number of epochs")
    parser.add_argument("--batch-size", type=int, help="Batch size")
    parser.add_argument("--learning-rate", type=float, help="Learning rate")
    parser.add_argument("--weight-decay", type=float, help="Weight decay")
    parser.add_argument("--patience", type=int, default=5, help="Early stopping patience")
    parser.add_argument("--output", "-o", required=True, help="Output file path")
    parser.add_argument("--force", "-f", action="store_true", help="Overwrite existing")

    args = parser.parse_args()

    # Generate code
    code = generate_training_code(
        model=args.model,
        dataset=args.dataset,
        optimizer=args.optimizer,
        loss=args.loss,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        patience=args.patience,
    )

    # Write output
    output_path = Path(args.output)
    if output_path.exists() and not args.force:
        print(f"Error: {output_path} already exists. Use --force to overwrite.")
        return 1

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(code)
    print(f"Generated: {output_path}")
    return 0


if __name__ == "__main__":
    exit(main())
