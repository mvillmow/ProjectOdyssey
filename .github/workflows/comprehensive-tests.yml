name: Comprehensive Tests

# Unified test workflow combining comprehensive tests with quality metrics
# Runs ALL tests (Mojo and Python) with metrics collection and quality analysis
# Target duration: < 15 minutes (parallelized across test groups)

on:
  # Run on all pull requests
  pull_request:

  # Run on pushes to main
  push:
    branches:
      - main

  # Allow manual runs with extended options
  workflow_dispatch:
    inputs:
      run_extended:
        description: 'Run extended analysis (SIMD, complexity)'
        required: false
        default: 'false'
        type: boolean

permissions:
  contents: read
  pull-requests: write  # For PR comments

jobs:
  # ============================================================================
  # Mojo Package Compilation Check - Verify shared package compiles
  # This runs BEFORE tests to catch compilation errors early
  # ============================================================================
  mojo-compilation:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    name: "Mojo Package Compilation"

    steps:
      - name: Checkout code
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3

      - name: Set up Pixi
        uses: prefix-dev/setup-pixi@v0.9.3
        with:
          pixi-version: latest
          cache: true

      - name: Cache Pixi environments
        uses: actions/cache@v4
        with:
          path: ~/.pixi
          key: pixi-${{ runner.os }}-${{ hashFiles('pixi.toml') }}
          restore-keys: |
            pixi-${{ runner.os }}-

      - name: Check Mojo version
        run: pixi run mojo --version

      - name: Compile shared package
        run: |
          echo "=================================================="
          echo "Compiling shared package..."
          echo "=================================================="

          REPO_ROOT="$(pwd)"
          mkdir -p compilation-results

          # Count total files for reporting
          total_files=$(find shared -name "*.mojo" -type f | wc -l)
          echo "Total .mojo files in shared/: $total_files"

          # Build the shared package (compiles all modules properly with relative imports)
          if pixi run mojo package -I "$REPO_ROOT" shared -o /tmp/shared.mojopkg 2>&1; then
            echo "‚úÖ PASSED: shared package compiled successfully"
            echo "compilation_status=success" >> $GITHUB_ENV
          else
            echo "‚ùå FAILED: shared package compilation"
            echo ""
            echo "Fix compilation errors before merging to main."
            echo "compilation_status=failed" >> $GITHUB_ENV
            exit 1
          fi

          # Save compilation summary
          cat > compilation-results/summary.txt << EOF
          Mojo Compilation Check
          ======================
          Total .mojo files: $total_files
          Status: success
          EOF

      - name: Upload compilation results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: mojo-compilation-results
          path: compilation-results/
          retention-days: 7

  # ============================================================================
  # Comprehensive Mojo Test Suite - Parallelized by Test Group
  # ============================================================================
  test-mojo-comprehensive:
    needs: [mojo-compilation]  # Only run tests if compilation succeeds
    runs-on: ubuntu-latest
    timeout-minutes: 15
    strategy:
      fail-fast: false  # Continue running other groups even if one fails
      matrix:
        test-group:
          - name: "Core"
            path: "tests/shared/core"
            pattern: "test_*.mojo"
          - name: "Training"
            path: "tests/shared/training"
            pattern: "test_*.mojo"
          - name: "Data"
            path: "tests/shared/data"
            pattern: "test_*.mojo"
          - name: "Integration Tests"
            path: "tests/shared/integration"
            pattern: "test_*.mojo"
          - name: "Shared Infra"
            path: "tests/shared"
            pattern: "test_imports.mojo utils/test_*.mojo fixtures/test_*.mojo"
          - name: "Benchmarks"
            path: "tests/shared/benchmarks"
            pattern: "bench_*.mojo"
          - name: "Configs"
            path: "tests/configs"
            pattern: "test_*.mojo"
          - name: "Tooling"
            path: "tests/tooling/benchmarks"
            pattern: "test_*.mojo"
          - name: "Top-Level Tests"
            path: "tests"
            pattern: "test_*.mojo training/test_*.mojo unit/test_*.mojo integration/test_*.mojo"
          - name: "Debug"
            path: "tests/debug"
            pattern: "test_*.mojo"
          - name: "LeNet-5 Examples"
            path: "examples/lenet-emnist"
            # Only run tests that don't require EMNIST dataset (test_lenet5.mojo, test_model.mojo)
            # Other tests (test_gradients, test_loss_decrease, test_predictions, etc.) require:
            #   datasets/emnist/ which must be downloaded separately
            pattern: "test_lenet5.mojo test_model.mojo"
          - name: "Core Types"
            path: "tests/core/types"
            pattern: "test_*.mojo"
          - name: "Autograd"
            path: "tests/shared/autograd"
            pattern: "test_*.mojo"
          - name: "Benchmarking"
            path: "tests/shared/benchmarking"
            pattern: "test_*.mojo"
          - name: "Data Formats"
            path: "tests/shared/data/formats"
            pattern: "test_*.mojo"
          - name: "Data Datasets"
            path: "tests/shared/data/datasets"
            pattern: "test_*.mojo"
          - name: "Testing Fixtures"
            path: "tests/shared/testing"
            pattern: "test_*.mojo"

    name: "${{ matrix.test-group.name }}"

    steps:
      - name: Checkout code
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3  # v6.0.0

      - name: Set up Pixi
        uses: prefix-dev/setup-pixi@v0.9.3
        with:
          pixi-version: latest
          cache: true

      - name: Cache Pixi environments
        uses: actions/cache@v4
        with:
          path: ~/.pixi
          key: pixi-${{ runner.os }}-${{ hashFiles('pixi.toml') }}
          restore-keys: |
            pixi-${{ runner.os }}-

      - name: Run test group
        # Integration tests may have segfaults on CI runners due to memory constraints
        # Allow them to fail without blocking the workflow (tracked in issue)
        continue-on-error: ${{ matrix.test-group.name == 'Integration Tests' }}
        run: |
          echo "=================================================="
          echo "Test Group: ${{ matrix.test-group.name }}"
          echo "Path: ${{ matrix.test-group.path }}"
          echo "=================================================="
          echo ""

          mkdir -p test-results

          test_count=0
          passed_count=0
          failed_count=0
          failed_tests=""
          start_time=$(date +%s)

          # Save repository root for imports
          REPO_ROOT="$(pwd)"
          TEST_PATH="${{ matrix.test-group.path }}"

          # Expand pattern into actual files (from repo root)
          test_files=""
          for pattern in ${{ matrix.test-group.pattern }}; do
            # Check if pattern contains wildcards or is a direct path
            if [[ "$pattern" == *"*"* ]]; then
              # Glob pattern - expand it with TEST_PATH prefix
              for file in $TEST_PATH/$pattern; do
                if [ -f "$file" ]; then
                  test_files="$test_files $file"
                fi
              done
            else
              # Direct file or subdirectory pattern
              if [ -f "$TEST_PATH/$pattern" ]; then
                test_files="$test_files $TEST_PATH/$pattern"
              elif [[ "$pattern" == *"/"* ]]; then
                # Subdirectory pattern like "datasets/test_*.mojo"
                for file in $TEST_PATH/$pattern; do
                  if [ -f "$file" ]; then
                    test_files="$test_files $file"
                  fi
                done
              fi
            fi
          done

          if [ -z "$test_files" ]; then
            echo "‚ö†Ô∏è  No test files found for this group"
            echo "This may be expected if tests haven't been created yet."
            echo '{"group": "${{ matrix.test-group.name }}", "tests": 0, "passed": 0, "failed": 0, "duration": 0}' > "test-results/${{ matrix.test-group.name }}.json"
            exit 0
          fi

          # Run each test file
          for test_file in $test_files; do
            if [ -f "$test_file" ]; then
              echo ""
              echo "=================================================="
              echo "Running: $test_file"
              echo "=================================================="
              test_count=$((test_count + 1))

              if pixi run mojo -I "$REPO_ROOT" -I . "$test_file"; then
                echo "‚úÖ PASSED: $test_file"
                passed_count=$((passed_count + 1))
              else
                echo "‚ùå FAILED: $test_file"
                failed_count=$((failed_count + 1))
                failed_tests="$failed_tests\\n  - $test_file"
              fi
            fi
          done

          end_time=$(date +%s)
          duration=$((end_time - start_time))

          echo ""
          echo "=================================================="
          echo "Test Group Summary: ${{ matrix.test-group.name }}"
          echo "=================================================="
          echo "Total: $test_count tests"
          echo "Passed: $passed_count tests"
          echo "Failed: $failed_count tests"
          echo "Duration: ${duration}s"
          echo ""

          # Save results as JSON for aggregation
          cat > "test-results/${{ matrix.test-group.name }}.json" << EOF
          {
            "group": "${{ matrix.test-group.name }}",
            "tests": $test_count,
            "passed": $passed_count,
            "failed": $failed_count,
            "duration": $duration
          }
          EOF

          # Also save human-readable format
          result_file="$REPO_ROOT/test-results/${{ matrix.test-group.name }}.txt"
          echo "Test Group: ${{ matrix.test-group.name }}" > "$result_file"
          echo "Total: $test_count tests run, $passed_count passed, $failed_count failed" >> "$result_file"
          echo "Duration: ${duration}s" >> "$result_file"
          if [ $failed_count -gt 0 ]; then
            echo "" >> "$result_file"
            echo "Failed tests:" >> "$result_file"
            echo -e "$failed_tests" >> "$result_file"
          fi

          if [ $failed_count -gt 0 ]; then
            exit 1
          fi

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-${{ matrix.test-group.name }}
          path: test-results/
          retention-days: 7

  # ============================================================================
  # Python Tests
  # ============================================================================
  test-python:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    name: "Python Tests"

    steps:
      - name: Checkout code
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3  # v6.0.0

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-timeout

          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi

          if [ -f requirements-dev.txt ]; then
            pip install -r requirements-dev.txt
          fi

      - name: Run Python tests
        run: |
          mkdir -p test-results

          # Collect all Python test paths
          TEST_PATHS=""

          # Unit tests
          if [ -d "tests/unit" ] && [ -n "$(find tests/unit -name 'test_*.py' 2>/dev/null)" ]; then
            TEST_PATHS="$TEST_PATHS tests/unit/"
          fi

          # Integration tests (agents, foundation)
          [ -f "tests/agents/test_integration.py" ] && TEST_PATHS="$TEST_PATHS tests/agents/test_integration.py"
          [ -f "tests/foundation/test_structure_integration.py" ] && TEST_PATHS="$TEST_PATHS tests/foundation/test_structure_integration.py"

          start_time=$(date +%s)
          if [ -n "$TEST_PATHS" ]; then
            echo "Running Python tests from: $TEST_PATHS"
            if pytest $TEST_PATHS --verbose --timeout=300 --maxfail=5; then
              end_time=$(date +%s)
              duration=$((end_time - start_time))
              echo '{"group": "Python Tests", "tests": 1, "passed": 1, "failed": 0, "duration": '"$duration"'}' > test-results/Python-Tests.json
            else
              end_time=$(date +%s)
              duration=$((end_time - start_time))
              echo '{"group": "Python Tests", "tests": 1, "passed": 0, "failed": 1, "duration": '"$duration"'}' > test-results/Python-Tests.json
              exit 1
            fi
          else
            echo "‚ö†Ô∏è  No Python tests found yet"
            echo '{"group": "Python Tests", "tests": 0, "passed": 0, "failed": 0, "duration": 0}' > test-results/Python-Tests.json
          fi

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: test-results-Python-Tests
          path: test-results/
          retention-days: 7

  # ============================================================================
  # Code Quality Analysis
  # ============================================================================
  code-quality:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    name: "Code Quality Analysis"

    steps:
      - name: Checkout code
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install analysis tools
        run: |
          pip install radon lizard

      - name: Analyze code metrics
        run: |
          echo "=================================================="
          echo "Code Quality Analysis"
          echo "=================================================="

          mkdir -p quality-reports

          # Python complexity analysis
          echo "## Python Code Complexity" > quality-reports/complexity.md
          echo "" >> quality-reports/complexity.md
          echo "Functions with complexity > 10 (consider refactoring):" >> quality-reports/complexity.md
          echo '```' >> quality-reports/complexity.md
          radon cc scripts/ -a -nc 2>/dev/null || echo "No complex functions found"
          echo '```' >> quality-reports/complexity.md

          # Mojo code statistics
          echo "" >> quality-reports/complexity.md
          echo "## Mojo Code Statistics" >> quality-reports/complexity.md
          echo "" >> quality-reports/complexity.md

          total_mojo_files=$(find shared tests -name "*.mojo" 2>/dev/null | wc -l)
          total_mojo_lines=$(find shared tests -name "*.mojo" -exec cat {} \; 2>/dev/null | wc -l)
          total_functions=$(grep -r "^fn " shared tests --include="*.mojo" 2>/dev/null | wc -l)
          total_structs=$(grep -r "^struct " shared tests --include="*.mojo" 2>/dev/null | wc -l)

          cat >> quality-reports/complexity.md << EOF
          | Metric | Count |
          |--------|-------|
          | Mojo Files | $total_mojo_files |
          | Lines of Code | $total_mojo_lines |
          | Functions (fn) | $total_functions |
          | Structs | $total_structs |
          EOF

          # Technical debt markers
          echo "" >> quality-reports/complexity.md
          echo "## Technical Debt Markers" >> quality-reports/complexity.md
          echo "" >> quality-reports/complexity.md

          todo_count=$(grep -r "TODO" shared tests scripts --include="*.mojo" --include="*.py" 2>/dev/null | wc -l)
          fixme_count=$(grep -r "FIXME" shared tests scripts --include="*.mojo" --include="*.py" 2>/dev/null | wc -l)

          cat >> quality-reports/complexity.md << EOF
          | Marker | Count |
          |--------|-------|
          | TODO | $todo_count |
          | FIXME | $fixme_count |
          EOF

          cat quality-reports/complexity.md

      - name: Upload quality reports
        uses: actions/upload-artifact@v5
        with:
          name: code-quality-reports
          path: quality-reports/
          retention-days: 30

  # ============================================================================
  # SIMD Analysis (Extended - manual trigger only)
  # ============================================================================
  simd-analysis:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    name: "SIMD Analysis"
    if: github.event_name == 'workflow_dispatch' && inputs.run_extended == true

    steps:
      - name: Checkout code
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3

      - name: Analyze SIMD usage
        run: |
          echo "=================================================="
          echo "SIMD Usage Analysis"
          echo "=================================================="

          mkdir -p simd-reports

          simd_count=$(grep -r "SIMD\[" shared --include="*.mojo" 2>/dev/null | wc -l)
          vectorize_count=$(grep -r "vectorize" shared --include="*.mojo" 2>/dev/null | wc -l)
          parallelize_count=$(grep -r "parallelize" shared --include="*.mojo" 2>/dev/null | wc -l)

          cat > simd-reports/analysis.md << EOF
          # SIMD Optimization Analysis

          ## Current Usage

          | Pattern | Count |
          |---------|-------|
          | SIMD[] declarations | $simd_count |
          | vectorize() calls | $vectorize_count |
          | parallelize() calls | $parallelize_count |

          ## Files with heavy loop usage (optimization candidates)
          EOF

          echo '```' >> simd-reports/analysis.md
          grep -rl "for .* in range" shared --include="*.mojo" 2>/dev/null | \
            xargs -I{} sh -c 'echo "{}": $(grep -c "for .* in range" "{}")' 2>/dev/null | \
            sort -t: -k2 -nr | head -10 >> simd-reports/analysis.md || echo "None found"
          echo '```' >> simd-reports/analysis.md

          cat simd-reports/analysis.md

      - name: Upload SIMD analysis
        uses: actions/upload-artifact@v5
        with:
          name: simd-analysis
          path: simd-reports/
          retention-days: 30

  # ============================================================================
  # Combined Test Report with Metrics
  # ============================================================================
  test-report:
    needs: [mojo-compilation, test-mojo-comprehensive, test-python, code-quality]
    runs-on: ubuntu-latest
    if: always()
    name: "Test Report"

    steps:
      - name: Checkout code
        uses: actions/checkout@1af3b93b6815bc44a9784bd300feb67ff0d1eeb3  # v6.0.0

      - name: Download all test results
        uses: actions/download-artifact@v6
        with:
          path: all-results/
          pattern: "*"
          merge-multiple: false

      - name: Generate combined report
        run: |
          echo "# üß™ Comprehensive Test Results" > test-report.md
          echo "" >> test-report.md
          echo "**Workflow Run**: [\`${{ github.run_number }}\`](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> test-report.md
          echo "**Commit**: \`${{ github.sha }}\`" >> test-report.md
          echo "**Date**: $(date -Iseconds)" >> test-report.md
          echo "" >> test-report.md

          total_groups=0
          passed_groups=0
          failed_groups=0
          total_tests=0
          total_passed=0
          total_failed=0
          total_duration=0

          echo "## Test Groups" >> test-report.md
          echo "" >> test-report.md
          echo "| Group | Tests | Passed | Failed | Duration |" >> test-report.md
          echo "|-------|-------|--------|--------|----------|" >> test-report.md

          # Process each result file
          for result_dir in all-results/test-results-*; do
            if [ -d "$result_dir" ]; then
              for json_file in "$result_dir"/*.json; do
                if [ -f "$json_file" ]; then
                  total_groups=$((total_groups + 1))

                  group=$(jq -r '.group' "$json_file" 2>/dev/null || echo "Unknown")
                  tests=$(jq -r '.tests' "$json_file" 2>/dev/null || echo "0")
                  passed=$(jq -r '.passed' "$json_file" 2>/dev/null || echo "0")
                  failed=$(jq -r '.failed' "$json_file" 2>/dev/null || echo "0")
                  duration=$(jq -r '.duration' "$json_file" 2>/dev/null || echo "0")

                  total_tests=$((total_tests + tests))
                  total_passed=$((total_passed + passed))
                  total_failed=$((total_failed + failed))
                  total_duration=$((total_duration + duration))

                  if [ "$failed" -gt 0 ]; then
                    failed_groups=$((failed_groups + 1))
                    status="‚ùå"
                  else
                    passed_groups=$((passed_groups + 1))
                    status="‚úÖ"
                  fi

                  echo "| $status $group | $tests | $passed | $failed | ${duration}s |" >> test-report.md
                fi
              done
            fi
          done

          echo "" >> test-report.md
          echo "## Summary" >> test-report.md
          echo "" >> test-report.md
          echo "| Metric | Value |" >> test-report.md
          echo "|--------|-------|" >> test-report.md
          echo "| Total Test Groups | $total_groups |" >> test-report.md
          echo "| Passed Groups | $passed_groups |" >> test-report.md
          echo "| Failed Groups | $failed_groups |" >> test-report.md
          echo "| Total Tests | $total_tests |" >> test-report.md
          echo "| Passed Tests | $total_passed |" >> test-report.md
          echo "| Failed Tests | $total_failed |" >> test-report.md
          echo "| Total Duration | ${total_duration}s |" >> test-report.md

          if [ $total_tests -gt 0 ]; then
            pass_rate=$(echo "scale=1; $total_passed * 100 / $total_tests" | bc 2>/dev/null || echo "0")
            echo "| Pass Rate | ${pass_rate}% |" >> test-report.md
          fi

          echo "" >> test-report.md

          if [ $total_failed -eq 0 ]; then
            echo "‚úÖ **All tests passed!**" >> test-report.md
          else
            echo "‚ùå **Some tests failed. See details above.**" >> test-report.md
          fi

          # Include code quality if available
          if [ -f "all-results/code-quality-reports/complexity.md" ]; then
            echo "" >> test-report.md
            echo "---" >> test-report.md
            echo "" >> test-report.md
            echo "## üìà Code Quality" >> test-report.md
            cat "all-results/code-quality-reports/complexity.md" >> test-report.md
          fi

          # Display report
          cat test-report.md

      - name: Upload combined report
        uses: actions/upload-artifact@v5
        with:
          name: comprehensive-test-report
          path: test-report.md
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');

            try {
              const report = fs.readFileSync('test-report.md', 'utf8');

              // Check if a comment already exists
              const { data: comments } = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });

              const botComment = comments.find(comment =>
                comment.user.type === 'Bot' &&
                comment.body.includes('üß™ Comprehensive Test Results')
              );

              const commentBody = report;

              if (botComment) {
                // Update existing comment
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: botComment.id,
                  body: commentBody
                });
                console.log('Updated existing PR comment');
              } else {
                // Create new comment
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: commentBody
                });
                console.log('Created new PR comment');
              }
            } catch (error) {
              console.error('Failed to comment on PR:', error);
            }

      - name: Fail if any tests failed
        run: |
          # Check if any test groups failed
          failed=0
          for result_dir in all-results/test-results-*; do
            if [ -d "$result_dir" ]; then
              for json_file in "$result_dir"/*.json; do
                if [ -f "$json_file" ]; then
                  failed_count=$(jq -r '.failed' "$json_file" 2>/dev/null || echo "0")
                  if [ "$failed_count" -gt 0 ]; then
                    failed=1
                    break
                  fi
                fi
              done
            fi
          done

          if [ $failed -eq 1 ]; then
            echo "‚ùå FAIL: Some tests failed. See test report for details."
            exit 1
          else
            echo "‚úÖ All tests passed!"
            exit 0
          fi
