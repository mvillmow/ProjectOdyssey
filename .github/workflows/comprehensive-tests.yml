name: Comprehensive Tests

# Unified test workflow combining comprehensive tests with quality metrics
# Runs ALL tests (Mojo and Python) with metrics collection and quality analysis
# Target duration: < 15 minutes (parallelized across test groups)

on:
  # Run on all pull requests
  pull_request:

  # Run on pushes to main
  push:
    branches:
      - main

  # Allow manual runs with extended options
  workflow_dispatch:
    inputs:
      run_extended:
        description: 'Run extended analysis (SIMD, complexity)'
        required: false
        default: 'false'
        type: boolean

permissions:
  contents: read
  pull-requests: write  # For PR comments

jobs:
  # ============================================================================
  # Mojo Syntax Validation - Catch deprecated patterns early
  # ============================================================================
  mojo-syntax-check:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    name: "Mojo Syntax Validation"

    steps:
      - name: Checkout code
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8

      - name: Check for deprecated List[Type](args) pattern
        run: |
          echo "=================================================="
          echo "Checking for deprecated List[Type](args) pattern..."
          echo "=================================================="

          # Search for deprecated pattern (any type)
          if grep -rE "List\[.*\]\([0-9]" --include="*.mojo" shared/ tests/ examples/ benchmarks/ scripts/ 2>/dev/null; then
            echo ""
            echo "‚ùå FAILED: Found deprecated List[Type](args) syntax"
            echo ""
            echo "The variadic List constructor does not exist in Mojo."
            echo "Use list literals instead: var shape = [3, 4, 5]"
            echo ""
            echo "See: https://docs.modular.com/mojo/manual/types#list"
            echo "See: CLAUDE.md section 'List Initialization'"
            exit 1
          else
            echo ""
            echo "‚úÖ PASSED: No deprecated List constructor patterns found"
          fi

  # ============================================================================
  # Mojo Package Compilation Check - Verify shared package compiles
  # This runs BEFORE tests to catch compilation errors early
  # ============================================================================
  mojo-compilation:
    needs: [mojo-syntax-check]
    runs-on: ubuntu-latest
    timeout-minutes: 10
    name: "Mojo Package Compilation"

    steps:
      - name: Checkout code
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8

      - name: Set up Pixi
        uses: prefix-dev/setup-pixi@v0.9.3
        with:
          pixi-version: latest
          cache: true

      - name: Cache Pixi environments
        uses: actions/cache@v5
        with:
          path: ~/.pixi
          key: pixi-${{ runner.os }}-${{ hashFiles('pixi.toml') }}
          restore-keys: |
            pixi-${{ runner.os }}-

      - name: Check Mojo version
        run: pixi run mojo --version

      - name: Install Just
        uses: extractions/setup-just@v3

      - name: Compile shared package
        run: |
          echo "=================================================="
          echo "Compiling shared package..."
          echo "=================================================="

          mkdir -p compilation-results

          # Count total files for reporting
          total_files=$(find shared -name "*.mojo" -type f | wc -l)
          echo "Total .mojo files in shared/: $total_files"

          # Build the shared package (compiles all modules properly with relative imports)
          export REPO_ROOT=$(pwd)
          if pixi run mojo package -I "$REPO_ROOT" shared -o /tmp/shared.mojopkg 2>&1; then
            echo "‚úÖ PASSED: shared package compiled successfully"
            echo "compilation_status=success" >> $GITHUB_ENV
          else
            echo "‚ùå FAILED: shared package compilation"
            echo ""
            echo "Fix compilation errors before merging to main."
            echo "compilation_status=failed" >> $GITHUB_ENV
            exit 1
          fi

          # Save compilation summary
          cat > compilation-results/summary.txt << EOF
          Mojo Compilation Check
          ======================
          Total .mojo files: $total_files
          Status: success
          EOF

      - name: Upload compilation results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: mojo-compilation-results
          path: compilation-results/
          retention-days: 7

  # ============================================================================
  # Test Coverage Validation - Ensure all tests are discovered
  # ============================================================================
  validate-test-coverage:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    name: "Test Coverage Validation"

    steps:
      - name: Checkout code
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install pyyaml

      - name: Validate test coverage
        id: validation
        continue-on-error: true
        run: |
          python scripts/validate_test_coverage.py
          echo "exit_code=$?" >> $GITHUB_OUTPUT

      - name: Post validation report to PR
        if: github.event_name == 'pull_request' && steps.validation.outputs.exit_code != '0'
        run: |
          python scripts/validate_test_coverage.py --post-pr

      - name: Fail if validation failed
        if: steps.validation.outputs.exit_code != '0'
        run: |
          echo "Test coverage validation failed. See PR comment for details."
          exit 1

  # ============================================================================
  # Comprehensive Mojo Test Suite - Parallelized by Test Group
  # ============================================================================
  test-mojo-comprehensive:
    needs: [mojo-compilation, validate-test-coverage]  # Only run tests if compilation and validation succeed
    runs-on: ubuntu-latest
    timeout-minutes: 15
    strategy:
      fail-fast: false  # Continue running other groups even if one fails
      matrix:
        test-group:
          # Core Tests - Split into domain-specific groups for parallelization
          - name: "Core Tensors"
            path: "tests/shared/core"
            pattern: "test_tensors.mojo test_arithmetic.mojo test_arithmetic_contiguous.mojo test_arithmetic_backward.mojo test_broadcasting.mojo test_shape.mojo test_shape_regression.mojo test_shape_edge_cases.mojo test_creation.mojo test_reduction.mojo test_reduction_forward.mojo test_reduction_edge_cases.mojo test_matrix.mojo test_matrix_edge_cases.mojo test_matmul.mojo test_pooling.mojo test_properties.mojo test_high_dimensional.mojo"
          - name: "Core Activations"
            path: "tests/shared/core"
            pattern: "test_activations.mojo test_activation_funcs.mojo test_activation_ops.mojo test_advanced_activations.mojo"
          - name: "Core Loss"
            path: "tests/shared/core"
            pattern: "test_losses.mojo test_loss_funcs.mojo test_loss_utils.mojo"
          - name: "Core DTypes"
            path: "tests/shared/core"
            pattern: "test_bf8.mojo test_bfloat16.mojo test_fp4.mojo test_fp8.mojo test_mxfp4.mojo test_nvfp4.mojo test_unsigned.mojo test_dtype_dispatch.mojo"
          - name: "Core Gradient"
            path: "tests/shared/core"
            pattern: "test_backward.mojo test_backward_compat_aliases.mojo test_gradient_checking.mojo test_gradient_numerical_stability.mojo test_variable_backward.mojo"
          - name: "Core Initializers"
            path: "tests/shared/core"
            pattern: "test_initializers.mojo test_initializers_validation.mojo"
          - name: "Core NN Modules"
            path: "tests/shared/core"
            pattern: "test_layers.mojo test_linear.mojo test_conv.mojo test_normalization.mojo test_dropout.mojo test_module.mojo"
          - name: "Core Elementwise"
            path: "tests/shared/core"
            pattern: "test_elementwise.mojo test_elementwise_dispatch.mojo test_elementwise_forward.mojo test_elementwise_edge_cases.mojo test_comparison_ops.mojo test_edge_cases.mojo"
          - name: "Core ExTensor"
            path: "tests/shared/core"
            pattern: "test_extensor_new_methods.mojo test_extensor_operators.mojo test_extensor_slicing.mojo test_memory_leaks.mojo test_extensor_serialization.mojo"
          - name: "Core Utilities"
            path: "tests/shared/core"
            pattern: "test_utilities.mojo test_utility.mojo test_utils.mojo test_validation.mojo test_validation_extended.mojo test_integration.mojo test_inplace_simd.mojo"
          - name: "Data"
            path: "tests/shared/data"
            pattern: "test_*.mojo datasets/test_*.mojo samplers/test_*.mojo transforms/test_*.mojo loaders/test_*.mojo formats/test_*.mojo"
          - name: "Integration Tests"
            path: "tests/shared/integration"
            pattern: "test_*.mojo"
          - name: "Shared Infra"
            path: "tests/shared"
            pattern: "test_imports.mojo test_data_generators.mojo test_model_utils.mojo test_serialization.mojo utils/test_*.mojo fixtures/test_*.mojo training/test_*.mojo"
          - name: "Helpers"
            path: "tests/helpers"
            pattern: "test_*.mojo"
          - name: "Tooling"
            path: "tests/tooling/benchmarks"
            pattern: "test_*.mojo"
          - name: "Examples"
            path: "tests/examples"
            pattern: "test_trait_based_serialization.mojo"
          - name: "Top-Level Tests"
            path: "tests"
            pattern: "test_*.mojo training/test_*.mojo unit/test_*.mojo integration/test_*.mojo"
          - name: "Debug"
            path: "tests/debug"
            pattern: "test_*.mojo"
          - name: "LeNet-5 Examples"
            path: "examples/lenet-emnist"
            # Only run tests that don't require EMNIST dataset (test_lenet5.mojo, test_model.mojo)
            # Other tests (test_gradients, test_loss_decrease, test_predictions, etc.) require:
            #   datasets/emnist/ which must be downloaded separately
            pattern: "test_lenet5.mojo test_model.mojo"
          - name: "Test Examples"
            path: "tests/examples"
            pattern: "test_*.mojo"
          - name: "Core Types"
            path: "tests/core/types"
            pattern: "test_*.mojo"
          - name: "Autograd"
            path: "tests/shared/autograd"
            pattern: "test_*.mojo"
          - name: "Benchmarking"
            path: "tests/shared/benchmarking"
            pattern: "test_*.mojo"
          - name: "Data Formats"
            path: "tests/shared/data/formats"
            pattern: "test_*.mojo"
          - name: "Data Datasets"
            path: "tests/shared/data/datasets"
            pattern: "test_*.mojo"
          - name: "Testing Fixtures"
            path: "tests/shared/testing"
            pattern: "test_*.mojo"
          - name: "Models"
            path: "tests/models"
            pattern: "test_*_layers.mojo"
          - name: "Data Loaders"
            path: "tests/shared/data/loaders"
            pattern: "test_*.mojo"
          - name: "Data Transforms"
            path: "tests/shared/data/transforms"
            pattern: "test_*.mojo"
          - name: "Data Samplers"
            path: "tests/shared/data/samplers"
            pattern: "test_*.mojo"
          - name: "Benchmark Framework"
            path: "benchmarks"
            pattern: "test_*.mojo"

    name: "${{ matrix.test-group.name }}"

    steps:
      - name: Checkout code
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8  # v6.0.1

      - name: Set up Pixi
        uses: prefix-dev/setup-pixi@v0.9.3
        with:
          pixi-version: latest
          cache: true

      - name: Cache Pixi environments
        uses: actions/cache@v5
        with:
          path: ~/.pixi
          key: pixi-${{ runner.os }}-${{ hashFiles('pixi.toml') }}
          restore-keys: |
            pixi-${{ runner.os }}-

      - name: Install Just
        uses: extractions/setup-just@v3

      - name: Run test group
        # Integration tests may have segfaults on CI runners due to memory constraints
        # Allow them to fail without blocking the workflow (tracked in issue)
        continue-on-error: ${{ matrix.test-group.name == 'Integration Tests' }}
        run: |
          echo "=================================================="
          echo "Test Group: ${{ matrix.test-group.name }}"
          echo "Path: ${{ matrix.test-group.path }}"
          echo "=================================================="
          echo ""

          mkdir -p test-results
          start_time=$(date +%s)

          # Use justfile to run test group
          # The justfile handles all the complexity of test discovery and execution
          if just test-group "${{ matrix.test-group.path }}" "${{ matrix.test-group.pattern }}"; then
            test_result="passed"
            exit_code=0
          else
            test_result="failed"
            exit_code=1
          fi


          end_time=$(date +%s)
          duration=$((end_time - start_time))

          # Create simplified result files
          # Note: Detailed metrics are in justfile output, these are for aggregation
          cat > "test-results/${{ matrix.test-group.name }}.json" << EOF
          {
            "group": "${{ matrix.test-group.name }}",
            "tests": 1,
            "passed": $([ "$test_result" = "passed" ] && echo 1 || echo 0),
            "failed": $([ "$test_result" = "failed" ] && echo 1 || echo 0),
            "duration": $duration
          }
          EOF

          echo "Test Group: ${{ matrix.test-group.name }}" > "test-results/${{ matrix.test-group.name }}.txt"
          echo "Result: $test_result" >> "test-results/${{ matrix.test-group.name }}.txt"
          echo "Duration: ${duration}s" >> "test-results/${{ matrix.test-group.name }}.txt"

          exit $exit_code

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: test-results-${{ matrix.test-group.name }}
          path: test-results/
          retention-days: 7

  # ============================================================================
  # Configs Tests - Non-blocking (known segfault issues)
  # ============================================================================
  test-configs:
    needs: [mojo-compilation, validate-test-coverage]
    runs-on: ubuntu-latest
    continue-on-error: true  # Non-blocking - investigating segfaults on PR branches
    timeout-minutes: 15
    name: "Configs"

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Pixi
        uses: prefix-dev/setup-pixi@v0.9.3
        with:
          pixi-version: latest
          cache: true

      - name: Install Just
        uses: extractions/setup-just@v3

      - name: Run Configs tests
        run: just test-group tests/configs "test_*.mojo"

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: test-results-Configs
          path: test-results/
          retention-days: 7

  # ============================================================================
  # NOTE: Training tests moved to weekly workflow (requires dataset downloads)
  # ============================================================================

  # ============================================================================
  # Benchmarks Tests - Non-blocking (known test discovery issues)
  # ============================================================================
  test-benchmarks:
    needs: [mojo-compilation, validate-test-coverage]
    runs-on: ubuntu-latest
    continue-on-error: true  # Non-blocking - investigating test discovery issues
    timeout-minutes: 15
    name: "Benchmarks"

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Pixi
        uses: prefix-dev/setup-pixi@v0.9.3
        with:
          pixi-version: latest
          cache: true

      - name: Install Just
        uses: extractions/setup-just@v3

      - name: Run Benchmarks tests
        run: just test-group tests/shared/benchmarks "bench_*.mojo"

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: test-results-Benchmarks
          path: test-results/
          retention-days: 7

  # ============================================================================
  # Core Layers Tests - Non-blocking (known Mojo runtime segfault issues)
  # ============================================================================
  test-core-layers:
    needs: [mojo-compilation, validate-test-coverage]
    runs-on: ubuntu-latest
    continue-on-error: true  # Non-blocking - investigating Mojo runtime segfaults
    timeout-minutes: 15
    name: "Core Layers"

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Pixi
        uses: prefix-dev/setup-pixi@v0.9.3
        with:
          pixi-version: latest
          cache: true

      - name: Install Just
        uses: extractions/setup-just@v3

      - name: Run Core Layers tests
        run: just test-group tests/shared/core/layers "test_*.mojo"

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: test-results-Core-Layers
          path: test-results/
          retention-days: 7

  # ============================================================================
  # Python Tests
  # ============================================================================
  test-python:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    name: "Python Tests"

    steps:
      - name: Checkout code
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8  # v6.0.1

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-timeout

          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi

          if [ -f requirements-dev.txt ]; then
            pip install -r requirements-dev.txt
          fi

      - name: Run Python tests
        run: |
          mkdir -p test-results

          # Collect all Python test paths
          TEST_PATHS=""

          # Unit tests
          if [ -d "tests/unit" ] && [ -n "$(find tests/unit -name 'test_*.py' 2>/dev/null)" ]; then
            TEST_PATHS="$TEST_PATHS tests/unit/"
          fi
          # Claude hook tests
          if [ -d "tests/claude " ] && [ -n "$(find tests/claude -name 'test_*.py' 2>/dev/null)" ]; then
            TEST_PATHS="$TEST_PATHS tests/claude/"
          fi

          # Integration tests (agents, foundation)
          [ -f "tests/agents/test_integration.py" ] && TEST_PATHS="$TEST_PATHS tests/agents/test_integration.py"
          [ -f "tests/foundation/test_structure_integration.py" ] && TEST_PATHS="$TEST_PATHS tests/foundation/test_structure_integration.py"

          start_time=$(date +%s)
          if [ -n "$TEST_PATHS" ]; then
            echo "Running Python tests from: $TEST_PATHS"
            if pytest $TEST_PATHS --verbose --timeout=300 --maxfail=5; then
              end_time=$(date +%s)
              duration=$((end_time - start_time))
              echo '{"group": "Python Tests", "tests": 1, "passed": 1, "failed": 0, "duration": '"$duration"'}' > test-results/Python-Tests.json
            else
              end_time=$(date +%s)
              duration=$((end_time - start_time))
              echo '{"group": "Python Tests", "tests": 1, "passed": 0, "failed": 1, "duration": '"$duration"'}' > test-results/Python-Tests.json
              exit 1
            fi
          else
            echo "‚ö†Ô∏è  No Python tests found yet"
            echo '{"group": "Python Tests", "tests": 0, "passed": 0, "failed": 0, "duration": 0}' > test-results/Python-Tests.json
          fi

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: test-results-Python-Tests
          path: test-results/
          retention-days: 7

  # ============================================================================
  # Code Quality Analysis
  # ============================================================================
  code-quality:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    name: "Code Quality Analysis"

    steps:
      - name: Checkout code
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install analysis tools
        run: |
          pip install radon lizard

      - name: Analyze code metrics
        run: |
          echo "=================================================="
          echo "Code Quality Analysis"
          echo "=================================================="

          mkdir -p quality-reports

          # Python complexity analysis
          echo "## Python Code Complexity" > quality-reports/complexity.md
          echo "" >> quality-reports/complexity.md
          echo "Functions with complexity > 10 (consider refactoring):" >> quality-reports/complexity.md
          echo '```' >> quality-reports/complexity.md
          radon cc scripts/ -a -nc 2>/dev/null || echo "No complex functions found"
          echo '```' >> quality-reports/complexity.md

          # Mojo code statistics
          echo "" >> quality-reports/complexity.md
          echo "## Mojo Code Statistics" >> quality-reports/complexity.md
          echo "" >> quality-reports/complexity.md

          total_mojo_files=$(find shared tests -name "*.mojo" 2>/dev/null | wc -l)
          total_mojo_lines=$(find shared tests -name "*.mojo" -exec cat {} \; 2>/dev/null | wc -l)
          total_functions=$(grep -r "^fn " shared tests --include="*.mojo" 2>/dev/null | wc -l)
          total_structs=$(grep -r "^struct " shared tests --include="*.mojo" 2>/dev/null | wc -l)

          cat >> quality-reports/complexity.md << EOF
          | Metric | Count |
          |--------|-------|
          | Mojo Files | $total_mojo_files |
          | Lines of Code | $total_mojo_lines |
          | Functions (fn) | $total_functions |
          | Structs | $total_structs |
          EOF

          # Technical debt markers
          echo "" >> quality-reports/complexity.md
          echo "## Technical Debt Markers" >> quality-reports/complexity.md
          echo "" >> quality-reports/complexity.md

          todo_count=$(grep -r "TODO" shared tests scripts --include="*.mojo" --include="*.py" 2>/dev/null | wc -l)
          fixme_count=$(grep -r "FIXME" shared tests scripts --include="*.mojo" --include="*.py" 2>/dev/null | wc -l)

          cat >> quality-reports/complexity.md << EOF
          | Marker | Count |
          |--------|-------|
          | TODO | $todo_count |
          | FIXME | $fixme_count |
          EOF

          cat quality-reports/complexity.md

      - name: Upload quality reports
        uses: actions/upload-artifact@v6
        with:
          name: code-quality-reports
          path: quality-reports/
          retention-days: 30

  # ============================================================================
  # SIMD Analysis (Extended - manual trigger only)
  # ============================================================================
  simd-analysis:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    name: "SIMD Analysis"
    if: github.event_name == 'workflow_dispatch' && inputs.run_extended == true

    steps:
      - name: Checkout code
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8

      - name: Analyze SIMD usage
        run: |
          echo "=================================================="
          echo "SIMD Usage Analysis"
          echo "=================================================="

          mkdir -p simd-reports

          simd_count=$(grep -r "SIMD\[" shared --include="*.mojo" 2>/dev/null | wc -l)
          vectorize_count=$(grep -r "vectorize" shared --include="*.mojo" 2>/dev/null | wc -l)
          parallelize_count=$(grep -r "parallelize" shared --include="*.mojo" 2>/dev/null | wc -l)

          cat > simd-reports/analysis.md << EOF
          # SIMD Optimization Analysis

          ## Current Usage

          | Pattern | Count |
          |---------|-------|
          | SIMD[] declarations | $simd_count |
          | vectorize() calls | $vectorize_count |
          | parallelize() calls | $parallelize_count |

          ## Files with heavy loop usage (optimization candidates)
          EOF

          echo '```' >> simd-reports/analysis.md
          grep -rl "for .* in range" shared --include="*.mojo" 2>/dev/null | \
            xargs -I{} sh -c 'echo "{}": $(grep -c "for .* in range" "{}")' 2>/dev/null | \
            sort -t: -k2 -nr | head -10 >> simd-reports/analysis.md || echo "None found"
          echo '```' >> simd-reports/analysis.md

          cat simd-reports/analysis.md

      - name: Upload SIMD analysis
        uses: actions/upload-artifact@v6
        with:
          name: simd-analysis
          path: simd-reports/
          retention-days: 30

  # ============================================================================
  # Combined Test Report with Metrics
  # ============================================================================
  test-report:
    needs: [mojo-compilation, validate-test-coverage, test-mojo-comprehensive, test-configs, test-benchmarks, test-core-layers, test-python, code-quality]
    runs-on: ubuntu-latest
    if: always()
    name: "Test Report"

    steps:
      - name: Checkout code
        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8  # v6.0.1

      - name: Download all test results
        uses: actions/download-artifact@v7
        with:
          path: all-results/
          pattern: "*"
          merge-multiple: false

      - name: Generate combined report
        run: |
          echo "# üß™ Comprehensive Test Results" > test-report.md
          echo "" >> test-report.md
          echo "**Workflow Run**: [\`${{ github.run_number }}\`](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> test-report.md
          echo "**Commit**: \`${{ github.sha }}\`" >> test-report.md
          echo "**Date**: $(date -Iseconds)" >> test-report.md
          echo "" >> test-report.md

          total_groups=0
          passed_groups=0
          failed_groups=0
          total_tests=0
          total_passed=0
          total_failed=0
          total_duration=0

          echo "## Test Groups" >> test-report.md
          echo "" >> test-report.md
          echo "| Group | Tests | Passed | Failed | Duration |" >> test-report.md
          echo "|-------|-------|--------|--------|----------|" >> test-report.md

          # Process each result file
          for result_dir in all-results/test-results-*; do
            if [ -d "$result_dir" ]; then
              for json_file in "$result_dir"/*.json; do
                if [ -f "$json_file" ]; then
                  total_groups=$((total_groups + 1))

                  group=$(jq -r '.group' "$json_file" 2>/dev/null || echo "Unknown")
                  tests=$(jq -r '.tests' "$json_file" 2>/dev/null || echo "0")
                  passed=$(jq -r '.passed' "$json_file" 2>/dev/null || echo "0")
                  failed=$(jq -r '.failed' "$json_file" 2>/dev/null || echo "0")
                  duration=$(jq -r '.duration' "$json_file" 2>/dev/null || echo "0")

                  total_tests=$((total_tests + tests))
                  total_passed=$((total_passed + passed))
                  total_failed=$((total_failed + failed))
                  total_duration=$((total_duration + duration))

                  if [ "$failed" -gt 0 ]; then
                    failed_groups=$((failed_groups + 1))
                    status="‚ùå"
                  else
                    passed_groups=$((passed_groups + 1))
                    status="‚úÖ"
                  fi

                  echo "| $status $group | $tests | $passed | $failed | ${duration}s |" >> test-report.md
                fi
              done
            fi
          done

          echo "" >> test-report.md
          echo "## Summary" >> test-report.md
          echo "" >> test-report.md
          echo "| Metric | Value |" >> test-report.md
          echo "|--------|-------|" >> test-report.md
          echo "| Total Test Groups | $total_groups |" >> test-report.md
          echo "| Passed Groups | $passed_groups |" >> test-report.md
          echo "| Failed Groups | $failed_groups |" >> test-report.md
          echo "| Total Tests | $total_tests |" >> test-report.md
          echo "| Passed Tests | $total_passed |" >> test-report.md
          echo "| Failed Tests | $total_failed |" >> test-report.md
          echo "| Total Duration | ${total_duration}s |" >> test-report.md

          if [ $total_tests -gt 0 ]; then
            pass_rate=$(echo "scale=1; $total_passed * 100 / $total_tests" | bc 2>/dev/null || echo "0")
            echo "| Pass Rate | ${pass_rate}% |" >> test-report.md
          fi

          echo "" >> test-report.md

          if [ $total_failed -eq 0 ]; then
            echo "‚úÖ **All tests passed!**" >> test-report.md
          else
            echo "‚ùå **Some tests failed. See details above.**" >> test-report.md
          fi

          # Include code quality if available
          if [ -f "all-results/code-quality-reports/complexity.md" ]; then
            echo "" >> test-report.md
            echo "---" >> test-report.md
            echo "" >> test-report.md
            echo "## üìà Code Quality" >> test-report.md
            cat "all-results/code-quality-reports/complexity.md" >> test-report.md
          fi

          # Display report
          cat test-report.md

      - name: Upload combined report
        uses: actions/upload-artifact@v6
        with:
          name: comprehensive-test-report
          path: test-report.md
          retention-days: 30

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');

            try {
              const report = fs.readFileSync('test-report.md', 'utf8');

              // Check if a comment already exists
              const { data: comments } = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });

              const botComment = comments.find(comment =>
                comment.user.type === 'Bot' &&
                comment.body.includes('üß™ Comprehensive Test Results')
              );

              const commentBody = report;

              if (botComment) {
                // Update existing comment
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: botComment.id,
                  body: commentBody
                });
                console.log('Updated existing PR comment');
              } else {
                // Create new comment
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: commentBody
                });
                console.log('Created new PR comment');
              }
            } catch (error) {
              console.error('Failed to comment on PR:', error);
            }

      - name: Fail if any tests failed
        run: |
          # Check if any test groups failed
          failed=0
          for result_dir in all-results/test-results-*; do
            if [ -d "$result_dir" ]; then
              for json_file in "$result_dir"/*.json; do
                if [ -f "$json_file" ]; then
                  failed_count=$(jq -r '.failed' "$json_file" 2>/dev/null || echo "0")
                  if [ "$failed_count" -gt 0 ]; then
                    failed=1
                    break
                  fi
                fi
              done
            fi
          done

          if [ $failed -eq 1 ]; then
            echo "‚ùå FAIL: Some tests failed. See test report for details."
            exit 1
          else
            echo "‚úÖ All tests passed!"
            exit 0
          fi
