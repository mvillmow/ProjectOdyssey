name: Performance Benchmarks

# Execute performance benchmarks and detect regressions
# Target duration: < 30 minutes (parallel execution)

on:
  # Manual dispatch (primary trigger)
  workflow_dispatch:
    inputs:
      suite:
        description: 'Benchmark suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - tensor-ops
          - model-training
          - data-loading

  # Scheduled nightly run at 2 AM UTC
  schedule:
    - cron: '0 2 * * *'

  # Pull requests with 'benchmark' label
  pull_request:
    types: [labeled, synchronize]

permissions:
  contents: read
  pull-requests: write  # For PR comments

jobs:
  # ============================================================================
  # Benchmark Execution - Matrix Strategy
  # ============================================================================
  benchmark-execution:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    # Only run if 'benchmark' label is present or scheduled/manual
    if: |
      github.event_name == 'workflow_dispatch' ||
      github.event_name == 'schedule' ||
      (github.event_name == 'pull_request' && contains(github.event.pull_request.labels.*.name, 'benchmark'))

    strategy:
      fail-fast: false
      matrix:
        benchmark-suite:
          - tensor-ops
          - model-training
          - data-loading

    steps:
      - name: Checkout code
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1

      - name: Set up Pixi
        uses: prefix-dev/setup-pixi@v0.8.1
        with:
          pixi-version: latest
          cache: true

      - name: Cache Pixi environments
        uses: actions/cache@v4
        with:
          path: ~/.pixi
          key: pixi-${{ runner.os }}-${{ hashFiles('pixi.toml') }}
          restore-keys: |
            pixi-${{ runner.os }}-

      - name: Check if should run this suite
        id: check-suite
        run: |
          REQUESTED_SUITE="${{ github.event.inputs.suite || 'all' }}"
          CURRENT_SUITE="${{ matrix.benchmark-suite }}"

          if [ "$REQUESTED_SUITE" = "all" ] || [ "$REQUESTED_SUITE" = "$CURRENT_SUITE" ]; then
            echo "run=true" >> $GITHUB_OUTPUT
          else
            echo "run=false" >> $GITHUB_OUTPUT
            echo "Skipping $CURRENT_SUITE (requested: $REQUESTED_SUITE)"
          fi

      - name: Run benchmark suite - ${{ matrix.benchmark-suite }}
        if: steps.check-suite.outputs.run == 'true'
        run: |
          echo "Running benchmark suite: ${{ matrix.benchmark-suite }}"

          # Check if benchmarks directory exists
          if [ ! -d "benchmarks" ]; then
            echo "⚠️  No benchmarks/ directory found yet"
            echo "This is expected during initial development."
            mkdir -p benchmark-results
            echo "No benchmarks implemented yet" > benchmark-results/${{ matrix.benchmark-suite }}.json
            exit 0
          fi

          # Check if this benchmark suite exists
          SUITE_PATH="benchmarks/${{ matrix.benchmark-suite }}"

          if [ -d "$SUITE_PATH" ]; then
            # Run Python benchmarks
            if [ -f "$SUITE_PATH/run_benchmark.py" ]; then
              echo "Running Python benchmark..."
              pixi run python "$SUITE_PATH/run_benchmark.py" --output benchmark-results/${{ matrix.benchmark-suite }}.json
            # Run Mojo benchmarks
            elif [ -f "$SUITE_PATH/run_benchmark.mojo" ]; then
              echo "Running Mojo benchmark..."
              pixi run mojo "$SUITE_PATH/run_benchmark.mojo" --output benchmark-results/${{ matrix.benchmark-suite }}.json
            else
              echo "⚠️  No benchmark runner found in $SUITE_PATH"
              mkdir -p benchmark-results
              echo '{"suite": "${{ matrix.benchmark-suite }}", "status": "not_implemented"}' > benchmark-results/${{ matrix.benchmark-suite }}.json
            fi
          else
            echo "⚠️  Benchmark suite not found: $SUITE_PATH"
            mkdir -p benchmark-results
            echo '{"suite": "${{ matrix.benchmark-suite }}", "status": "not_found"}' > benchmark-results/${{ matrix.benchmark-suite }}.json
          fi

          echo "✅ Benchmark suite complete: ${{ matrix.benchmark-suite }}"

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.benchmark-suite }}
          path: benchmark-results/
          retention-days: 90  # Keep benchmark history longer

  # ============================================================================
  # Regression Detection
  # ============================================================================
  regression-detection:
    needs: benchmark-execution
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Download current benchmark results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: current-results/

      - name: Download baseline results from main branch
        continue-on-error: true
        run: |
          echo "Downloading baseline benchmark results from main branch..."

          # Create baseline directory
          mkdir -p baseline-results

          # Try to download from latest workflow run on main
          # This is a placeholder - in production, you'd fetch from a specific artifact
          echo "⚠️  Baseline comparison not yet implemented"
          echo "Future: Compare against main branch baseline results"

      - name: Compare performance
        run: |
          echo "Comparing performance against baseline..."

          # Check if current results exist
          if [ ! -d "current-results" ] || [ -z "$(ls -A current-results)" ]; then
            echo "⚠️  No benchmark results to compare"
            exit 0
          fi

          echo "Current results available. Baseline comparison will be implemented."

      - name: Detect regressions
        id: detect-regressions
        run: |
          echo "Detecting performance regressions..."

          # Regression thresholds:
          # - Critical: > 25% slower (block merge)
          # - High: 10-25% slower (require justification)
          # - Medium: 5-10% slower (warning)

          CRITICAL_REGRESSIONS=0
          HIGH_REGRESSIONS=0
          MEDIUM_REGRESSIONS=0

          echo "critical=$CRITICAL_REGRESSIONS" >> $GITHUB_OUTPUT
          echo "high=$HIGH_REGRESSIONS" >> $GITHUB_OUTPUT
          echo "medium=$MEDIUM_REGRESSIONS" >> $GITHUB_OUTPUT

          echo "✅ No regressions detected (baseline comparison not yet implemented)"

      - name: Generate regression report
        run: |
          mkdir -p regression-results
          echo "Regression analysis pending implementation" > regression-results/report.txt

      - name: Upload regression results
        uses: actions/upload-artifact@v4
        with:
          name: regression-results
          path: regression-results/
          retention-days: 90

  # ============================================================================
  # Benchmark Report
  # ============================================================================
  benchmark-report:
    needs: [benchmark-execution, regression-detection]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1

      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: all-results/

      - name: Download regression results
        uses: actions/download-artifact@v4
        with:
          name: regression-results
          path: regression-results/

      - name: Generate benchmark report
        run: |
          echo "# ⚡ Performance Benchmark Results" > benchmark-report.md
          echo "" >> benchmark-report.md
          echo "**Workflow Run**: [\`${{ github.run_number }}\`](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> benchmark-report.md
          echo "**Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> benchmark-report.md
          echo "" >> benchmark-report.md

          # List benchmark suites
          echo "## Benchmark Suites" >> benchmark-report.md
          echo "" >> benchmark-report.md

          for suite in tensor-ops model-training data-loading; do
            RESULT_FILE="all-results/benchmark-results-$suite/$suite.json"

            if [ -f "$RESULT_FILE" ]; then
              # Check if benchmark was implemented
              if grep -q "not_implemented\|not_found" "$RESULT_FILE"; then
                echo "- ⚠️  **$suite**: Not yet implemented" >> benchmark-report.md
              else
                echo "- ✅ **$suite**: Completed" >> benchmark-report.md
              fi
            else
              echo "- ❌ **$suite**: Failed or did not run" >> benchmark-report.md
            fi
          done

          echo "" >> benchmark-report.md

          # Regression summary
          echo "## Regression Analysis" >> benchmark-report.md
          echo "" >> benchmark-report.md
          echo "⚠️  Baseline comparison not yet implemented" >> benchmark-report.md
          echo "" >> benchmark-report.md
          echo "Future: Performance regression detection against main branch baseline" >> benchmark-report.md
          echo "" >> benchmark-report.md

          # Status
          echo "## Status" >> benchmark-report.md
          echo "" >> benchmark-report.md

          if [ "${{ needs.benchmark-execution.result }}" = "success" ]; then
            echo "✅ **Benchmark execution completed successfully**" >> benchmark-report.md
          else
            echo "❌ **Some benchmarks failed**" >> benchmark-report.md
          fi

          # Display report
          cat benchmark-report.md

      - name: Upload benchmark report
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report
          path: benchmark-report.md
          retention-days: 90

      - name: Update benchmark history
        run: |
          echo "Updating benchmark history..."
          echo "Future: Store benchmark results in a database or artifact store"

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            try {
              const report = fs.readFileSync('benchmark-report.md', 'utf8');

              // Check if a comment already exists
              const { data: comments } = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });

              const botComment = comments.find(comment =>
                comment.user.type === 'Bot' &&
                comment.body.includes('⚡ Performance Benchmark Results')
              );

              if (botComment) {
                // Update existing comment
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: botComment.id,
                  body: report
                });
                console.log('Updated existing PR comment');
              } else {
                // Create new comment
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: report
                });
                console.log('Created new PR comment');
              }
            } catch (error) {
              console.error('Failed to comment on PR:', error);
            }

      - name: Fail if critical regressions detected
        run: |
          # Check for critical regressions
          CRITICAL="${{ needs.regression-detection.outputs.critical || '0' }}"

          if [ "$CRITICAL" != "0" ]; then
            echo "❌ CRITICAL: $CRITICAL critical performance regressions detected (> 25% slower)"
            exit 1
          fi

          echo "✅ No critical performance regressions"
