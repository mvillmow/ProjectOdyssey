"""Element-wise mathematical operations for ExTensor.

Implements mathematical functions like exp, log, sqrt, trigonometric functions, etc.
"""

from collections import List
from .extensor import ExTensor
from .dtype_dispatch import (
    dispatch_unary,
    dispatch_binary,
    dispatch_float_unary,
    dispatch_float_binary,
)
from .broadcasting import broadcast_shapes, compute_broadcast_strides
from math import sqrt as math_sqrt
from math import exp as math_exp
from math import log as math_log
from math import sin as math_sin
from math import cos as math_cos
from math import tanh as math_tanh
from math import ceil as math_ceil
from math import floor as math_floor
from math import trunc as math_trunc
from memory import UnsafePointer


@always_inline
fn math_round[T: DType](x: Scalar[T]) -> Scalar[T]:
    """Round to nearest integer (banker's rounding - round half to even).

    Examples:
            -2.5 → -2.0 (nearest even)
            -1.4 → -1.0
             0.5 →  0.0 (nearest even)
             1.4 →  1.0
             2.5 →  2.0 (nearest even)
    """
    var floor_val = math_floor(x)
    var frac = x - floor_val

    # If exactly 0.5, round to nearest even integer
    if frac == Scalar[T](0.5):
        # Check if floor_val is even
        var floor_int = Int(floor_val)
        if floor_int % 2 == 0:
            return floor_val  # Already even, round down
        else:
            return floor_val + Scalar[T](1.0)  # Round up to even
    # For -0.5, need to check ceiling
    elif frac == Scalar[T](-0.5):
        var ceil_val = math_ceil(x)
        var ceil_int = Int(ceil_val)
        if ceil_int % 2 == 0:
            return ceil_val  # Round up to even
        else:
            return floor_val  # Round down to even
    # Otherwise, standard rounding
    elif frac >= Scalar[T](0.5):
        return math_ceil(x)
    else:
        return floor_val


# ============================================================================
# Unary Operations (Element-wise)
# ============================================================================


@always_inline
fn _abs_op[T: DType](x: Scalar[T]) -> Scalar[T]:
    """Absolute value operation."""
    if x >= Scalar[T](0):
        return x
    else:
        return -x


fn abs(tensor: ExTensor) raises -> ExTensor:
    """Absolute value element-wise.

    Args:
            tensor: Input tensor.

    Returns:
            A new tensor with absolute values.

    Examples:
    ```
            var a = full(shape, -3.0, DType.float32)
            var b = abs(a)  # All values become 3.0
    ```
    """
    return dispatch_unary[_abs_op](tensor)


@always_inline
fn _sign_op[T: DType](x: Scalar[T]) -> Scalar[T]:
    """Sign operation: -1, 0, or 1."""
    if x > Scalar[T](0):
        return Scalar[T](1)
    elif x < Scalar[T](0):
        return Scalar[T](-1)
    else:
        return Scalar[T](0)


fn sign(tensor: ExTensor) raises -> ExTensor:
    """Sign function element-wise (-1, 0, or 1).

    Args:
            tensor: Input tensor.

    Returns:
            A new tensor with sign values (-1 for negative, 0 for zero, 1 for positive).

    Examples:
    ```
            var a = tensor([-2.0, 0.0, 3.0])
            var b = sign(a)  # [-1.0, 0.0, 1.0]
    ```
    """
    return dispatch_unary[_sign_op](tensor)


@always_inline
fn _exp_op[T: DType](x: Scalar[T]) -> Scalar[T]:
    """Exponential operation."""

    @parameter
    if T == DType.float16 or T == DType.float32:
        return Scalar[T](math_exp(Float32(x)))
    else:
        return Scalar[T](math_exp(Float64(x)))


fn exp(tensor: ExTensor) raises -> ExTensor:
    """Exponential function element-wise (e^x).

    Args:
            tensor: Input tensor.

    Returns:
            A new tensor with exponential values.

    Examples:
    ```
            var a = zeros(shape, DType.float32)
            var b = exp(a)  # All values become 1.0 (e^0)
    ```
    """
    return dispatch_float_unary[_exp_op](tensor)


@always_inline
fn _log_op[T: DType](x: Scalar[T]) -> Scalar[T]:
    """Natural logarithm operation."""

    @parameter
    if T == DType.float16 or T == DType.float32:
        return Scalar[T](math_log(Float32(x)))
    else:
        return Scalar[T](math_log(Float64(x)))


fn log(tensor: ExTensor) raises -> ExTensor:
    """Natural logarithm element-wise (ln(x)).

    Args:
            tensor: Input tensor (must have positive values).

    Returns:
            A new tensor with logarithm values.

    Raises:
            Error: If any value is <= 0.

    Examples:
    ```
            var a = ones(shape, DType.float32)
            var b = log(a)  # All values become 0.0 (ln(1))
    ```
    """
    return dispatch_float_unary[_log_op](tensor)


@always_inline
fn _sqrt_op[T: DType](x: Scalar[T]) -> Scalar[T]:
    """Square root operation."""

    @parameter
    if T == DType.float16 or T == DType.float32:
        return Scalar[T](math_sqrt(Float32(x)))
    else:
        return Scalar[T](math_sqrt(Float64(x)))


fn sqrt(tensor: ExTensor) raises -> ExTensor:
    """Square root element-wise.

    Args:
            tensor: Input tensor (must have non-negative values).

    Returns:
            A new tensor with square root values.

    Raises:
            Error: If any value is < 0.

    Examples:
    ```
            var a = full(shape, 4.0, DType.float32)
            var b = sqrt(a)  # All values become 2.0
    ```
    """
    return dispatch_float_unary[_sqrt_op](tensor)


@always_inline
fn _sin_op[T: DType](x: Scalar[T]) -> Scalar[T]:
    """Sine operation."""

    @parameter
    if T == DType.float16 or T == DType.float32:
        return Scalar[T](math_sin(Float32(x)))
    else:
        return Scalar[T](math_sin(Float64(x)))


fn sin(tensor: ExTensor) raises -> ExTensor:
    """Sine function element-wise.

    Args:
            tensor: Input tensor (values in radians).

    Returns:
            A new tensor with sine values.

    Examples:
    ```
            var a = zeros(shape, DType.float32)
            var b = sin(a)  # All values become 0.0 (sin(0))
    ```
    """
    return dispatch_float_unary[_sin_op](tensor)


@always_inline
fn _cos_op[T: DType](x: Scalar[T]) -> Scalar[T]:
    """Cosine operation."""

    @parameter
    if T == DType.float16 or T == DType.float32:
        return Scalar[T](math_cos(Float32(x)))
    else:
        return Scalar[T](math_cos(Float64(x)))


fn cos(tensor: ExTensor) raises -> ExTensor:
    """Cosine function element-wise.

    Args:
            tensor: Input tensor (values in radians).

    Returns:
            A new tensor with cosine values.

    Examples:
    ```
            var a = zeros(shape, DType.float32)
            var b = cos(a)  # All values become 1.0 (cos(0))
    ```
    """
    return dispatch_float_unary[_cos_op](tensor)


@always_inline
fn _tanh_op[T: DType](x: Scalar[T]) -> Scalar[T]:
    """Hyperbolic tangent operation."""

    @parameter
    if T == DType.float16 or T == DType.float32:
        return Scalar[T](math_tanh(Float32(x)))
    else:
        return Scalar[T](math_tanh(Float64(x)))


fn tanh(tensor: ExTensor) raises -> ExTensor:
    """Hyperbolic tangent function element-wise.

    Args:
            tensor: Input tensor.

    Returns:
            A new tensor with tanh values (range: -1 to 1).

    Examples:
    ```
            var a = zeros(shape, DType.float32)
            var b = tanh(a)  # All values become 0.0 (tanh(0))
    ```
    """
    return dispatch_float_unary[_tanh_op](tensor)


# ============================================================================
# Dtype-specialized forward pass helpers
# ============================================================================


fn _clip_forward_impl[
    dtype: DType
](result: ExTensor, tensor: ExTensor, min_val: Float64, max_val: Float64):
    """Dtype-specialized clip forward: clamp values to [min, max]."""
    var min_t = Scalar[dtype](min_val)
    var max_t = Scalar[dtype](max_val)
    var in_ptr = tensor._data.bitcast[Scalar[dtype]]()
    var out_ptr = result._data.bitcast[Scalar[dtype]]()
    for i in range(tensor.numel()):
        var val = in_ptr[i]
        if val < min_t:
            out_ptr[i] = min_t
        elif val > max_t:
            out_ptr[i] = max_t
        else:
            out_ptr[i] = val


fn _dispatch_clip_forward(
    result: ExTensor, tensor: ExTensor, min_val: Float64, max_val: Float64
) raises:
    """Runtime dispatch for clip forward pass."""
    var dtype = tensor.dtype()
    if dtype == DType.float16:
        _clip_forward_impl[DType.float16](result, tensor, min_val, max_val)
    elif dtype == DType.float32:
        _clip_forward_impl[DType.float32](result, tensor, min_val, max_val)
    elif dtype == DType.float64:
        _clip_forward_impl[DType.float64](result, tensor, min_val, max_val)
    elif dtype == DType.int8:
        _clip_forward_impl[DType.int8](result, tensor, min_val, max_val)
    elif dtype == DType.int16:
        _clip_forward_impl[DType.int16](result, tensor, min_val, max_val)
    elif dtype == DType.int32:
        _clip_forward_impl[DType.int32](result, tensor, min_val, max_val)
    elif dtype == DType.int64:
        _clip_forward_impl[DType.int64](result, tensor, min_val, max_val)
    else:
        raise Error("clip: unsupported dtype")


fn _log10_forward_impl[
    dtype: DType
](result: ExTensor, tensor: ExTensor, numel: Int) raises:
    """Dtype-specialized log10 forward: log(x) / log(10)."""
    alias ln10 = Scalar[dtype](2.302585092994046)
    var in_ptr = tensor._data.bitcast[Scalar[dtype]]()
    var out_ptr = result._data.bitcast[Scalar[dtype]]()
    for i in range(numel):
        var val = in_ptr[i]
        if val <= Scalar[dtype](0):
            raise Error("log10 requires positive values")
        out_ptr[i] = math_log(val) / ln10


fn _dispatch_log10_forward(
    result: ExTensor, tensor: ExTensor, numel: Int
) raises:
    """Runtime dispatch for log10 forward pass."""
    var dtype = tensor.dtype()
    if dtype == DType.float16:
        _log10_forward_impl[DType.float16](result, tensor, numel)
    elif dtype == DType.float32:
        _log10_forward_impl[DType.float32](result, tensor, numel)
    elif dtype == DType.float64:
        _log10_forward_impl[DType.float64](result, tensor, numel)
    else:
        raise Error("log10: unsupported dtype (requires float type)")


fn _log2_forward_impl[
    dtype: DType
](result: ExTensor, tensor: ExTensor, numel: Int) raises:
    """Dtype-specialized log2 forward: log(x) / log(2)."""
    alias ln2 = Scalar[dtype](0.6931471805599453)
    var in_ptr = tensor._data.bitcast[Scalar[dtype]]()
    var out_ptr = result._data.bitcast[Scalar[dtype]]()
    for i in range(numel):
        var val = in_ptr[i]
        if val <= Scalar[dtype](0):
            raise Error("log2 requires positive values")
        out_ptr[i] = math_log(val) / ln2


fn _dispatch_log2_forward(
    result: ExTensor, tensor: ExTensor, numel: Int
) raises:
    """Runtime dispatch for log2 forward pass."""
    var dtype = tensor.dtype()
    if dtype == DType.float16:
        _log2_forward_impl[DType.float16](result, tensor, numel)
    elif dtype == DType.float32:
        _log2_forward_impl[DType.float32](result, tensor, numel)
    elif dtype == DType.float64:
        _log2_forward_impl[DType.float64](result, tensor, numel)
    else:
        raise Error("log2: unsupported dtype (requires float type)")


fn _logical_and_impl[
    dtype: DType
](
    result: ExTensor,
    a: ExTensor,
    b: ExTensor,
    strides_a: List[Int],
    strides_b: List[Int],
    result_shape: List[Int],
    total_elems: Int,
):
    """Dtype-specialized logical AND."""
    alias zero = Scalar[dtype](0)
    alias one = Scalar[dtype](1)
    var a_ptr = a._data.bitcast[Scalar[dtype]]()
    var b_ptr = b._data.bitcast[Scalar[dtype]]()
    var out_ptr = result._data.bitcast[Scalar[dtype]]()

    for result_idx in range(total_elems):
        # Convert result_idx to multi-dimensional index
        var remaining = result_idx
        var idx_a = 0
        var idx_b = 0
        for d in range(len(result_shape) - 1, -1, -1):
            var coord = remaining % result_shape[d]
            remaining //= result_shape[d]
            idx_a += coord * strides_a[d]
            idx_b += coord * strides_b[d]

        var val_a = a_ptr[idx_a]
        var val_b = b_ptr[idx_b]
        var bool_result = val_a != zero and val_b != zero
        out_ptr[result_idx] = one if bool_result else zero


fn _dispatch_logical_and(
    result: ExTensor,
    a: ExTensor,
    b: ExTensor,
    strides_a: List[Int],
    strides_b: List[Int],
    result_shape: List[Int],
    total_elems: Int,
) raises:
    """Runtime dispatch for logical AND."""
    var dtype = a.dtype()
    if dtype == DType.float16:
        _logical_and_impl[DType.float16](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.float32:
        _logical_and_impl[DType.float32](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.float64:
        _logical_and_impl[DType.float64](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.int8:
        _logical_and_impl[DType.int8](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.int16:
        _logical_and_impl[DType.int16](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.int32:
        _logical_and_impl[DType.int32](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.int64:
        _logical_and_impl[DType.int64](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    else:
        raise Error("logical_and: unsupported dtype")


fn _logical_or_impl[
    dtype: DType
](
    result: ExTensor,
    a: ExTensor,
    b: ExTensor,
    strides_a: List[Int],
    strides_b: List[Int],
    result_shape: List[Int],
    total_elems: Int,
):
    """Dtype-specialized logical OR."""
    alias zero = Scalar[dtype](0)
    alias one = Scalar[dtype](1)
    var a_ptr = a._data.bitcast[Scalar[dtype]]()
    var b_ptr = b._data.bitcast[Scalar[dtype]]()
    var out_ptr = result._data.bitcast[Scalar[dtype]]()

    for result_idx in range(total_elems):
        var remaining = result_idx
        var idx_a = 0
        var idx_b = 0
        for d in range(len(result_shape) - 1, -1, -1):
            var coord = remaining % result_shape[d]
            remaining //= result_shape[d]
            idx_a += coord * strides_a[d]
            idx_b += coord * strides_b[d]

        var val_a = a_ptr[idx_a]
        var val_b = b_ptr[idx_b]
        var bool_result = val_a != zero or val_b != zero
        out_ptr[result_idx] = one if bool_result else zero


fn _dispatch_logical_or(
    result: ExTensor,
    a: ExTensor,
    b: ExTensor,
    strides_a: List[Int],
    strides_b: List[Int],
    result_shape: List[Int],
    total_elems: Int,
) raises:
    """Runtime dispatch for logical OR."""
    var dtype = a.dtype()
    if dtype == DType.float16:
        _logical_or_impl[DType.float16](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.float32:
        _logical_or_impl[DType.float32](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.float64:
        _logical_or_impl[DType.float64](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.int8:
        _logical_or_impl[DType.int8](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.int16:
        _logical_or_impl[DType.int16](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.int32:
        _logical_or_impl[DType.int32](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.int64:
        _logical_or_impl[DType.int64](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    else:
        raise Error("logical_or: unsupported dtype")


fn _logical_not_impl[
    dtype: DType
](result: ExTensor, tensor: ExTensor, numel: Int):
    """Dtype-specialized logical NOT."""
    alias zero = Scalar[dtype](0)
    alias one = Scalar[dtype](1)
    var in_ptr = tensor._data.bitcast[Scalar[dtype]]()
    var out_ptr = result._data.bitcast[Scalar[dtype]]()
    for i in range(numel):
        var bool_result = in_ptr[i] == zero
        out_ptr[i] = one if bool_result else zero


fn _dispatch_logical_not(result: ExTensor, tensor: ExTensor, numel: Int) raises:
    """Runtime dispatch for logical NOT."""
    var dtype = tensor.dtype()
    if dtype == DType.float16:
        _logical_not_impl[DType.float16](result, tensor, numel)
    elif dtype == DType.float32:
        _logical_not_impl[DType.float32](result, tensor, numel)
    elif dtype == DType.float64:
        _logical_not_impl[DType.float64](result, tensor, numel)
    elif dtype == DType.int8:
        _logical_not_impl[DType.int8](result, tensor, numel)
    elif dtype == DType.int16:
        _logical_not_impl[DType.int16](result, tensor, numel)
    elif dtype == DType.int32:
        _logical_not_impl[DType.int32](result, tensor, numel)
    elif dtype == DType.int64:
        _logical_not_impl[DType.int64](result, tensor, numel)
    else:
        raise Error("logical_not: unsupported dtype")


fn _logical_xor_impl[
    dtype: DType
](
    result: ExTensor,
    a: ExTensor,
    b: ExTensor,
    strides_a: List[Int],
    strides_b: List[Int],
    result_shape: List[Int],
    total_elems: Int,
):
    """Dtype-specialized logical XOR."""
    alias zero = Scalar[dtype](0)
    alias one = Scalar[dtype](1)
    var a_ptr = a._data.bitcast[Scalar[dtype]]()
    var b_ptr = b._data.bitcast[Scalar[dtype]]()
    var out_ptr = result._data.bitcast[Scalar[dtype]]()

    for result_idx in range(total_elems):
        var remaining = result_idx
        var idx_a = 0
        var idx_b = 0
        for d in range(len(result_shape) - 1, -1, -1):
            var coord = remaining % result_shape[d]
            remaining //= result_shape[d]
            idx_a += coord * strides_a[d]
            idx_b += coord * strides_b[d]

        var val_a = a_ptr[idx_a]
        var val_b = b_ptr[idx_b]
        var bool_a = val_a != zero
        var bool_b = val_b != zero
        # XOR: True if exactly one is True
        var bool_result = (bool_a and not bool_b) or (not bool_a and bool_b)
        out_ptr[result_idx] = one if bool_result else zero


fn _dispatch_logical_xor(
    result: ExTensor,
    a: ExTensor,
    b: ExTensor,
    strides_a: List[Int],
    strides_b: List[Int],
    result_shape: List[Int],
    total_elems: Int,
) raises:
    """Runtime dispatch for logical XOR."""
    var dtype = a.dtype()
    if dtype == DType.float16:
        _logical_xor_impl[DType.float16](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.float32:
        _logical_xor_impl[DType.float32](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.float64:
        _logical_xor_impl[DType.float64](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.int8:
        _logical_xor_impl[DType.int8](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.int16:
        _logical_xor_impl[DType.int16](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.int32:
        _logical_xor_impl[DType.int32](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    elif dtype == DType.int64:
        _logical_xor_impl[DType.int64](
            result, a, b, strides_a, strides_b, result_shape, total_elems
        )
    else:
        raise Error("logical_xor: unsupported dtype")


fn clip(
    tensor: ExTensor, min_val: Float64, max_val: Float64
) raises -> ExTensor:
    """Clip (clamp) values to a range element-wise.

    Args:
            tensor: Input tensor.
            min_val: Minimum value.
            max_val: Maximum value.

    Returns:
            A new tensor with clipped values.

    Raises:
            Error: If min_val > max_val.

    Examples:
    ```
            var a = tensor([-5.0, 0.0, 10.0])
            var b = clip(a, 0.0, 5.0)  # [0.0, 0.0, 5.0]
    ```
    """
    if min_val > max_val:
        raise Error("clip requires min_val <= max_val")

    var result = ExTensor(tensor.shape(), tensor.dtype())
    _dispatch_clip_forward(result, tensor, min_val, max_val)
    return result^


# ============================================================================
# Rounding operations
# ============================================================================


@always_inline
fn _ceil_op[T: DType](x: Scalar[T]) -> Scalar[T]:
    """Ceiling operation."""

    @parameter
    if T == DType.float16 or T == DType.float32:
        return Scalar[T](math_ceil(Float32(x)))
    else:
        return Scalar[T](math_ceil(Float64(x)))


fn ceil(tensor: ExTensor) raises -> ExTensor:
    """Ceiling function element-wise (round up to nearest integer).

    Args:
            tensor: Input tensor.

    Returns:
            A new tensor with ceiling values.

    Examples:
    ```
            var a = tensor([1.2, 2.5, 3.9])
            var b = ceil(a)  # [2.0, 3.0, 4.0]
    ```
    """
    return dispatch_float_unary[_ceil_op](tensor)


@always_inline
fn _floor_op[T: DType](x: Scalar[T]) -> Scalar[T]:
    """Floor operation."""

    @parameter
    if T == DType.float16 or T == DType.float32:
        return Scalar[T](math_floor(Float32(x)))
    else:
        return Scalar[T](math_floor(Float64(x)))


fn floor(tensor: ExTensor) raises -> ExTensor:
    """Floor function element-wise (round down to nearest integer).

    Args:
            tensor: Input tensor.

    Returns:
            A new tensor with floor values.

    Examples:
    ```
            var a = tensor([1.2, 2.5, 3.9])
            var b = floor(a)  # [1.0, 2.0, 3.0]
    ```
    """
    return dispatch_float_unary[_floor_op](tensor)


@always_inline
fn _round_op[T: DType](x: Scalar[T]) -> Scalar[T]:
    """Round operation."""

    @parameter
    if T == DType.float16 or T == DType.float32:
        return Scalar[T](math_round(Float32(x)))
    else:
        return Scalar[T](math_round(Float64(x)))


fn round(tensor: ExTensor) raises -> ExTensor:
    """Round to nearest integer element-wise.

    Args:
            tensor: Input tensor.

    Returns:
            A new tensor with rounded values.

    Examples:
    ```
            var a = tensor([1.2, 2.5, 3.9])
            var b = round(a)  # [1.0, 2.0, 4.0] (or [1.0, 3.0, 4.0] depending on rounding mode)
    ```
    """
    return dispatch_float_unary[_round_op](tensor)


@always_inline
fn _trunc_op[T: DType](x: Scalar[T]) -> Scalar[T]:
    """Truncate operation."""

    @parameter
    if T == DType.float16 or T == DType.float32:
        return Scalar[T](math_trunc(Float32(x)))
    else:
        return Scalar[T](math_trunc(Float64(x)))


fn trunc(tensor: ExTensor) raises -> ExTensor:
    """Truncate to integer element-wise (round toward zero).

    Args:
            tensor: Input tensor.

    Returns:
            A new tensor with truncated values.

    Examples:
    ```
            var a = tensor([1.9, -2.9, 3.1])
            var b = trunc(a)  # [1.0, -2.0, 3.0]
    ```
    """
    return dispatch_float_unary[_trunc_op](tensor)


# ============================================================================
# Logical operations
# ============================================================================


fn logical_and(a: ExTensor, b: ExTensor) raises -> ExTensor:
    """Logical AND element-wise with broadcasting.

    Args:
            a: First input tensor.
            b: Second input tensor.

    Returns:
            Boolean tensor (True where both are non-zero).

    Raises:
            Error: If shapes are not broadcast-compatible or dtypes don't match.

        Broadcasting:
            Shapes are broadcast to a common output shape following NumPy rules.
            Dimensions are compatible if they are equal or one is 1.

    Examples:
    ```
            var a = tensor([0.0, 1.0, 2.0])
            var b = tensor([0.0, 0.0, 1.0])
            var c = logical_and(a, b)  # [False, False, True]

            # Broadcasting example
            var x = ones([3, 1, 5], DType.float32)
            var y = ones([3, 4, 5], DType.float32)
            var z = logical_and(x, y)  # Shape (3, 4, 5)
    ```
    """
    if a.dtype() != b.dtype():
        raise Error("logical_and: tensors must have same dtype")

    # Compute broadcast shape
    var result_shape = broadcast_shapes(a.shape(), b.shape())
    var result = ExTensor(result_shape, a.dtype())

    # Compute broadcast strides
    var strides_a = compute_broadcast_strides(a.shape(), result_shape)
    var strides_b = compute_broadcast_strides(b.shape(), result_shape)

    # Calculate total elements in result
    var total_elems = 1
    for i in range(len(result_shape)):
        total_elems *= result_shape[i]

    _dispatch_logical_and(
        result, a, b, strides_a, strides_b, result_shape, total_elems
    )
    return result^


fn logical_or(a: ExTensor, b: ExTensor) raises -> ExTensor:
    """Logical OR element-wise with broadcasting.

    Args:
            a: First input tensor.
            b: Second input tensor.

    Returns:
            Boolean tensor (True where either is non-zero).

    Raises:
            Error: If shapes are not broadcast-compatible or dtypes don't match.

        Broadcasting:
            Shapes are broadcast to a common output shape following NumPy rules.
            Dimensions are compatible if they are equal or one is 1.

    Examples:
    ```
            var a = tensor([0.0, 1.0, 2.0])
            var b = tensor([0.0, 0.0, 1.0])
            var c = logical_or(a, b)  # [False, True, True]

            # Broadcasting example
            var x = ones([3, 1, 5], DType.float32)
            var y = ones([3, 4, 5], DType.float32)
            var z = logical_or(x, y)  # Shape (3, 4, 5)
    ```
    """
    if a.dtype() != b.dtype():
        raise Error("logical_or: tensors must have same dtype")

    # Compute broadcast shape
    var result_shape = broadcast_shapes(a.shape(), b.shape())
    var result = ExTensor(result_shape, a.dtype())

    # Compute broadcast strides
    var strides_a = compute_broadcast_strides(a.shape(), result_shape)
    var strides_b = compute_broadcast_strides(b.shape(), result_shape)

    # Calculate total elements in result
    var total_elems = 1
    for i in range(len(result_shape)):
        total_elems *= result_shape[i]

    _dispatch_logical_or(
        result, a, b, strides_a, strides_b, result_shape, total_elems
    )
    return result^


fn logical_not(tensor: ExTensor) raises -> ExTensor:
    """Logical NOT element-wise.

    Args:
            tensor: Input tensor.

    Returns:
            Boolean tensor (True where input is zero).

    Examples:
    ```
            var a = tensor([0.0, 1.0, 2.0])
            var b = logical_not(a)  # [True, False, False]
    ```
    """
    var result = ExTensor(tensor.shape(), tensor.dtype())
    _dispatch_logical_not(result, tensor, tensor.numel())
    return result^


fn logical_xor(a: ExTensor, b: ExTensor) raises -> ExTensor:
    """Logical XOR element-wise with broadcasting.

    Args:
            a: First input tensor.
            b: Second input tensor.

    Returns:
            Boolean tensor (True where exactly one is non-zero).

    Raises:
            Error: If shapes are not broadcast-compatible or dtypes don't match.

        Broadcasting:
            Shapes are broadcast to a common output shape following NumPy rules.
            Dimensions are compatible if they are equal or one is 1.

    Examples:
    ```
            var a = tensor([0.0, 1.0, 0.0, 1.0])
            var b = tensor([0.0, 0.0, 1.0, 1.0])
            var c = logical_xor(a, b)  # [False, True, True, False]

            # Broadcasting example
            var x = ones([3, 1, 5], DType.float32)
            var y = ones([3, 4, 5], DType.float32)
            var z = logical_xor(x, y)  # Shape (3, 4, 5)
    ```
    """
    if a.dtype() != b.dtype():
        raise Error("logical_xor: tensors must have same dtype")

    # Compute broadcast shape
    var result_shape = broadcast_shapes(a.shape(), b.shape())
    var result = ExTensor(result_shape, a.dtype())

    # Compute broadcast strides
    var strides_a = compute_broadcast_strides(a.shape(), result_shape)
    var strides_b = compute_broadcast_strides(b.shape(), result_shape)

    # Calculate total elements in result
    var total_elems = 1
    for i in range(len(result_shape)):
        total_elems *= result_shape[i]

    _dispatch_logical_xor(
        result, a, b, strides_a, strides_b, result_shape, total_elems
    )
    return result^


# ============================================================================
# Additional transcendental functions
# ============================================================================


fn log10(tensor: ExTensor) raises -> ExTensor:
    """Base-10 logarithm element-wise.

    Args:
            tensor: Input tensor (must have positive values).

    Returns:
            A new tensor with log10 values.

    Raises:
            Error: If any value is <= 0.

    Examples:
    ```
            var a = tensor([1.0, 10.0, 100.0])
            var b = log10(a)  # [0.0, 1.0, 2.0]
    ```
    """
    var result = ExTensor(tensor.shape(), tensor.dtype())
    _dispatch_log10_forward(result, tensor, tensor.numel())
    return result^


fn log2(tensor: ExTensor) raises -> ExTensor:
    """Base-2 logarithm element-wise.

    Args:
            tensor: Input tensor (must have positive values).

    Returns:
            A new tensor with log2 values.

    Raises:
            Error: If any value is <= 0.

    Examples:
    ```
            var a = tensor([1.0, 2.0, 8.0])
            var b = log2(a)  # [0.0, 1.0, 3.0]
    ```
    """
    var result = ExTensor(tensor.shape(), tensor.dtype())
    _dispatch_log2_forward(result, tensor, tensor.numel())
    return result^


# ============================================================================
# Backward Pass (Gradient Computation)
# ============================================================================


fn exp_backward(grad_output: ExTensor, x: ExTensor) raises -> ExTensor:
    """Compute gradient for exponential function.

        For Y = exp(X), given ∂L/∂Y, computes:
            ∂L/∂X = ∂L/∂Y * exp(X)

    Args:
            grad_output: Gradient from upstream (∂L/∂Y).
            x: Input from forward pass.

    Returns:
            Gradient w.r.t. input (∂L/∂X).

    Examples:
    ```
            var x = ones([3, 4])
            var grad_y = ones([3, 4])
            var grad_x = exp_backward(grad_y, x)  # grad_x = grad_y * exp(x)
    ```
    """
    var result = ExTensor(grad_output.shape(), grad_output.dtype())

    for i in range(grad_output.numel()):
        var grad = grad_output._get_float64(i)
        var x_val = x._get_float64(i)
        # Compute exp(x) for the gradient
        result._set_float64(i, grad * math_exp(x_val))

    return result


fn log_backward(grad_output: ExTensor, x: ExTensor) raises -> ExTensor:
    """Compute gradient for natural logarithm.

        For Y = log(X), given ∂L/∂Y, computes:
            ∂L/∂X = ∂L/∂Y / X

        Includes numerical stability: adds epsilon to prevent division by zero.

    Args:
            grad_output: Gradient from upstream (∂L/∂Y).
            x: Input from forward pass (must be positive).

    Returns:
            Gradient w.r.t. input (∂L/∂X).

    Examples:
            var x = full([3, 4], 2.0)
            var y = log(x)
            var grad_y = ones([3, 4])
            var grad_x = log_backward(grad_y, x)  # grad_x = grad_y / x

        Numerical Stability:
            Uses epsilon = 1e-10 to prevent division by zero.
    """
    alias EPSILON = 1e-10

    var result = ExTensor(grad_output.shape(), grad_output.dtype())

    for i in range(grad_output.numel()):
        var grad = grad_output._get_float64(i)
        var x_val = x._get_float64(i)
        # Add epsilon for numerical stability
        result._set_float64(i, grad / (x_val + EPSILON))

    return result


fn sqrt_backward(grad_output: ExTensor, x: ExTensor) raises -> ExTensor:
    """Compute gradient for square root.

        For Y = sqrt(X), given ∂L/∂Y, computes:
            ∂L/∂X = ∂L/∂Y / (2 * sqrt(X))

    Args:
            grad_output: Gradient from upstream (∂L/∂Y).
            x: Input from forward pass.

    Returns:
            Gradient w.r.t. input (∂L/∂X).

    Examples:
            var x = full([3, 4], 4.0)
            var grad_y = ones([3, 4])
            var grad_x = sqrt_backward(grad_y, x)  # grad_x = grad_y / (2 * sqrt(4.0)) = 0.25

        Numerical Stability:
            Uses epsilon = 1e-10 to prevent division by zero when sqrt(X) ≈ 0.
    """
    alias EPSILON = 1e-10

    var result = ExTensor(grad_output.shape(), grad_output.dtype())

    for i in range(grad_output.numel()):
        var grad = grad_output._get_float64(i)
        var x_val = x._get_float64(i)
        # grad / (2 * sqrt(x))
        # Add epsilon for numerical stability
        result._set_float64(i, grad / (2.0 * math_sqrt(x_val) + EPSILON))

    return result


fn abs_backward(grad_output: ExTensor, x: ExTensor) raises -> ExTensor:
    """Compute gradient for absolute value.

        For Y = |X|, given ∂L/∂Y, computes:
            ∂L/∂X = ∂L/∂Y * sign(X)

        where sign(X) = 1 if X > 0, -1 if X < 0, 0 if X = 0.

        Note: Gradient at X=0 is technically undefined, we use 0 by convention.

    Args:
            grad_output: Gradient from upstream (∂L/∂Y).
            x: Input from forward pass.

    Returns:
            Gradient w.r.t. input (∂L/∂X).

    Examples:
    ```
            var x = tensor([-2.0, -1.0, 0.0, 1.0, 2.0])
            var y = abs(x)
            var grad_y = ones([5])
            var grad_x = abs_backward(grad_y, x)  # [-1, -1, 0, 1, 1]
    ```
    """
    var result = ExTensor(grad_output.shape(), grad_output.dtype())

    for i in range(grad_output.numel()):
        var grad = grad_output._get_float64(i)
        var x_val = x._get_float64(i)

        # Compute sign(x): 1 if x > 0, -1 if x < 0, 0 if x = 0
        var sign_x: Float64 = 0.0
        if x_val > 0.0:
            sign_x = 1.0
        elif x_val < 0.0:
            sign_x = -1.0
        # else: sign_x = 0.0 (at x=0, gradient is undefined, use 0)

        result._set_float64(i, grad * sign_x)

    return result


fn clip_backward(
    grad_output: ExTensor, x: ExTensor, min_val: Float64, max_val: Float64
) raises -> ExTensor:
    """Compute gradient for clip (clamp) operation.

        For Y = clip(X, min, max), given ∂L/∂Y, computes:
            ∂L/∂X = ∂L/∂Y  if min <= X <= max
            ∂L/∂X = 0       if X < min or X > max

        Gradient flows through only where input is within bounds.

    Args:
            grad_output: Gradient from upstream (∂L/∂Y).
            x: Input from forward pass.
            min_val: Minimum clip value.
            max_val: Maximum clip value.

    Returns:
            Gradient w.r.t. input (∂L/∂X).

    Examples:
    ```
            var x = tensor([-2.0, -1.0, 0.0, 1.0, 2.0])
            var y = clip(x, -1.0, 1.0)  # [-1, -1, 0, 1, 1]
            var grad_y = ones([5])
            var grad_x = clip_backward(grad_y, x, -1.0, 1.0)  # [0, 1, 1, 1, 0]
    ```
    """
    var result = ExTensor(grad_output.shape(), grad_output.dtype())

    for i in range(grad_output.numel()):
        var grad = grad_output._get_float64(i)
        var x_val = x._get_float64(i)

        # Gradient flows through only if min <= x <= max
        if x_val >= min_val and x_val <= max_val:
            result._set_float64(i, grad)
        else:
            result._set_float64(i, 0.0)

    return result


fn log10_backward(grad_output: ExTensor, x: ExTensor) raises -> ExTensor:
    """Compute gradient for base-10 logarithm.

        For Y = log10(X), given ∂L/∂Y, computes:
            ∂L/∂X = ∂L/∂Y / (X * ln(10))

    Args:
            grad_output: Gradient from upstream (∂L/∂Y).
            x: Input from forward pass (must be positive).

    Returns:
            Gradient w.r.t. input (∂L/∂X).

        Numerical Stability:
            Uses epsilon = 1e-10 to prevent division by zero.
    """
    alias EPSILON = 1e-10
    alias LN10 = 2.302585092994046  # ln(10)

    var result = ExTensor(grad_output.shape(), grad_output.dtype())

    for i in range(grad_output.numel()):
        var grad = grad_output._get_float64(i)
        var x_val = x._get_float64(i)
        result._set_float64(i, grad / (x_val * LN10 + EPSILON))

    return result


fn log2_backward(grad_output: ExTensor, x: ExTensor) raises -> ExTensor:
    """Compute gradient for base-2 logarithm.

        For Y = log2(X), given ∂L/∂Y, computes:
            ∂L/∂X = ∂L/∂Y / (X * ln(2))

    Args:
            grad_output: Gradient from upstream (∂L/∂Y).
            x: Input from forward pass (must be positive).

    Returns:
            Gradient w.r.t. input (∂L/∂X).

        Numerical Stability:
            Uses epsilon = 1e-10 to prevent division by zero.
    """
    alias EPSILON = 1e-10
    alias LN2 = 0.6931471805599453  # ln(2)

    var result = ExTensor(grad_output.shape(), grad_output.dtype())

    for i in range(grad_output.numel()):
        var grad = grad_output._get_float64(i)
        var x_val = x._get_float64(i)
        result._set_float64(i, grad / (x_val * LN2 + EPSILON))

    return result
