"""Variable - Autograd-enabled tensor wrapper.

Provides automatic differentiation capabilities by wrapping ExTensor with
gradient tracking and computation graph recording.

This module implements a tape-based autograd system similar to PyTorch's eager
mode execution, where operations are recorded during the forward pass and
replayed in reverse during backward propagation.

Key Concepts:
- Variable wraps an ExTensor and adds requires_grad flag and grad storage
- Operations on Variables are recorded in a gradient tape
- Calling .backward() triggers automatic gradient computation via chain rule
- Gradients accumulate across multiple backward passes (call .zero_grad() to reset)

Examples:
    from shared.autograd import Variable, GradientTape

    # Create gradient tape
    var tape = GradientTape()
    tape.enable()

    # Create variables with gradient tracking
    var x = Variable(zeros(shape, dtype), requires_grad=True, tape)
    var y = Variable(ones(shape, dtype), requires_grad=True, tape)

    # Perform operations (recorded in tape)
    var z = variable_add(x, y, tape)
    var loss = variable_sum(z, tape)

    # Compute gradients automatically
    loss.backward(tape)

    # Access gradients
    print(tape.get_grad(x.id))  # dLoss/dx
    print(tape.get_grad(y.id))  # dLoss/dy
"""

from ..core.extensor import ExTensor, ones_like, zeros_like
from ..core.arithmetic import add, subtract, multiply, divide
from ..core.reduction import sum as tensor_sum, mean as tensor_mean
from ..core.matrix import matmul
from ..core.activation import relu, sigmoid, tanh

from .tape import (
    GradientTape,
    SavedTensors,
    OP_ADD,
    OP_SUBTRACT,
    OP_MULTIPLY,
    OP_DIVIDE,
    OP_SUM,
    OP_MEAN,
    OP_MATMUL,
    OP_RELU,
    OP_SIGMOID,
    OP_TANH,
    OP_NEG,
)


struct Variable(Copyable, Movable):
    """Tensor wrapper with automatic differentiation support.

    Variable extends ExTensor with gradient tracking capabilities. Each Variable
    maintains:
    - data: The actual tensor values (ExTensor)
    - id: Unique identifier for tape tracking
    - requires_grad: Whether to track operations for this variable

    Gradients are stored in the GradientTape's registry, not in the Variable
    itself. This allows for gradient accumulation and cleanup via the tape.

    Attributes:
        data: The underlying ExTensor containing values
        id: Unique identifier for gradient tracking
        requires_grad: Flag indicating whether this Variable participates in autograd

    Note:
        Operations on Variables create new Variables. The tape records which
        operations were performed and on which variables, enabling automatic
        gradient computation.
    """

    var data: ExTensor
    var id: Int
    var requires_grad: Bool

    fn __init__(
        out self,
        owned data: ExTensor,
        requires_grad: Bool,
        mut tape: GradientTape,
    ) raises:
        """Initialize a Variable and register it with the tape.

        Args:
            data: The tensor values to wrap (ownership transferred)
            requires_grad: Whether to track gradients for this variable
            tape: The gradient tape to register with

        Examples:
            var tape = GradientTape()
            var x = Variable(zeros(shape, dtype), True, tape)
        """
        self.data = data^
        self.requires_grad = requires_grad
        self.id = tape.register_variable(requires_grad)

    fn __copyinit__(out self, existing: Self):
        """Copy constructor."""
        self.data = existing.data
        self.id = existing.id
        self.requires_grad = existing.requires_grad

    fn __moveinit__(out self, deinit existing: Self):
        """Move constructor."""
        self.data = existing.data^
        self.id = existing.id
        self.requires_grad = existing.requires_grad

    fn __init__(
        out self,
        owned data: ExTensor,
        requires_grad: Bool,
        id: Int,
    ):
        """Initialize a Variable with explicit ID (internal use).

        Args:
            data: The tensor values to wrap (ownership transferred)
            requires_grad: Whether to track gradients for this variable
            id: Pre-assigned variable ID

        Note:
            This constructor is primarily for internal use when creating
            output Variables from operations.
        """
        self.data = data^
        self.requires_grad = requires_grad
        self.id = id

    fn backward(self, mut tape: GradientTape) raises:
        """Compute gradients via automatic differentiation.

        Triggers backward pass through the computation graph, computing gradients
        for all Variables with requires_grad=True that were used to compute this
        Variable.

        The gradient of this Variable with respect to itself is initialized to
        ones (d_self/d_self = 1), then gradients are propagated backward through
        the graph using the chain rule.

        Args:
            tape: The gradient tape that recorded operations

        Examples:
            var x = Variable(data, True, tape)
            var loss = compute_loss(x, tape)
            loss.backward(tape)  # Computes gradients for all inputs
            print(tape.get_grad(x.id))  # dLoss/dx
        """
        # Initialize gradient of output to ones
        var grad = ones_like(self.data)
        tape.backward(self.id, grad^)

    fn detach(self) -> ExTensor:
        """Get the underlying tensor without gradient tracking.

        Useful for breaking the computation graph when you want to use values
        without tracking gradients.

        Returns:
            The underlying ExTensor (copy)

        Examples:
            var x = Variable(data, True, tape)
            var y = x.detach()  # y is just an ExTensor, no gradient tracking
        """
        return self.data

    fn shape(self) -> List[Int]:
        """Get the shape of the underlying tensor.

        Returns:
            List of dimension sizes
        """
        return self.data.shape()

    fn numel(self) -> Int:
        """Get the number of elements in the tensor.

        Returns:
            Total number of elements
        """
        return self.data.numel()

    fn dtype(self) -> DType:
        """Get the data type of the underlying tensor.

        Returns:
            The DType of the tensor
        """
        return self.data.dtype()


# ============================================================================
# Variable Operations
# ============================================================================
# These functions perform operations on Variables and record them in the tape.
# They follow the functional API pattern - each operation creates a new Variable.


fn variable_add(
    a: Variable,
    b: Variable,
    mut tape: GradientTape,
) raises -> Variable:
    """Add two Variables element-wise.

    Args:
        a: First input Variable
        b: Second input Variable
        tape: Gradient tape for recording

    Returns:
        New Variable containing a + b
    """
    var result_data = add(a.data, b.data)
    var result_id = tape.register_variable(a.requires_grad or b.requires_grad)

    # Record operation for backward pass
    if tape.enabled and (a.requires_grad or b.requires_grad):
        var input_ids = List[Int]()
        input_ids.append(a.id)
        input_ids.append(b.id)
        var saved = SavedTensors()
        tape.record(OP_ADD, input_ids^, result_id, saved^)

    return Variable(result_data^, a.requires_grad or b.requires_grad, result_id)


fn variable_subtract(
    a: Variable,
    b: Variable,
    mut tape: GradientTape,
) raises -> Variable:
    """Subtract two Variables element-wise.

    Args:
        a: First input Variable
        b: Second input Variable
        tape: Gradient tape for recording

    Returns:
        New Variable containing a - b
    """
    var result_data = subtract(a.data, b.data)
    var result_id = tape.register_variable(a.requires_grad or b.requires_grad)

    if tape.enabled and (a.requires_grad or b.requires_grad):
        var input_ids = List[Int]()
        input_ids.append(a.id)
        input_ids.append(b.id)
        var saved = SavedTensors()
        tape.record(OP_SUBTRACT, input_ids^, result_id, saved^)

    return Variable(result_data^, a.requires_grad or b.requires_grad, result_id)


fn variable_multiply(
    a: Variable,
    b: Variable,
    mut tape: GradientTape,
) raises -> Variable:
    """Multiply two Variables element-wise.

    Args:
        a: First input Variable
        b: Second input Variable
        tape: Gradient tape for recording

    Returns:
        New Variable containing a * b
    """
    var result_data = multiply(a.data, b.data)
    var result_id = tape.register_variable(a.requires_grad or b.requires_grad)

    if tape.enabled and (a.requires_grad or b.requires_grad):
        var input_ids = List[Int]()
        input_ids.append(a.id)
        input_ids.append(b.id)

        # Save inputs for backward pass
        var saved = SavedTensors()
        saved.add_tensor(a.data)
        saved.add_tensor(b.data)
        tape.record(OP_MULTIPLY, input_ids^, result_id, saved^)

    return Variable(result_data^, a.requires_grad or b.requires_grad, result_id)


fn variable_divide(
    a: Variable,
    b: Variable,
    mut tape: GradientTape,
) raises -> Variable:
    """Divide two Variables element-wise.

    Args:
        a: Numerator Variable
        b: Denominator Variable
        tape: Gradient tape for recording

    Returns:
        New Variable containing a / b
    """
    var result_data = divide(a.data, b.data)
    var result_id = tape.register_variable(a.requires_grad or b.requires_grad)

    if tape.enabled and (a.requires_grad or b.requires_grad):
        var input_ids = List[Int]()
        input_ids.append(a.id)
        input_ids.append(b.id)

        # Save inputs for backward pass
        var saved = SavedTensors()
        saved.add_tensor(a.data)
        saved.add_tensor(b.data)
        tape.record(OP_DIVIDE, input_ids^, result_id, saved^)

    return Variable(result_data^, a.requires_grad or b.requires_grad, result_id)


fn variable_matmul(
    a: Variable,
    b: Variable,
    mut tape: GradientTape,
) raises -> Variable:
    """Matrix multiply two Variables.

    Args:
        a: First matrix Variable
        b: Second matrix Variable
        tape: Gradient tape for recording

    Returns:
        New Variable containing a @ b
    """
    var result_data = matmul(a.data, b.data)
    var result_id = tape.register_variable(a.requires_grad or b.requires_grad)

    if tape.enabled and (a.requires_grad or b.requires_grad):
        var input_ids = List[Int]()
        input_ids.append(a.id)
        input_ids.append(b.id)

        # Save inputs for backward pass
        var saved = SavedTensors()
        saved.add_tensor(a.data)
        saved.add_tensor(b.data)
        tape.record(OP_MATMUL, input_ids^, result_id, saved^)

    return Variable(result_data^, a.requires_grad or b.requires_grad, result_id)


fn variable_sum(
    x: Variable,
    mut tape: GradientTape,
    axis: Int = -1,
) raises -> Variable:
    """Sum a Variable along an axis (or all elements if axis=-1).

    Args:
        x: Input Variable
        tape: Gradient tape for recording
        axis: Axis to sum along (-1 for full reduction)

    Returns:
        New Variable containing the sum
    """
    var result_data = tensor_sum(x.data, axis)
    var result_id = tape.register_variable(x.requires_grad)

    if tape.enabled and x.requires_grad:
        var input_ids = List[Int]()
        input_ids.append(x.id)

        # Save input shape and axis for backward pass
        var saved = SavedTensors()
        saved.add_shape(x.data.shape())
        saved.add_scalar(Float64(axis))
        tape.record(OP_SUM, input_ids^, result_id, saved^)

    return Variable(result_data^, x.requires_grad, result_id)


fn variable_mean(
    x: Variable,
    mut tape: GradientTape,
    axis: Int = -1,
) raises -> Variable:
    """Mean of a Variable along an axis (or all elements if axis=-1).

    Args:
        x: Input Variable
        tape: Gradient tape for recording
        axis: Axis to average along (-1 for full reduction)

    Returns:
        New Variable containing the mean
    """
    var result_data = tensor_mean(x.data, axis)
    var result_id = tape.register_variable(x.requires_grad)

    if tape.enabled and x.requires_grad:
        var input_ids = List[Int]()
        input_ids.append(x.id)

        # Save input shape and axis for backward pass
        var saved = SavedTensors()
        saved.add_shape(x.data.shape())
        saved.add_scalar(Float64(axis))
        tape.record(OP_MEAN, input_ids^, result_id, saved^)

    return Variable(result_data^, x.requires_grad, result_id)


fn variable_relu(
    x: Variable,
    mut tape: GradientTape,
) raises -> Variable:
    """Apply ReLU activation to a Variable.

    Args:
        x: Input Variable
        tape: Gradient tape for recording

    Returns:
        New Variable containing ReLU(x)
    """
    var result_data = relu(x.data)
    var result_id = tape.register_variable(x.requires_grad)

    if tape.enabled and x.requires_grad:
        var input_ids = List[Int]()
        input_ids.append(x.id)

        # Save input for backward pass
        var saved = SavedTensors()
        saved.add_tensor(x.data)
        tape.record(OP_RELU, input_ids^, result_id, saved^)

    return Variable(result_data^, x.requires_grad, result_id)


fn variable_sigmoid(
    x: Variable,
    mut tape: GradientTape,
) raises -> Variable:
    """Apply sigmoid activation to a Variable.

    Args:
        x: Input Variable
        tape: Gradient tape for recording

    Returns:
        New Variable containing sigmoid(x)
    """
    var result_data = sigmoid(x.data)
    var result_id = tape.register_variable(x.requires_grad)

    if tape.enabled and x.requires_grad:
        var input_ids = List[Int]()
        input_ids.append(x.id)

        # Save output for backward pass (sigmoid_backward uses output)
        var saved = SavedTensors()
        saved.add_tensor(result_data)
        tape.record(OP_SIGMOID, input_ids^, result_id, saved^)

    return Variable(result_data^, x.requires_grad, result_id)


fn variable_tanh(
    x: Variable,
    mut tape: GradientTape,
) raises -> Variable:
    """Apply tanh activation to a Variable.

    Args:
        x: Input Variable
        tape: Gradient tape for recording

    Returns:
        New Variable containing tanh(x)
    """
    var result_data = tanh(x.data)
    var result_id = tape.register_variable(x.requires_grad)

    if tape.enabled and x.requires_grad:
        var input_ids = List[Int]()
        input_ids.append(x.id)

        # Save output for backward pass (tanh_backward uses output)
        var saved = SavedTensors()
        saved.add_tensor(result_data)
        tape.record(OP_TANH, input_ids^, result_id, saved^)

    return Variable(result_data^, x.requires_grad, result_id)


fn variable_neg(
    x: Variable,
    mut tape: GradientTape,
) raises -> Variable:
    """Negate a Variable element-wise.

    Args:
        x: Input Variable
        tape: Gradient tape for recording

    Returns:
        New Variable containing -x
    """
    # Create negated tensor
    var result_data = zeros_like(x.data)
    var size = x.data.numel()
    for i in range(size):
        result_data._data[i] = -x.data._data[i]

    var result_id = tape.register_variable(x.requires_grad)

    if tape.enabled and x.requires_grad:
        var input_ids = List[Int]()
        input_ids.append(x.id)
        var saved = SavedTensors()
        tape.record(OP_NEG, input_ids^, result_id, saved^)

    return Variable(result_data^, x.requires_grad, result_id)
