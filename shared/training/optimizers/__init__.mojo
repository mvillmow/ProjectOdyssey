"""
Optimizers

Optimizer implementations for training neural networks.

Includes:
- SGD (Stochastic Gradient Descent) with momentum
- Adam (Adaptive Moment Estimation)
- AdamW (Adam with Weight Decay)
- RMSprop (Root Mean Square Propagation)

All optimizers implement the Optimizer trait for consistent interface.
"""

# Export optimizer implementations
# These will be populated during implementation phase

# from .base import Optimizer
# from .sgd import SGD
# from .adam import Adam
# from .adamw import AdamW
# from .rmsprop import RMSprop
