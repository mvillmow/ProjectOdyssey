# Day Forty-Seven: Training Robustness

**Project:** ML Odyssey Manual
**Date:** December 26, 2025
**Branch:** `main`
**Tags:** #training #gradient-clipping #checkpointing #migration #robustness

---

> **Note:** This blog post was AI-generated based on git commit history.
> Content reflects actual work done but was not written in real-time.

---

## TL;DR

Training robustness improvements. Added comprehensive gradient clipping utilities, implemented CheckpointManager for model checkpointing, migrated AlexNet, MobileNetV1, and GoogLeNet to the unified TrainingLoop, and added in-place gradient operations for memory efficiency. Also fixed conv2d backward pass for float16 overflow prevention.

**Key insight:** Robust training isn't just about algorithmsâ€”it's about handling edge cases (gradient explosion, checkpoint corruption, precision overflow) gracefully.

---

## Gradient Clipping

### Comprehensive Utilities

```mojo
fn clip_grad_norm(
    grads: List[ExTensor],
    max_norm: Float32,
    norm_type: Float32 = 2.0
) -> Float32:
    """Clip gradients by global norm, return original norm."""
    var total_norm = compute_grad_norm(grads, norm_type)
    var clip_coef = max_norm / (total_norm + 1e-6)
    if clip_coef < 1.0:
        for grad in grads:
            grad *= clip_coef
    return total_norm

fn clip_grad_value(grads: List[ExTensor], clip_value: Float32):
    """Clip gradients by value (element-wise)."""
    for grad in grads:
        grad.clamp_(-clip_value, clip_value)
```

### Why It Matters

Gradient explosion can:
- NaN out training
- Destabilize weights
- Cause numerical overflow

Clipping prevents this while preserving gradient direction.

---

## CheckpointManager

```mojo
struct CheckpointManager:
    var directory: Path
    var max_to_keep: Int
    var best_metric: Float32
    var checkpoints: List[Path]

    fn save(
        mut self,
        model: Model,
        optimizer: Optimizer,
        epoch: Int,
        metrics: Dict[String, Float32]
    ) raises:
        """Save checkpoint, manage retention."""
        var path = self.directory / f"checkpoint_epoch_{epoch}.pt"
        save_checkpoint(path, model, optimizer, metrics)
        self.checkpoints.append(path)

        # Keep only max_to_keep checkpoints
        while len(self.checkpoints) > self.max_to_keep:
            var old = self.checkpoints.pop(0)
            remove_file(old)

        # Track best
        if metrics["val_loss"] < self.best_metric:
            self.best_metric = metrics["val_loss"]
            copy_file(path, self.directory / "best.pt")
```

---

## Model Migration to TrainingLoop

Migrated classic architectures to unified TrainingLoop:

### AlexNet
```mojo
fn train_alexnet():
    var model = AlexNet(num_classes=1000)
    var optimizer = Adam(model.parameters(), lr=1e-3)
    var loop = TrainingLoop(model, optimizer, CrossEntropyLoss())
    loop.train(epochs=90, train_loader, val_loader)
```

### MobileNetV1
Same pattern with MobileNet-specific hyperparameters.

### GoogLeNet
Same pattern, handling auxiliary classifiers.

---

## Float16 Overflow Prevention

Fixed conv2d backward for float16:

```mojo
fn conv2d_backward[dtype: DType](
    grad_output: ExTensor[dtype],
    input: ExTensor[dtype],
    weight: ExTensor[dtype]
) -> Tuple[ExTensor[dtype], ExTensor[dtype]]:
    # For float16, accumulate in float32 to prevent overflow
    @parameter
    if dtype == DType.float16:
        var grad_input_f32 = _conv2d_backward_input[DType.float32](...)
        var grad_weight_f32 = _conv2d_backward_weight[DType.float32](...)
        return (grad_input_f32.to(dtype), grad_weight_f32.to(dtype))
    else:
        return (_conv2d_backward_input[dtype](...), _conv2d_backward_weight[dtype](...))
```

---

## In-Place Gradient Operations

Added memory-efficient in-place ops:

```mojo
fn add_grad_(grad: ExTensor, delta: ExTensor):
    """In-place gradient accumulation."""
    @parameter
    fn simd_add[width: Int](i: Int):
        grad.store(i, grad.load[width](i) + delta.load[width](i))
    vectorize[simd_add, simd_width](grad.numel())
```

---

## What's Next

### Immediate Priorities

1. **Distributed training** - Multi-GPU support
2. **Mixed precision** - AMP training
3. **Learning rate finder** - Automatic LR selection

---

## Reflections

1. **Robust > fast** - Training that crashes isn't training
2. **Checkpoints are insurance** - Saves hours of lost work
3. **Overflow is subtle** - Float16 needs careful handling

---

**Status:** Gradient clipping added, CheckpointManager implemented, models migrated

**Next:** Distributed training, mixed precision, LR finder

### Stats

- **Commits:** 59
- **Gradient utilities:** clip_grad_norm, clip_grad_value
- **Models migrated:** AlexNet, MobileNetV1, GoogLeNet
- **Checkpoint features:** CheckpointManager with retention
- **Overflow fix:** conv2d backward for float16

---

*This post was reconstructed from git history by AI on December 30, 2025.*
