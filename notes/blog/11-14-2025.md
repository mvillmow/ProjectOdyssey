# Day Eight: The Agent Explosion and Workflow Optimization

**Project:** ML Odyssey Manual
**Date:** November 14, 2025
**Branch:** `main`
**Tags:** #agentic-workflows #optimization #process-design #checklists #token-budgeting

---

## TL;DR

Anthropic gave me $1,000 in compute credits for 15 days, enabling parallel workflow testing. Burned ~$100/day,
giving me enough budget to test multiple concurrent agent flows. Realized that agentic workflows need structured
checklists instead of pure prompt-driven direction—agents work better with explicit validation steps, just like
surgical checklists reduce human error. Identified 8 core workflow patterns that need automation and concluded
that this agent orchestration system could be a separate, universally useful product.

**Key insight:** Agentic workflows thrive with structure, not autonomy.

---

## The Parallel Experiment Setup

With $1,000 in compute credits available, I'm running at ~$100/day. That's 10 days of runway to test
workflows in parallel. This is different from the typical "one workflow at a time" approach—I can spin up
multiple agent chains simultaneously and measure what works.

The goal: identify which agent patterns are genuinely useful versus which ones are just "doing more work."

---

## The Core Realization: Checklists Beat Autonomy

This is the insight that emerged while working through parallel workflows:

**Agentic prompting works great for general guidance, but fails for specific, sequential steps.**

When you tell an agent "figure out the best approach," they do something reasonable. But when you need them to
do the same things consistently every single time—with verification—prompt-based direction breaks down.

Doctors don't rely on surgical intuition. They have checklists. The reason? Checklists catch mistakes that
autonomy misses.

This mirrors exactly what I'm seeing with agents. The best-performing workflows aren't the ones where agents
have maximum freedom. They're the ones where each agent has a specific, bounded set of responsibilities with
explicit validation steps.

---

## Eight Core Workflows I Need

Based on the patterns I keep manually doing:

1. **Cleanup all worktrees** - Use subagents in parallel, validate they're gone
1. **Review pull requests** - Run specialized review subagents in parallel, leave individual comments
1. **Implement a feature** - Work with Chief Architect on design, track progress
1. **Handle reviewed PRs** - Read comments, respond to each one, re-test, push fixes
1. **Plan the current PR** - Break down what's happening, identify risks and dependencies
1. **Validate completion** - Verify that tasks were actually done correctly, not just claimed
1. **Update blog entries** - Convert commits/work into narrative format
1. **Update GitHub issues** - Sync issue descriptions with current repository state

Simple in hindsight, but they're the eight things I'm constantly repeating manually.

---

## The Prompt Clarity Problem

I also noticed I'm constantly re-typing prompts because they're unclear or too broad. The fix isn't better LLMs—it's
**clearer prompt design**.

The workflows need to be:

- Explicit about what "done" means
- Clear about validation steps
- Structured as checklists, not freeform directions
- Programmatically checkable when possible

This is boring infrastructure, but it's the difference between "agent systems that sometimes work" and "agent
systems that reliably work."

---

## The Business Angle

Here's what's interesting: agent orchestration with structured workflows seems universally useful.

It looks like it might compete with Claude Code, but I think there's a complementary angle: Claude Code is for
interactive problem-solving, but this would be for workflow automation—running specific, validated sequences
of tasks.

I might extract this into a separate repository. It could be a product, not just an ML Odyssey implementation
detail.

---

## What's Next

Immediate priorities:

1. **Formalize the 8 core workflows** - Design each one as an explicit checklist with validation
1. **Build programmatic validation** - Add `verify_*` functions that check success conditions
1. **Improve prompt clarity** - Rewrite agent prompts to be more specific and less open-ended
1. **Stress test with parallel runs** - Use the compute credits to run multiple workflows simultaneously
1. **Measure and iterate** - Use token consumption data to optimize which workflows need refinement

---

## Reflections

This experiment is already changing how I think about agentic systems:

1. **Structure creates reliability** - Agents with checklists outperform agents with autonomy. This mirrors
   real teams.
1. **Explicit beats implicit** - "Validate that X happened" beats "do your best." Programmatic verification
   changes everything.
1. **Workflows are products** - Agent orchestration feels like something worth extracting and polishing. The
   demand signal is real.
1. **Constraints are features** - The $100/day budget is forcing architectural decisions that make the system
   better, not worse.

The parallel experiment is off to a strong start. Eight well-defined workflows, each with explicit validation
steps, could be the foundation of something genuinely useful.

---

**Status:** Parallel workflows running, core patterns identified, agent orchestration system stabilizing

**Next:** Formalize checklist-based workflows, implement validation functions, begin stress testing

### Stats

- $1,000 in compute credits allocated
- ~$100/day burn rate achieved
- 8 core workflows identified
- 15 days of parallel testing runway
- ~10+ hours of repeated manual prompts reduced to patterns
- 1 developer convinced that workflow structure > agentic freedom
