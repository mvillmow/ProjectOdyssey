# Day Twenty-One: Optimizer Suite

**Project:** ML Odyssey Manual
**Date:** December 2, 2025
**Branch:** `main`
**Tags:** #optimizers #adam #schedulers #gradients #training

---

> **Note:** This blog post was AI-generated based on git commit history.
> Content reflects actual work done but was not written in real-time.

---

## TL;DR

Massive optimizer and training infrastructure day. Implemented Adam optimizer, learning rate schedulers, gradient clipping utilities, and tape-based automatic differentiation. Also added depthwise separable convolutions, multi-head attention, instance norm, group norm, and dropout with mask caching. The training stack is now feature-complete.

**Key insight:** A good optimizer is worth more than a clever model architectureâ€”Adam with proper learning rate scheduling can make mediocre architectures perform well.

---

## Optimizer Implementations

### Adam Optimizer

The workhorse of deep learning:

```mojo
struct Adam(Optimizer):
    var lr: Float32
    var beta1: Float32 = 0.9
    var beta2: Float32 = 0.999
    var eps: Float32 = 1e-8
    var m: List[ExTensor]  # First moment estimates
    var v: List[ExTensor]  # Second moment estimates
    var t: Int = 0         # Timestep

    fn step(mut self, params: List[ExTensor], grads: List[ExTensor]):
        self.t += 1
        for i in range(len(params)):
            # Update biased first moment estimate
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grads[i]
            # Update biased second moment estimate
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grads[i] * grads[i]
            # Bias correction
            var m_hat = self.m[i] / (1 - self.beta1 ** self.t)
            var v_hat = self.v[i] / (1 - self.beta2 ** self.t)
            # Update parameters
            params[i] -= self.lr * m_hat / (sqrt(v_hat) + self.eps)
```

### Learning Rate Schedulers

Implemented common scheduling strategies:

- **StepLR** - Decay by factor every N epochs
- **CosineAnnealingLR** - Smooth cosine decay
- **OneCycleLR** - Super-convergence schedule
- **ReduceOnPlateau** - Adaptive based on validation loss

---

## Advanced Layers

### Depthwise Separable Convolutions

Essential for efficient mobile architectures:

```mojo
struct DepthwiseSeparableConv2D(Layer):
    var depthwise: Conv2D  # Per-channel convolution
    var pointwise: Conv2D  # 1x1 mixing convolution

    fn forward(self, x: ExTensor) -> ExTensor:
        var dw = self.depthwise.forward(x)
        return self.pointwise.forward(dw)
```

### Multi-Head Attention

For transformer architectures:

```mojo
struct MultiHeadAttention(Layer):
    var num_heads: Int
    var head_dim: Int
    var q_proj: Linear
    var k_proj: Linear
    var v_proj: Linear
    var out_proj: Linear
```

### Normalization Layers

- **Instance Norm** - Per-instance normalization
- **Group Norm** - Flexible group-based normalization

---

## Dropout with Mask Caching

Efficient dropout implementation:

```mojo
struct Dropout(Layer):
    var p: Float32
    var mask: ExTensor  # Cached for backward pass
    var training: Bool

    fn forward(mut self, x: ExTensor) -> ExTensor:
        if not self.training:
            return x
        # Generate and cache mask
        self.mask = random.bernoulli(x.shape, 1 - self.p) / (1 - self.p)
        return x * self.mask
```

---

## Gradient Utilities

Added gradient clipping:

```mojo
fn clip_grad_norm(grads: List[ExTensor], max_norm: Float32) -> Float32:
    """Clip gradients by global norm, return original norm."""
    var total_norm = compute_grad_norm(grads)
    if total_norm > max_norm:
        var scale = max_norm / total_norm
        for grad in grads:
            grad *= scale
    return total_norm
```

---

## What's Next

### Immediate Priorities

1. **AdamW** - Adam with decoupled weight decay
2. **LARS** - Layer-wise Adaptive Rate Scaling
3. **Mixed precision** - FP16 optimizer states

---

## Reflections

1. **Adam is robust** - Good default for most problems
2. **Mask caching is essential** - Same mask for forward and backward
3. **Schedulers matter** - Learning rate schedule can make or break training

---

**Status:** Optimizer suite complete, schedulers working, advanced layers implemented

**Next:** AdamW, LARS, mixed precision support

### Stats

- **Commits:** 54
- **Optimizers:** Adam with full feature set
- **Schedulers:** 4 implementations
- **New layers:** Depthwise conv, multi-head attention, instance/group norm
- **Utilities:** Gradient clipping, tape-based autograd

---

*This post was reconstructed from git history by AI on December 30, 2025.*
