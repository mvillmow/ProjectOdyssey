# Day Five: The Limits of Agentic Workflows

**Project:** ML Odyssey Manual
**Date:** November 11, 2025
**Branch:** `main`
**Tags:** #cleanup #documentation #agents #token-limits #side-effects

---

## TL;DR

Spent the evening cleaning up the repository while agents are "asleep"—API budget exhausted after just 4 days of
heavy agentic work. Made 8 cleanup commits (7:50 PM - 8:37 PM) fixing licensing, removing misplaced files, making
scripts executable, removing invalid constraints, and tidying up documentation. Also discovered the painful side
effects of API limits: can't use Claude chat, can't submit feedback, everything associated with the account hits the
wall. The core lesson: **token budgets force better design, but they also expose coordination problems in agentic
workflows**.

**Key commits:** [`c3a96fd`](https://github.com/mvillmow/ml-odyssey/commit/c3a96fd) (blog post + cleanup start),
[`21846ad`](https://github.com/mvillmow/ml-odyssey/commit/21846ad) (unify BSD licensing),
[`3c5281b`](https://github.com/mvillmow/ml-odyssey/commit/3c5281b) (remove misplaced markdown),
[`6c3a599`](https://github.com/mvillmow/ml-odyssey/commit/6c3a599) (fix missing scripts dir),
[`6ecca87`](https://github.com/mvillmow/ml-odyssey/commit/6ecca87) (make scripts executable),
[`1c04a5c`](https://github.com/mvillmow/ml-odyssey/commit/1c04a5c) (remove invalid constraints),
[`bb2d737`](https://github.com/mvillmow/ml-odyssey/commit/bb2d737) (cleanup paper templates),
[`7f884b7`](https://github.com/mvillmow/ml-odyssey/commit/7f884b7) (add WIP notices to shared library)

---

## The Cleanup Session

After hitting API budget limits on day 4, I spent the evening doing something I've been putting off: cleaning up the
repository. This is the "first draft is quick and dirty" phase meeting the "second iteration learns from first draft"
phase.

The work happened fast, back-to-back commits from 7:50 PM through 8:37 PM:

### Licensing Unification

Discovered the repository had mixed licensing: MIT in some places, BSD 3-Clause in others. Cleaned it up to use BSD
3-Clause consistently across all files. This matters less for an internal project but becomes important if this ever
becomes open-source.

Commit: [`21846ad`](https://github.com/mvillmow/ml-odyssey/commit/21846ad)

### Misplaced Files

Found a markdown file that had been accidentally placed in the wrong directory structure. Removed it and ensured the
directory organization matched the planned hierarchy.

Commit: [`3c5281b`](https://github.com/mvillmow/ml-odyssey/commit/3c5281b)

### Scripts Directory

Discovered the `scripts/` directory was referenced in documentation but missing from the repository. Created it and
ensured all shell scripts had proper execute permissions.

Commits: [`6c3a599`](https://github.com/mvillmow/ml-odyssey/commit/6c3a599),
[`6ecca87`](https://github.com/mvillmow/ml-odyssey/commit/6ecca87)

### Constraint Cleanup

Removed several constraints from CLAUDE.md that were no longer valid for current Claude versions. The file had
documented limitations from earlier iterations that no longer applied.

Commit: [`1c04a5c`](https://github.com/mvillmow/ml-odyssey/commit/1c04a5c)

### Paper Template Organization

Cleaned up the paper template structure and ensured consistency across template files.

Commit: [`bb2d737`](https://github.com/mvillmow/ml-odyssey/commit/bb2d737)

### Shared Library Notices

Added "Work In Progress" notices to shared library documentation to signal to future agents that these components are
still being refined.

Commit: [`7f884b7`](https://github.com/mvillmow/ml-odyssey/commit/7f884b7)

---

## Hitting the Wall

Token budget limits hit hard on day 4. The $100 Max plan from Anthropic isn't enough to run agents for a full week of
heavy experimentation. But here's the thing that surprised me: **it wasn't a single limit that killed it**.

### The Multiple Limits Problem

I expected to hit either:

- Opus model limit (tokens-per-day for the most capable model)
- 5-hour concurrent usage limit (maximum concurrent execution time)

But instead, the system hit a **third limit: total usage across all models in the monthly cycle**.

This is... complicated. Because it means even if I optimize individual models, there's a ceiling on how much total
work I can do in a given billing period. And that ceiling is lower than I expected for a week of continuous agent
work.

### The Coordination Problem

This creates a painful situation: I'm in the middle of building agent workflows, and the API access gets cut off
mid-task. Unlike a local development environment where I can iterate continuously, the agent system has these hard
stops that leave things in inconsistent states.

Tomorrow I'll focus on getting proper model tiering (from day 4's analysis) fully operational. That should help, but
it's too late to test at scale this week.

---

## Side Effects of the Limits

The token budget exhaustion created some side effects I didn't anticipate:

### Claude Chat is Blocked

I can't use Claude chat at all while on the $100 Max plan. The web interface just says "you've hit your limits."

That's... frustrating? I can still use Claude Code locally, but the interactive chat experience is completely blocked.

### Even Feedback Doesn't Work

The `/feedback` command in Claude Code is now blocked because feedback submission counts against the account limits.

How is *submitting feedback* counted against my API usage? This creates a perverse incentive: now I can't report
issues with the model because reporting issues costs tokens.

### Phone Number Entanglement

The free plan is associated with my phone number, and the phone number is associated with my paid account. So when
the paid account hits limits, the "free" tier becomes inaccessible too.

This is a design problem, not a technical limitation. But it's worth noting.

---

## Three Discoveries

### Discovery 1: Agentic Workflows Need Exclusive Access

I've discovered that **parallel human and agent work on the same codebase causes chaos**.

When I'm modifying code while agents are running, the agents get confused. They read a state, generate a plan, and by
the time they implement it, the state has changed. This causes duplicate work, conflicting changes, and wasted tokens.

The solution I've been using: give agents exclusive access to parts of the codebase during active work. This works,
but it's not scalable.

A better solution would be:

- Event-driven coordination (not prompt-driven)
- Distributed version control integration
- Conflict detection and merge strategies
- Clear ownership regions

### Discovery 2: Token Budgets Force Better Architecture

Ironically, hitting the budget limit forced me to do what I should have done earlier: **cleanup and optimization**.

The philosophy here is important:

- **First draft**: Quick and dirty, does the job
- **Second iteration**: Learns from first draft, gets closer to production
- **Third iteration**: Fully tested, well-designed, documented, production-ready

I'm at "first draft done, time for second iteration." The cleanup work isn't because the code is broken—it's working
fine. It's because the *infrastructure* needs to be efficient before I scale it up.

### Discovery 3: Token Budgets Expose Inefficiency

The agent system made inefficiencies visible. Without token accounting, I could run agents inefficiently and never
notice. With token limits, every token-wasting loop becomes painful.

This is actually good. It means I'm being forced to think about:

- Which agent should do which task?
- How much reasoning does this task actually need?
- Can this be done with a cheaper model?
- Is there redundant work happening?

---

## What's Next

Immediate priorities for tomorrow and the next few days:

1. **Implement token-optimized model selection** - From day 4's analysis, ensure agents are using the right model tier
   for their task complexity
1. **Build agent-exclusive workspaces** - Give agents their own branches/regions to avoid coordination conflicts
1. **Create event-driven coordination** - Move from prompt-driven to event-driven agent triggering to reduce chat
   overhead
1. **Set up token tracking per agent** - Measure which agents are expensive and optimize further
1. **Design a DSL for agent workflows** - Eventually replace prompt-based coordination with structured commands

The cleanup work is done. Now it's about making the agent system efficient enough to scale up into the actual ML
implementation work.

---

## Reflections

This day was less about building features and more about **understanding the constraints that shape design**.

1. **Token budgets force better architecture** - Nothing focuses the mind like a hard limit. This is making me think
   about efficiency earlier than I otherwise would.
1. **Agentic coordination is hard** - Parallel autonomous agents stepping on each other's toes is a real problem. This
   is an area where AI research is still catching up.
1. **Three-phase iteration is real** - First draft quick-dirty, second draft learning, third draft production. The
   cleanup work today is the *expected* second phase, not a sign of failure.
1. **API budget limits have weird side effects** - The feedback mechanism being blocked is a design anti-pattern worth
   noting.

I've validated that agents *work* and can produce real value. Now I'm in the optimization phase. This is the right
time to fix inefficiencies before scaling up.

The infrastructure foundation is solid. The agents work. The coordination challenges are understood. Time to make it
efficient.

---

**Status:** Repository cleaned up, agent system paused for budget reset, cleanup and optimization phase active

**Next:** Model-tiered optimization, event-driven coordination, token tracking per agent

### Stats

- 8 cleanup commits in 47 minutes
- 1 API budget exhausted after 4 days (not 7)
- 3 unexpected API limit side effects discovered
- Multiple coordination problems identified
- 1 developer realizing token limits are a feature, not a bug
- 1 very patient person waiting for budget to reset
