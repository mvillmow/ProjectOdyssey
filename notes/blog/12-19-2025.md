# Day Forty: Beyond Supervision - Autonomous Agentic Workflows

**Project:** ML Odyssey Manual
**Date:** December 19, 2025
**Branch:** `main`
**Tags:** #agentic-workflows #automation #scripting #claude-code #autonomous-agents #workflow-optimization

---

## TL;DR

Moving past manual Claude Code supervision. The quality is now high enough that constant watching isn't necessary—but Claude keeps asking questions that slow down end-to-end work. Solution: write scripts that run Claude Code programmatically for autonomous task execution. Created `plan_issues.py` (plans GitHub issues) and `implement_issues.py` (implements planned issues). Successfully processed 30+ GitHub issues with minimal intervention. Token efficiency is suffering from reduced cache reuse, but the approach shows promise. Next evolution: scripts for PR creation, review, and response—pulling the well-defined agentic workflows out of Claude's decision loop entirely.

**Key insight:** Supervision is overhead when quality is sufficient. Autonomous execution trades cache efficiency for parallelism and reduced waiting.

---

## Moving Past Supervision

This week I've been experimenting with moving past Claude Code. One thing I've noticed is that the quality is high enough that I don't really need to watch everything. But Claude keeps asking me questions. This is annoying—if I don't answer them, it slows down the end-to-end work.

The realization: the bottleneck isn't Claude's capability, it's my availability to answer questions and confirm decisions.

---

## The Autonomous Workflow Scripts

So this week I started writing scripts to run Claude Code and programmatically do tasks for me. This approach gets more work done in a shorter amount of time, but isn't as well-tuned yet.

### Current Scripts

**`plan_issues.py`** - Goes through GitHub issues and creates a plan for each item:

- Reads issue context
- Generates implementation plan
- Posts plan as issue comment
- Marks issue as ready for implementation

**`implement_issues.py`** - Takes a planned GitHub issue and implements it:

- Reads the plan from issue comments
- Creates feature branch
- Implements the changes
- Runs tests and validation
- Creates PR linked to issue

### Results So Far

I was able to cycle through 30+ GitHub issues without effort once I got most of the kinks fixed from the scripts. I still had to have a single run of Claude to go through everything as some things broke and weren't fixed, but overall it met my pretty low bar of success.

---

## The Token Efficiency Puzzle

I'm hitting my token limit quicker despite running fewer total tokens. The pattern suggests this approach is cycling through different tasks, so I'm not getting the cached reuse as well.

### The Tradeoff

| Approach | Token Efficiency | Parallelism | Waiting Time |
|----------|------------------|-------------|--------------|
| Manual supervision | High (cached context) | Low | High (my availability) |
| Autonomous scripts | Lower (context switching) | High | Low |

The autonomous approach trades token efficiency for development velocity. Whether that's a good tradeoff depends on what's bottlenecking: token budget or calendar time.

---

## The Next Evolution: PR Workflow Scripts

I think I'm going to create additional scripts for the PR workflow:

1. **`create_prs.py`** - Creates pull requests en masse from completed implementations
2. **`review_prs.py`** - Reviews pull requests with structured feedback
3. **`respond_prs.py`** - Responds to PR review comments and makes requested changes

### Why This Architecture

This form of agentic work will do better than having Claude Code attempt to manage everything itself. These agent flows are:

- **Well-defined** - Clear inputs and outputs
- **Sequential** - Must run in a specific order
- **Predictable** - Same structure every time

Wasting tokens on Claude going through pre-defined steps seems wasteful. Better to encode the workflow in code and have Claude focus on the creative work within each step.

---

## Architectural Realization

I'm beautifying the scripts to make them run better, but really I need to just experiment and see what can be done while I work on my agentic test environment.

### What I've Learned

1. **Protect the workflow better** - Need error handling and recovery
2. **More consistent PRs** - The output quality varies too much
3. **Token efficiency investigation** - Need to understand the cache penalty

### The Bigger Picture

I'm pretty sure this needs to be pulled out into a separate project. But will get it working first before doing that. The pattern of "encode the workflow, let Claude fill in the details" seems generalizable beyond this specific project.

---

## What's Next

### Immediate Priorities

1. **Complete PR workflow scripts** - `create_prs.py`, `review_prs.py`, `respond_prs.py`
2. **Improve error handling** - Scripts should recover gracefully from failures
3. **Investigate cache efficiency** - Understand why token reuse is lower
4. **Standardize outputs** - Make PRs more consistent in structure and quality
5. **Consider extraction** - Evaluate pulling autonomous workflow into separate repo

### Experimental Questions

1. Can autonomous execution match manual supervision quality?
2. What's the optimal balance of autonomy vs. interactive confirmation?
3. How much token efficiency loss is acceptable for parallelism gains?
4. Can workflows be composed (planning → implementation → PR → review)?

---

## Reflections

This week's work revealed several patterns about autonomous agent development:

1. **Supervision is overhead** - When quality is sufficient, watching becomes a bottleneck. The agent doesn't need my attention; it needs my decisions. Encode the decisions in code.

2. **Well-defined workflows belong in code** - Plan → Implement → PR → Review is predictable. Having Claude figure out "what to do next" wastes tokens on a solved problem.

3. **Cache efficiency has a cost** - Context switching between tasks prevents cache reuse. This is the price of parallelism. Need to measure whether the tradeoff is worth it.

4. **Low bars of success are fine initially** - 30+ issues processed with "most things working" is enough to validate the approach. Polish comes after proof-of-concept.

5. **Separation of concerns applies to agents too** - Don't make one agent do everything. Specialized scripts for specialized tasks. Claude provides intelligence; code provides structure.

The meta-insight: I'm building an agent harness. Claude is the engine; the scripts are the chassis. The engine is powerful, but it needs direction. The chassis provides direction without limiting the engine's capability.

---

**Status:** Autonomous workflow prototype working, 30+ issues processed, PR workflow scripts planned

**Next:** Complete PR workflow scripts, improve error handling, investigate cache efficiency, consider project extraction

### Stats: Autonomous Workflow Metrics

**Scripts Developed:**

- `plan_issues.py` - Issue planning automation
- `implement_issues.py` - Implementation automation

**Processing Capacity:**

- **30+ GitHub issues** processed in single session
- **Minimal intervention** - mostly kink fixes in scripts
- **One cleanup pass** needed for incomplete work

**Efficiency Tradeoffs:**

- **Token usage:** Higher per-task (context switching)
- **Total throughput:** Higher (parallelism + no waiting)
- **Cache reuse:** Lower (different task contexts)
- **Calendar time:** Lower (no supervision bottleneck)

**Quality Observation:**

- Met "pretty low bar of success" - proof of concept validated
- Some things broke and weren't fixed automatically
- Single Claude run needed for cleanup pass
- Quality-velocity tradeoff needs calibration

---

**Pattern observed:** Transitioning from interactive to programmatic agent orchestration. The workflow structure is moving into code; Claude's role is shifting from "figure out what to do" to "do this specific thing well."
