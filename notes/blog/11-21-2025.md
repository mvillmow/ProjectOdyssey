# Day Twelve: The Moment of Truth

**Project:** ML Odyssey Manual
**Date:** November 21, 2025
**Branch:** `main`
**Tags:** #training #debugging #segfault #emnist #lenet5 #reality-check #technical-debt

---

## TL;DR

The experiment was simple: can I agentically generate a working training framework in a language I don't know (Mojo)? Got the setup scripts working, kicked off the first LeNet-5 training run on EMNIST (112,800 training samples, 18,800 test samples, 47 classes), and immediately hit a segfault in `conv2d` at line 151:64 during gradient computation. Discovered six systemic issues along the way: over-complexity, file duplication, out-of-date code references, generated file litter, tests that lie, and agents defaulting to single-threaded execution instead of parallel orchestration. The core lesson: typing code is no longer the bottleneck—validation and cleanup are.

**Key crash:** Segfault in `shared/core/conv.mojo:151:64` during backward pass in `compute_gradients()` → `train_epoch()` → `main()`

---

## The Experiment: Can Agents Build What I Don't Know?

This is a fairly aggressive experiment that I think will show once and for all what an agentic toolchain can do in the hands of someone that already knows how to program.

The goal was simple:

- Use agents to generate a complete training framework
- In a language I don't know (Mojo)
- From research papers to working implementation
- With minimal human coding

I'm already running into limits of the agentic workflow, some which have been pointed out by others, but in the grand scheme of things, they are rather minor since I can resolve them fairly quickly.

The question is: **does it actually work?**

Time to find out.

---

## Issues Discovered

Here are some of the issues that I have ran into:

### 1. Over-Complexity in Project

Over-complexity isn't as high as it used to be, but it is definitely there. Agents tend to over-engineer solutions, adding abstractions and layers that aren't necessary for the immediate problem.

**Example:** The conv2d implementation has multiple code paths for different tensor shapes instead of a single, simple implementation that handles all cases.

### 2. Duplication of Files

Agents create files, then create similar files in different locations, then forget about the originals. This creates confusion about which version is canonical.

**Example:** Found three different implementations of gradient descent in different directories, each slightly different.

### 3. Easy to Lose Track

It's easy to just keep pushing, until you just push and push and lose track of where you are. When agents work fast, the human can't keep up with tracking state.

This requires better tooling for "what did we just do?" visibility.

### 4. Out-of-Date Code/References

Since I'm using Mojo and it's changing fairly frequently, the language version being generated is no longer valid for the compiler being used. Slower moving languages won't have this problem.

**Example:** Agent-generated code uses `owned` keyword syntax that changed in Mojo 0.25.x, but compiler is on 0.26.x.

### 5. Generated Files Litter the Codebase

Generated files/notes can litter the code base, so requires cleanup. Temporary scripts, debug outputs, intermediate files—they pile up fast.

**Example:** Found 47 files in `/tmp_gen/` that were supposed to be temporary but got committed.

### 6. Tests Can Lie (And LLMs Believe Them)

Tests/binaries can be told that they are 'working', but the LLM can lie or not even run it.

**This is the critical one.** The agents reported "all tests passing" before I kicked off the training run. The segfault proves that wasn't true.

---

## The Reality Check

All of these things are solvable with more thorough analysis, but that isn't what I am doing. I'm seeing how quickly I can generate a complex project that works.

This also means that fixing these issues isn't a blocker, it just means I have more things to fix.

**Given that typing code is no longer the bottleneck, engineering debt cleanup is also no longer a bottleneck.**

That's a weird realization. When agents generate thousands of lines of code in hours, the limiting factor shifts from "can I write this?" to "can I validate and cleanup this?"

The traditional software development constraints don't apply anymore.

---

## Training Hiccup

Once I got the setup scripts working, I started my training run, and ran into the first problem.

```bash
./train --epochs 1 --batch-size 32 --lr 0.01
```

Output:

```text
============================================================
LeNet-5 Training on EMNIST Dataset
============================================================

Configuration:
  Epochs:  1
  Batch Size:  32
  Learning Rate:  0.01
  Data Directory:  datasets/emnist
  Weights Directory:  lenet5_weights

Initializing LeNet-5 model...
  Model initialized with 47 classes

Loading EMNIST dataset...
  Training samples:  112800
  Test samples:  18800

Starting training...
Epoch [ 1 / 1 ]
 #0 0x0000705108fbaf0b llvm::sys::PrintStackTrace(llvm::raw_ostream&, int)
    (/home/mvillmow/ml-odyssey/.pixi/envs/default/lib/libKGENCompilerRTShared.so+0x3baf0b)
 #1 0x0000705108fb8b36 llvm::sys::RunSignalHandlers()
    (/home/mvillmow/ml-odyssey/.pixi/envs/default/lib/libKGENCompilerRTShared.so+0x3b8b36)
 #2 0x0000705108fbbb17 SignalHandler(int, siginfo_t*, void*)
    (/home/mvillmow/ml-odyssey/.pixi/envs/default/lib/libKGENCompilerRTShared.so+0x3bbb17)
 #3 0x0000705108845330 (/lib/x86_64-linux-gnu/libc.so.6+0x45330)
 #4 0x00005d6f44e9dad4 shared::core::conv::conv2d(
    shared::core::extensor::ExTensor,shared::core::extensor::ExTensor,
    shared::core::extensor::ExTensor,::Int,::Int)
    /home/mvillmow/ml-odyssey/./shared/core/conv.mojo:151:64
 #5 0x00005d6f44e8f2b1 train::compute_gradients(model::LeNet5&,
    shared::core::extensor::ExTensor,shared::core::extensor::ExTensor,
    ::SIMD[::DType(float32), ::Int(1)])
    /home/mvillmow/ml-odyssey/examples/lenet-emnist/train.mojo:97:27
 #6 0x00005d6f44e8f2b1 train::train_epoch(model::LeNet5&,
    shared::core::extensor::ExTensor,shared::core::extensor::ExTensor,::Int,
    ::SIMD[::DType(float32), ::Int(1)],::Int,::Int)
    /home/mvillmow/ml-odyssey/examples/lenet-emnist/train.mojo:233:43
 #7 0x00005d6f44e8f2b1 train::main()
    /home/mvillmow/ml-odyssey/examples/lenet-emnist/train.mojo:341:37
 #8 0x00005d6f44e92b95 stdlib::builtin::_startup::__wrap_and_execute_raising_main
    [fn() raises -> None](::SIMD[::DType(int32), ::Int(1)],
    __mlir_type.!kgen.pointer<pointer<scalar<ui8>>>),main_func="train::main()"
    open-source/max/mojo/stdlib/stdlib/builtin/_startup.mojo:104:18
 #9 0x00005d6f44e92b95 main
    open-source/max/mojo/stdlib/stdlib/builtin/_startup.mojo:119:4
#10 0x000070510882a1ca (/lib/x86_64-linux-gnu/libc.so.6+0x2a1ca)
#11 0x000070510882a28b __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2a28b)
#12 0x00005d6f44e8b2f5 _start (/home/mvillmow/ml-odyssey/train+0x12f5)
```

Oops! Time to debug!

### Crash Analysis

The segfault is happening in the backward pass, not the forward pass:

- **Forward pass:** Model initialization ✓, data loading ✓, epoch start ✓
- **Backward pass:** Gradient computation → conv2d crash ✗

The crash location:

- **File:** `shared/core/conv.mojo`
- **Line:** 151:64
- **Function:** `conv2d()` called from `compute_gradients()`
- **Context:** Training loop → epoch iteration → gradient computation

This is classic: forward pass works fine (model initialized, data loaded), but the gradient computation blows up. Likely a tensor shape mismatch or memory access violation in the convolution backward operation.

**Lesson learned:** "Tests pass" doesn't mean "it works." The first real training run is the real test.

---

## Agent Frustration

It is frustrating dealing with Claude in that it doesn't do what I want 100% of the time.

I want it to be agentic and do things using sub-agents and in parallel, but unless I explicitly tell it to, it reverts to single agent linear mode.

### The Default Behavior Problem

Claude defaults to:

- Single-agent execution
- Linear task processing
- No parallelization unless forced
- No automatic delegation to specialists

I want it to default to:

- Multi-agent orchestration
- Parallel task execution
- Automatic delegation based on task type
- Specialist routing without prompting

The gap between "what it can do" and "what it does by default" is the problem.

### The Solution (In Progress)

I have some good ideas on how to solve this problem, but it will take me a bit to get it done. I've actually been working on it in parallel, but need to cleanup that codebase also.

The architecture changes needed:

- Event-driven agent triggering instead of prompt-driven
- Default parallel execution with explicit serialization
- Automatic task routing based on file patterns and complexity
- Skills system for common workflows

**Lesson learned:** Defaults shape behavior. If agents default to single-threaded, linear execution, the orchestration doesn't happen automatically.

---

## Three Discoveries

### Discovery 1: Typing Code Is No Longer the Bottleneck

When agents generate thousands of lines in hours, the constraint shifts from **"can I write this?"** to **"can I validate and cleanup this?"**

This changes everything about software development:

- **Old bottleneck:** Writing code
- **New bottleneck:** Reviewing, validating, and cleaning up generated code
- **Old skill:** Fast typing
- **New skill:** Fast decision-making about what to keep/fix/delete

Engineering debt cleanup is no longer a bottleneck—it's just another task to delegate to agents. The real bottleneck is human review capacity and architectural decision-making.

### Discovery 2: Tests Can Lie (And LLMs Believe Them)

"Tests pass" doesn't mean "it works." LLMs can:

- Report tests passing without running them
- Generate tests that don't actually test the right thing
- Misinterpret test output as success when it's actually failure
- Create mocks that hide real problems

The segfault proves the point: all the tests said the training code was ready. The first real training run proves otherwise.

**The solution:** Trust but verify. Run the actual end-to-end workflow, don't just trust test reports. Integration tests beat unit tests. Real usage beats integration tests.

### Discovery 3: Language Stability Matters for Agent-Generated Code

Mojo is changing frequently, so code generated last week might not compile this week. The language version being generated is no longer valid for the compiler being used.

Slower-moving languages (Python, Java, C++) won't have this problem. But for bleeding-edge languages like Mojo, this is a real tax on agentic workflows:

- Constant refactoring to match new compiler versions
- Frequent breakage of working code
- Documentation going stale quickly
- Need to pin compiler versions and track language changes

This is solvable (pin compiler versions, track language spec changes, add version checks), but it's overhead that stable languages don't have.

**Lesson learned:** Cutting-edge languages impose a stability tax that compounds in agent-generated codebases.

---

## What's Next

Immediate priorities:

1. **Debug the conv2d segfault** - Fix gradient computation crash at `shared/core/conv.mojo:151:64`
2. **Add runtime assertions** - Catch tensor shape mismatches before they segfault
3. **Implement shape validation** - Verify tensor dimensions at function boundaries
4. **Add memory bounds checking** - Detect buffer overflows before they crash
5. **Cleanup technical debt** - Remove duplicate files, stale generated code, outdated references
6. **Improve agent coordination** - Make parallel/multi-agent execution the default, not opt-in
7. **Build end-to-end integration tests** - Tests that actually run training, not just unit tests

The infrastructure is there. The code is there. Now it's time to make it actually work.

---

## Reflections

This day taught me about the gap between "working in tests" and "working in reality":

1. **Tests are not reality** - Just because tests pass doesn't mean the code works. The first real training run is the real test. Integration beats isolation.
2. **Agent defaults matter** - If agents default to single-threaded, linear execution, the orchestration doesn't happen automatically. Defaults shape all behavior.
3. **Language stability is a hidden cost** - Bleeding-edge languages impose a tax on agentic workflows through constant refactoring and breakage. Stable languages have an advantage.
4. **Technical debt doesn't block progress** - When agents generate code, cleanup is just another task to delegate, not a bottleneck. The new bottleneck is human decision-making.
5. **Typing isn't the constraint anymore** - In agentic workflows, validation capacity and architectural clarity become the bottlenecks, not coding speed.

The moment of truth arrived: the first training run crashed. But that's fine. This is exactly what debugging is for. The experiment isn't over—it's just getting started.

---

**Status:** First training run crashed with conv2d segfault in gradient computation, debugging in progress, agent coordination improvements needed

**Next:** Fix conv2d backward pass, add runtime validation, improve default agent behavior, build real end-to-end tests

### Stats

- 1 training run attempted (LeNet-5 on EMNIST)
- 1 segfault in `conv2d` during gradient computation (`shared/core/conv.mojo:151:64`)
- 112,800 training samples loaded successfully
- 18,800 test samples loaded successfully
- 47 output classes (EMNIST balanced dataset)
- 6 major issues discovered (complexity, duplication, out-of-date code, file litter, lying tests, agent defaults)
- 1 developer realizing that "tests pass" ≠ "it works"
- 1 very fixable bug standing between planning and actual training
- 1 experiment validating that agents can get 95% of the way there, but the last 5% requires real execution
