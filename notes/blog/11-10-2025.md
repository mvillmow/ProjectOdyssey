# Day Four: Agents vs. Linters

**Project:** ML Odyssey Manual
**Date:** November 10, 2025
**Branch:** `blog-11-10-2025`
**Tags:** #agents #linting #model-selection #token-optimization #architecture

---

## TL;DR

Spent the day analyzing why AI-generated code doesn't pass linters, then optimized the agent system
to use cheaper models where they're good enough. The core insight: training data is polluted with
unlinted code, so models learn to generate unlinted output, increasing token count. Working around
that by optimizing model tier assignments (Opus for planning, Sonnet for design, Haiku for simple
tasks). Made 2 commits analyzing the linting cost problem and model-tier optimization. Also read
[Anthropic's multi-agent research blog](https://www.anthropic.com/engineering/multi-agent-research-system)
and realized I've independently stumbled into the same patterns they're experimenting with.

**Key commits:** [`fbf9a13`](https://github.com/mvillmow/ml-odyssey/commit/fbf9a13) (agentic
workflows notes), [`404486f`](https://github.com/mvillmow/ml-odyssey/commit/404486f) (optimize
model tier assignments)

---

## The Linting Problem

AI agents produce code that doesn't pass linters. This seems obvious in retrospect, but it's worth
digging into *why*.

Many developers don't use linters. That means the training data—the internet's code repositories,
GitHub, tutorials, examples—is polluted with unlinted code. When you train a model on that data, it
learns to generate unlinted code.

So when I ask an agent to generate markdown, it generates markdown with:

- Missing blank lines around code blocks
- Code blocks without language tags
- Missing blank lines around lists
- Lines exceeding 120 characters
- Missing blank lines around headings

And when I ask it to generate Mojo code, it generates code that `mojo format` complains about.

### Why This Matters

Each linting fix cycle burns tokens. I've spent significant tokens just fixing what should have been
output correctly in the first place.

Looking at recent commits:

- [`4f955af`](https://github.com/mvillmow/ml-odyssey/commit/4f955af) - Markdown linting fixes
- [`3a90ec3`](https://github.com/mvillmow/ml-odyssey/commit/3a90ec3) - Mojo formatting
- [`0f3d455`](https://github.com/mvillmow/ml-odyssey/commit/0f3d455) - More Mojo formatting
- [`1e3222d`](https://github.com/mvillmow/ml-odyssey/commit/1e3222d) - Link check fixes

#### Example 1: Markdown Linting (4f955af)

### Before

```markdown
**Core Module** (18 components):
- 6 High Complexity (Senior Engineer): Tensor, Linear, Conv2D, SGD, Adam, AdamW
```text

### After:

```markdown
**Core Module** (18 components):

- 6 High Complexity (Senior Engineer): Tensor, Linear, Conv2D, SGD, Adam, AdamW
```text

Missing blank line after header. The linter wants breathing room.

#### Example 2: Mojo Formatting (3a90ec3)

### Before:

```mojo
results.append(BenchmarkResult(
    name="SGD-placeholder",
    duration_ms=0.0,
    throughput=0.0,
    memory_mb=0.0
))
```text

### After:

```mojo
results.append(
    BenchmarkResult(
        name="SGD-placeholder",
        duration_ms=0.0,
        throughput=0.0,
        memory_mb=0.0,
    )
)
```text

`mojo format` wants trailing commas, different indentation, and the constructor call on its own line.

#### Example 3: More Mojo Formatting (0f3d455)

### Before:

```mojo
# ============================================================================
# Linear Layer Tests

# ============================================================================
```text

### After:

```mojo
# ============================================================================
# Linear Layer Tests
# ============================================================================
```text

This one's subtle: the blank line should be *after* the closing separator, not in the middle.

#### Example 4: Link Check Fixes (1e3222d)

### Before:

```markdown
- [Test Architecture](../../../../../../home/mvillmow/ml-odyssey/notes/issues/48/test-architecture.md)
```

### After:

```markdown
- [Test Architecture](https://github.com/mvillmow/ml-odyssey/issues/48)
```

Absolute paths break the link checker. Use GitHub issue links for issue-specific content.

Rough estimate: **10k tokens per round of linting analysis, multiple rounds per day**. Extrapolate
that across a 1,655-issue project and you're looking at serious token burn.

This doesn't seem like a big deal, but small inefficencies at scale are expensive!

### The Solution (For Model Owners)

This is a training data problem, not an inference problem. Model owners should:

1. **Clean training data** - Run datasets through linters before training
1. **Establish standard format** - Use one linter per language as the canonical format
1. **Verify at training time** - Make sure models learn to generate linter-compliant output

Cost upfront: Data cleanup cost and higher token usage during training.
Cost downtime: Massive token savings for every user relying on that model for code generation.

---

## The Model Tier Problem

While thinking about linting inefficiency, I also realized something else: **I'm using the wrong
models for the wrong tasks**.

Token limits killed me on day 3. I was burning through budget because every agent was using the
most capable 4.5 model(Sonnet).

Looking at my current setup, I was running most tasks on what I thought was optimized. But then I
looked at the actual model capabilities and realized: **not all tasks need the same reasoning power**.

### The Analysis

High-level tasks need high-level reasoning:

- **Planning & Architecture** → Opus (most capable, slowest, most expensive)
- **Design & Orchestration** → Sonnet (good balance of capability and cost)
- **Simple Execution** → Haiku (fast, cheap, surprisingly capable)

Looking at Anthropic's own benchmarks:

- Haiku 4.5 is nearly at Opus 4 level on many tasks and competitive with Sonnet 4
- Sonnet 4.5 is ahead of Opus 4.1 on most metrics

So the question isn't "what's the best model?" but "what's the best model *for this specific task*?"

### The Optimization

I updated agent assignments to use tiered models based on task complexity:

- **Level 0-1 (Planning/Orchestration)** → Opus (complex decisions)
- **Level 2 (Design)** → Sonnet (architectural decisions)
- **Level 3-4 (Specialists/Implementation)** → Haiku/Sonnet (depends on task)
- **Level 5 (Junior/Boilerplate)** → Haiku (straightforward execution)

Commit [`404486f`](https://github.com/mvillmow/ml-odyssey/commit/404486f) optimizes model tier
assignments across the 48-agent hierarchy.

The math:

- All Haiku: Fast but might miss nuance
- All Opus or Sonnet: Guaranteed quality but burns tokens (what I was doing)
- Mixed tiers: Best quality-per-token ratio

---

## The Discovery: Anthropic's Multi-Agent Research

While researching this, I found Anthropic's blog post on multi-agent research systems:
[anthropic.com/engineering/multi-agent-research-system](https://www.anthropic.com/engineering/multi-agent-research-system)

This is *remarkably* similar to what I've independently built:

- **Multi-level hierarchy** - My 6 levels extends their planning/execution/coordination structure
- **Specialized roles** - My 48 agents with specific domains mirrors their specialist pattern
- **Token efficiency** - They're solving the exact same "how do we avoid re-planning?" problem I hit
- **Orchestration patterns** - Similar routing logic to get tasks to the right specialist

They mention using tool servers (like Claude's MCP protocol) to offload repetitive work. I haven't
done that yet, but it's on my list.

---

## Three Discoveries

### Discovery 1: Training Data Quality Cascades

Poor training data (unlinted code) produces poor outputs (unlinted code), which burns tokens fixing
it downstream. This is a market failure that model owners could fix by cleaning their training data.

The hidden cost of serving unlinted code generated models is borne by users, not model makers.

### Discovery 2: Model Selection Is Task-Specific

I was over-specifying by using one model tier for everything. I currently believe the right approach is:

- **Use Opus only for high-complexity planning tasks**
- **Use Sonnet for design, coordination, and complex implementation**
- **Use Haiku for straightforward execution**

This reduces token burn while maintaining quality. It's a classic optimization: the right tool for
the job, not the best tool for every job.

### Discovery 3: I'm Following Anthropic's Playbook

The multi-agent architecture I built independently is very close to what Anthropic is actively
researching. That's either validation that I'm on the right track, or a warning sign that it's a
harder problem than it looks.

Probably both.

---

## What's Next

1. **Implement MCP servers** - Offload repetitive operations to avoid token burn on boilerplate
1. **Build agent token tracking** - Measure actual token usage per agent to optimize further
1. **Create agentic language** - Design, or use, a DSL for dynamic workflows that doesn't require LLM reasoning
1. **Start Mojo implementation** - Foundation is almost done, soon it will be time for actual ML code
1. **Continue model tier tuning** - Gather data on which models work best for each task

The infrastructure is solid. Now it's about making it *efficient* before diving into the core ML
work.

---

## Reflections

This day was less about building and more about *analyzing what I built*:

1. **Linting is a training data problem** - Not something users should have to pay to fix
1. **Model selection matters** - Using the right model for the task saves tokens and time
1. **Anthropic's research validates the approach** - Good sign that others are exploring similar
   patterns
1. **Token budgets force better design** - Constraints breed innovation

I'm not yet into the core Mojo neural network work. Still in infrastructure and optimization. But
that's probably fine, getting this right now means the actual ML work will be faster and cheaper.
This kind of work used to take weeks/months, now it can be done in days, so the cost of focusing on
it up front will pay off quickly.

Let's see if the tiered model approach actually helps.

---

**Status:** Agent system optimized with tiered model assignments, linting strategy documented,
infrastructure complete

**Next:** Implement MCP servers and token tracking, then begin Mojo implementation

### Stats:

- 2 commits analyzing linting and model optimization
- 48 agents assigned optimal model tiers
- ~40 recent linting issues analyzed
- 1 developer realizing models should ship pre-linted
- 1 moment of validation from Anthropic's research team (hypothetical but satisfying)
