# 11-10-2025: Agents vs Linters

Today I'm going to focus on two aspects, agents and linters, and how they often cause problems with each other. Agents produce code thas flaws, and linters tell the agent what the flaws are. It looks like there is a flaw with current agents, they don't do well outputing documents in a way that follows standard linting tools. I'm wondering if this is due to most developers not using linters, thus polluting the training set. I think this is something that model owners need to fix, where they clean up their data sets to pass linters, or a sane subset of them. I don't really think it matters what linter for each programming language they use to cleanup their datasets, just that some linter cleans up the dataset. Sure it might cost extra tokens to put out properly formatted inputs, but I think it would drastically help the output of coding tools. If the agent puts it out into a linting format, then this can be documented, and can become the defacto standard linter for that language. This doesn't stop other linters from fixing up the code, but it would make it easier since the code would be in a sane format already. Right now i'm spending quite a bit of effort(i.e. tokens) just fixing linting issues.


## Linting

To give an example of what the problem is, lets take a look at some recent linter related commits.

### Markdown linting issues

<insert examples for markdown from https://github.com/mvillmow/ml-odyssey/commit/4f955afa8adc46e658526e21ed4d909b63f6abee>

### Mojo linting issues

<insert examples for mojo from https://github.com/mvillmow/ml-odyssey/commit/3a90ec365f0d051670c4ec72092692cd308129f5 and https://github.com/mvillmow/ml-odyssey/commit/0f3d4558378df94baec1dda0b8b56e5bfa502c46>

### Link check issues

<insert examples for link-check from https://github.com/mvillmow/ml-odyssey/commit/1e3222d7f1fb9de76f0bee17f5e6dd62fc1138b5>

<Give a estimate on the number of tokens this takes, with an assumption on 10k tokens for analysis and then write a blob explaining how this would save money in the long run by extrapolating the cost of linting at training time vs linting at inference time>

## Agents

Since I'm running into issues with token limits, I've also started analyzing what models to use with what agents. Its obvious that hihg level architecture models that do planning need to prioritize Opus for planning, but what about orchestration? Testing? Implementation? or even Review? What about cases where its known exactly what needs to be implemented and thinking doesn't need to occur at much detail. I think there is a case to be made to have simpler tasks automatically use the cheaper models, so I went ahead and analyzed all the agents to figure out which ones belong to which category. I've made changes to the agentic flow so that by default they use Opus, Sonnet, or Haiku, instead of everything using Haiku.

### Agentic Key

<describe the process for selecting agents based on this PR https://github.com/mvillmow/ml-odyssey/commit/404486f50b56411ff95207994742a1a608e538a0>

<provide a key documenting the agent to color mapping>

### Agentic Layout

To give the reader a clear picture, here is the current layout of the agents.

<Insert a mermaid or other text based graph into the markdown that gives me the 48 agents as a directed graph, with each node representing an agent, and the links between nodes are the delegation or response direction. Have Opus be red, sonnet be green, and haiku be orange. Make sure both the agent and model names are visible>

## Summary

I'm still experimenting with agents and using simple tasks to fine tune them. Running everything at Opus level blows through token budgets to quickly, and even using Sonnet which is 5x cheaper than Opus, using parallel sub-agents, the token budget gets hit within the time frame. My hope is that adding Haiku, which is 3x cheaper than Sonnet(15x cheaper than Opus), yet scores close enough to Sonnet and Opus that I don't think the quality will suffer. Honestly, I'm not really sure where Opus even fits in anymore since Haiku is 15x cheaper and is only generation behind in performance. Don't believe me? Look at Anthropic's own numbers.

<Show this image
https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F029af67124b67bdf0b50691a8921b46252c023d2-1920x1625.png&w=3840&q=75>

<Also show this image
https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fbde326699c667506c87f74b09a6355961d29eb26-2600x2084.png&w=3840&q=75>

This basically haiku between Opus 4 and Opus 4.1, and Opus 4.1 behind Sonnet 4.5. My guess is they somehow believe that Opus is 'too powerful' to release the 4.5 version, but looking at Opus 4 vs Sonnet 4, I don't see the 5x cost benefit showing up even there.


## Extra information

I was reading through some blog posts and ran across something interesting from Anthropic. https://www.anthropic.com/engineering/multi-agent-research-system. This is very similar to the system I've stumbled into through my experience and running into some of the same problems with token usage. I have some ideas for getting aroudn these limits, but I need to better utilize the file system to store intermediate results. I'm thinking of creating my own agentic language for dynamic flows so that it isn't burning through tokens at such a high pace. To do that I need to build some infrastructure to measure agentic token usage, but that is something for the future.
