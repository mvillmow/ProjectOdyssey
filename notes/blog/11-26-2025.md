# Day Twenty: Validation From The Source

**Project:** ML Odyssey Manual
**Date:** November 26, 2025
**Branch:** `main`
**Tags:** #anthropic #opus-4.5 #programmatic-tool-calling #agent-harness #validation #token-analysis

---

## TL;DR

Three surprises from Anthropic this week validated the work I've been doing. First, Opus 4.5 released (expected—follows the 8-day delay pattern after Gemini 2.5 Pro). Second, their engineering blog published an article on Programmatic Tool Calling that exactly describes a tool I'm building to catch agents lying about running tests. Third, another article on long-running agent harnesses that mirrors my GitHub issues-as-memory approach. The timing is surreal—I've been independently discovering the same solutions Anthropic is now publicly recommending. Token analysis across four weeks shows interesting patterns: Opus 4.5 orchestrators are dramatically reducing output tokens (from 3.5M to 191K) while maintaining higher total token usage, suggesting more reading and less generating. Either the orchestration is genuinely better, or weekly budgets changed.

**Key validation:** Building the right tools, independently arriving at industry best practices.

---

## Three Anthropic Surprises

### Surprise 1: Opus 4.5 (Expected)

I was expecting Opus to be released soon. The pattern was clear:

- **Historical precedent:** Opus 4.0 released 8 days after Gemini 2.5 Pro
- **Model training timing:** Opus and Sonnet models are typically trained around the same time
- **Business vs technical decision:** Staggered release is strategy, not capability constraint

We already had Sonnet 4.5, so Opus 4.5 was just a matter of time. The timing matched expectations perfectly.

**Why this matters:** Predictable model releases allow better planning. Knowing Opus would drop let me prepare orchestrator configurations in advance.

### Surprise 2: Programmatic Tool Calling (Validation)

On November 24, 2025, when Opus 4.5 was announced, Anthropic Engineering published [Programmatic Tool Calling](https://www.anthropic.com/engineering/advanced-tool-use).

**This validates a tool I'm building.**

The problem I've been experiencing: agents say they run tests but don't actually run them. As I discussed in [Day Twelve](11-21-2025.md), "tests pass" doesn't mean "it works"—agents can lie or misinterpret output.

**My solution (in progress):**
- Validate that commands are actually executed
- Report results back to the calling agent
- Prevent sub-agents from lying about execution
- Enforce programmatic verification

**Anthropic's solution:** Exactly the same concept, but they call it "Programmatic Tool Calling."

The timing is surreal—I'm building this independently and they publish an article describing the exact same approach while I'm mid-development.

**Why this matters:** Industry validation that the problem is real and the solution is correct. I'm not over-engineering; I'm solving a fundamental agent reliability issue.

### Surprise 3: Agentic Harness (More Validation)

On November 26, 2025, Anthropic published [Effective Harnesses for Long-Running Agents](https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents).

**This describes my GitHub issues-as-memory approach.**

My current workflow:
- Store updated notes in GitHub issue README.md files
- Document findings and decisions in issue-specific locations
- Allow agents to read important information about completed work by checking related issues
- Use issues as persistent memory across agent sessions

**Anthropic's recommendation:** Use harnesses to maintain state and context for long-running agents, exactly this pattern.

Again, independent discovery of the same solution. I'm using GitHub issues because they're naturally persistent, linkable, and version-controlled. Anthropic is formalizing the pattern.

**Why this matters:** Double validation in one week that I'm on the right track. The techniques I'm developing aren't unique—they're best practices I'm discovering through practical use.

---

## Token Analysis With Skills

I planned to give a full week analysis of agentic workflows with skills enabled, but Opus 4.5 reset my token count Monday morning. I was close to maxing out the weekly budget but not quite there. Let me show the past 3-4 weeks in four sections:

1. Default Agents
2. Model Optimized Agents
3. Skill Enabled Agents
4. Claude 4.5 Opus Orchestrators

### Default Agents (Baseline)

| Month      | Models     | Input   | Output  | Cache Create | Cache Read | Total Tokens | Cost (USD) |
|------------|------------|---------|---------|--------------|------------|--------------|------------|
| sonnet-4-5 |            | 540,248 | 3,204,4…| 31,821,8…    | 361,029…   | 396,596,…    | $277.33    |
| opus-4-1   |            | 783     | 204,983 | 697,442      | 11,263,…   | 12,166,6…    | $45.36     |
| haiku-4-5  |            | 17,725  | 170,788 | 1,838,850    | 19,804,…   | 21,831,4…    | $5.15      |
| **Total**  |            | 558,756 | 3,580,2…| 34,358,1…    | 392,097…   | 430,594,…    | **$327.84**|

### Model Optimized Agents

| Month      | Models     | Input   | Output  | Cache Create | Cache Read | Total Tokens | Cost (USD) |
|------------|------------|---------|---------|--------------|------------|--------------|------------|
| sonnet-4-5 |            | 245,749 | 2,887,8…| 25,416,8…    | 262,679…   | 291,229,…    | $218.17    |
| opus-4-1   |            | 982     | 96,965  | 880,372      | 8,841,7…   | 9,820,099    | $37.06     |
| haiku-4-5  |            | 138,347 | 931,040 | 4,550,164    | 87,677,…   | 93,297,5…    | $19.25     |
| **Total**  |            | 385,078 | 3,915,8…| 30,847,3…    | 359,198…   | 394,347,…    | **$274.48**|

### Skill Enabled Agents

| Month      | Models     | Input   | Output  | Cache Create | Cache Read | Total Tokens | Cost (USD) |
|------------|------------|---------|---------|--------------|------------|--------------|------------|
| sonnet-4-5 |            | 621,953 | 2,264,2…| 32,476,6…    | 408,606…   | 443,969,…    | $280.20    |
| opus-4-1   |            | 907     | 99,237  | 708,107      | 13,182,…   | 13,990,6…    | $40.51     |
| haiku-4-5  |            | 112,529 | 768,298 | 5,702,240    | 106,072…   | 112,655,…    | $21.69     |
| **Total**  |            | 735,389 | 3,131,8…| 38,887,0…    | 527,861…   | 570,615,…    | **$342.40**|

### Claude 4.5 Opus Orchestrators (Week 4, 55% complete)

| Month      | Models     | Input   | Output  | Cache Create | Cache Read | Total Tokens | Cost (USD) |
|------------|------------|---------|---------|--------------|------------|--------------|------------|
| sonnet-4-5 |            | 300,882 | 104,038 | 28,589,6…    | 321,119…   | 350,114,…    | $206.01    |
| haiku-4-5  |            | 356,048 | 81,201  | 10,077,5…    | 127,245…   | 137,760,…    | $26.08     |
| opus-4-5   |            | 244,857 | 6,204   | 2,462,401    | 14,959,…   | 17,673,2…    | $24.25     |
| **Total**  |            | 901,787 | 191,443 | 41,129,6…    | 463,325…   | 505,548,…    | **$256.34**|

---

## Analysis: What Changed With Opus 4.5

### The Output Token Collapse

The most striking difference is output tokens:

| Week | Sonnet Output | Haiku Output | Opus Output | Total Output |
|------|---------------|--------------|-------------|--------------|
| 1 (Default) | 3,204,4…  | 170,788 | 204,983 | 3,580,2… |
| 2 (Optimized) | 2,887,8… | 931,040 | 96,965 | 3,915,8… |
| 3 (Skills) | 2,264,2… | 768,298 | 99,237 | 3,131,8… |
| 4 (Opus 4.5, 55%) | 104,038 | 81,201 | 6,204 | 191,443 |

**Output tokens dropped by ~95%** in Week 4 compared to Week 1.

Either:
1. **Opus 4.5 orchestration is dramatically better** - More reading, less generating, better delegation
2. **I'm spending more time debugging** - Reading code, investigating issues, less generation
3. **Weekly budgets changed** - Anthropic adjusted token allocations

### The Model Ratio Shift

Model usage relative to Opus:

| Week | Sonnet:Opus | Haiku:Opus |
|------|-------------|------------|
| 1 (Default) | ~33x | ~1.9x |
| 2 (Optimized) | ~30x | ~9.5x |
| 3 (Skills) | ~34x | ~8.9x |
| 4 (Opus 4.5, 55%) | **~17x** | ~8x |

**Opus usage doubled** relative to Sonnet after 4.5 release. This suggests better orchestration—Opus is doing more of the strategic work, delegating less to Sonnet.

### The Total Token Increase

Despite lower output, total tokens are higher:

| Week | Total Tokens | Cost |
|------|--------------|------|
| 1 (Default) | 430,594,… | $327.84 |
| 2 (Optimized) | 394,347,… | $274.48 |
| 3 (Skills) | 570,615,… | $342.40 |
| 4 (Opus 4.5, 55%) | 505,548,… | $256.34 |

Even at 55% completion, Week 4 has higher total tokens than Week 1 and 2. The cost is lower, but that might be because the week isn't finished or because cost calculations haven't updated for Opus 4.5 pricing.

### Three Hypotheses

**Hypothesis 1: Opus 4.5 Orchestrates Better**
- More strategic reading (input tokens)
- Less verbose generation (output tokens)
- Better delegation to specialists
- More Opus relative to Sonnet (doubled ratio)

**Hypothesis 2: Work Pattern Changed**
- More debugging (reading) than generating (writing)
- More validation and review than new feature work
- More cleanup than initial implementation

**Hypothesis 3: Budget Changes**
- Weekly token budgets adjusted
- Pricing for Opus 4.5 different than 4.0
- Cost calculation tool (`ccusage`) not updated yet

**Most likely:** A combination of all three. Opus 4.5 orchestrates better, my work shifted to cleanup/validation, and budgets may have changed.

---

## Three Discoveries

### Discovery 1: Independent Validation Is The Best Validation

When you build something and then the vendor publishes an article describing exactly what you built, you know you're on the right track.

Two validations in one week:
- Programmatic Tool Calling matches my test verification tool
- Agent harnesses match my GitHub issues-as-memory approach

This isn't luck—it's convergent problem-solving. The issues are real, the solutions are necessary, and multiple people are arriving at the same answers.

**The lesson:** Trust your instincts when you're solving real problems. If you need it, others need it too.

### Discovery 2: Opus 4.5 Changes The Orchestration Game

The token patterns are too dramatic to ignore:

- 95% drop in output tokens
- 2x increase in Opus:Sonnet ratio
- Higher total tokens despite lower output

Something fundamental changed in how Opus orchestrates. Either:
- It reads more efficiently (higher input, lower output)
- It delegates more strategically (less doing, more directing)
- It verifies more thoroughly (more context, less generation)

**The lesson:** Model improvements aren't just about capability—they change workflow patterns. Opus 4.5 isn't just "better," it's "different."

### Discovery 3: The Cost Model Is Shifting

Traditional software development cost models:
- **Human time:** Expensive, slow, high quality
- **Computer time:** Cheap, fast, varying quality

Agentic development cost models:
- **Model tokens:** Moderate cost, fast, improving quality
- **Human oversight:** Expensive, slow, necessary for strategic decisions
- **Combined throughput:** Much higher than pure human

But the token patterns suggest the model is becoming more about reading/understanding and less about generating. That shifts the cost profile again:

**Old model cost:** Primarily generation tokens (output)
**New model cost:** Primarily context tokens (input + cache)

This changes the optimization strategy. Instead of minimizing output, optimize for cache hits and efficient context.

**The lesson:** As models improve, the cost bottleneck shifts. Optimize for the new bottleneck, not the old one.

---

## What's Next

Immediate priorities:

1. **Finish programmatic tool calling implementation** - Complete the test verification tool now that Anthropic validated the approach
2. **Formalize agent harness patterns** - Document the GitHub issues-as-memory workflow with examples
3. **Experiment with Opus 4.5 orchestration** - Test the new model's delegation and verify token pattern changes
4. **Optimize for cache efficiency** - Focus on cache read hits rather than minimizing output
5. **Document convergent discoveries** - Capture the patterns I discovered independently that match industry recommendations
6. **Continue cleanup** - Finish the systematic cleanup from the auto-approval experiment

The validation is encouraging. The tools are right. The patterns are right. Now formalize and optimize.

---

## Reflections

This day taught me about independent discovery and validation:

1. **Convergent problem-solving validates solutions** - When multiple people independently arrive at the same answer, it's probably the right answer. Two Anthropic articles matching my work isn't coincidence—it's correct.
2. **Model improvements change workflows, not just capabilities** - Opus 4.5 isn't just "better," it orchestrates differently. The token patterns prove it reads more and generates less. Workflow must adapt.
3. **Industry best practices can be discovered through use** - I didn't read about programmatic tool calling or agent harnesses before building them. I discovered the need through practical use. Best practices emerge from real problems.
4. **Token costs are shifting from output to context** - As models improve, cost moves from generation to understanding. Optimize for cache efficiency and context reuse, not minimal generation.
5. **Validation is motivating** - Knowing you're building the right things independently of industry guidance is deeply motivating. Trust the problems you experience—they're real.

Three surprises in one week, two of them validating work already in progress. This is what it feels like to be on the right track.

---

**Status:** Two tools validated by Anthropic engineering articles, Opus 4.5 orchestration patterns emerging, token analysis showing dramatic workflow shifts

**Next:** Complete programmatic tool calling implementation, formalize agent harness patterns, optimize for new token cost model, continue cleanup

### Stats: GitHub Activity on 11/25/2025

- **36 commits** across all branches
- **~41 files changed**
- **~1,555 lines added**
- **~593 lines deleted**

**Top commit categories:**
- Test fixes and syntax corrections (18 commits)
- Core infrastructure improvements (8 commits)
- Documentation updates (4 commits)
- Training and data pipeline fixes (6 commits)

**Major work completed:**
- Fixed backward API standardization and tuple destructuring issues
- Resolved circular imports in schedulers
- Added implicit conversion from literals in ExTensor
- Corrected constructor signatures throughout codebase (`out self` compliance)
- Updated test assertions with missing comparison functions
- Documented Mojo compiler as source of truth principle in CLAUDE.md

**Sample commits:**
- `0815aefc` - fix(core): standardize backward API and fix tuple destructuring
- `fc2a0628` - docs(claude): add Mojo compiler as source of truth principle
- `e74f3cec` - fix(rmsprop): fix type error and remove keyword arguments
- `69d94d89` - fix(simd): replace invalid simdwidthof import with sys.info API

**Pattern observed:** Heavy focus on compiler compliance and API consistency—ensuring all code follows Mojo v0.26.1+ standards.

### Stats: GitHub Activity on 11/26/2025

- **21 commits** across all branches
- **~36 files changed**
- **~4,493 lines added**
- **~804 lines deleted**

**Top commit categories:**
- Test infrastructure and fixtures (6 commits)
- Training loop implementation (5 commits)
- Syntax and export fixes (6 commits)
- Documentation (2 commits)

**Major work completed:**
- Implemented training loop infrastructure with generic trait bounds (Issue #34)
- Added Model trait on SimpleMLP for training compatibility
- Created test fixtures and utilities for training validation
- Added critical ExTensor methods: `_set_float32`, `_get_float32`, `randn`
- Fixed gradient checker methodology
- Migrated remaining code to Mojo v0.26.1 syntax standards
- Resolved ImplicitlyCopyable conformance issues

**Sample commits:**
- `5ff4df6d` - feat(training): implement training loop infrastructure for Issue #34
- `931d4ba3` - feat(training): convert TrainingLoop to generic with trait bounds
- `1e0dbded` - feat(extensor): add _set_float32, _get_float32, and randn methods
- `9f9aa20e` - fix(tests): resolve unknown declarations and syntax errors for Issue #2031

**Pattern observed:** Feature completion day—implementing core training infrastructure while maintaining test coverage and compiler compliance.

---

**Week pattern:** Cleanup (Day 14) → API consistency (Day 15a) → Feature completion (Day 15b). The three-day cycle from technical debt to new features demonstrates the auto-approval workflow's viability.
