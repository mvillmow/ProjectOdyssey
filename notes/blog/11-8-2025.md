# Day Two: The Review System Takes Shape

**Project:** ML Odyssey Manual
**Date:** November 8, 2025
**Branch:** `log-11-8-2025`
**Tags:** #agents #code-review #markdown-linting #pre-commit #testing #specialization

---

## TL;DR

Spent the day fixing the agent system that I thought was "done" yesterday. Turns out generalist agents try to
do too much at once, so I built a specialized review hierarchy instead: 14 review specialist agents coordinated
by a new Level 2 orchestrator. Also discovered that AI-generated markdown is a linting nightmare—fixed 1,100+
errors across the codebase. Added pre-commit hooks to catch this stuff automatically. Made 25 commits total,
added ~11,000 lines of agent configuration code, and shipped
[Issue #65](https://github.com/mvillmow/ml-odyssey/issues/65) for agent packaging. The system is starting to look
like a real software organization.

**Key commits:** [`5f3de76`](https://github.com/mvillmow/ml-odyssey/commit/5f3de76) (14 review agents),
[`442d398`](https://github.com/mvillmow/ml-odyssey/commit/442d398) (markdown standards),
[`6b622fd`](https://github.com/mvillmow/ml-odyssey/commit/6b622fd) (final linting fixes)

---

## The Generalist Problem

Yesterday I thought the 6-level agent hierarchy was done. This morning I realized it wasn't.

The base-level agents I'd built were **too general**. When I asked an agent to review code, it tried to check
implementation quality, security, performance, documentation, test coverage, AND style all at once. The result?
Superficial reviews that missed details and gave me way too much feedback to process.

I wanted targeted, specific changes. Not "here are 47 things wrong with your code" but "here are the 3 security
issues" or "here's what's missing from your tests."

### The Solution: Extreme Specialization

I decided to go the opposite direction: **make each agent do exactly one thing**.

Built out a complete code review hierarchy:

- **1 Code Review Orchestrator** (Level 2) - Routes code changes to the right specialists
- **13 Review Specialists** (Level 3) - Each owns a specific review domain

The specialists:

1. **Implementation Review** - Code quality, logic, maintainability
2. **Documentation Review** - Docs clarity, completeness, accuracy
3. **Test Review** - Test coverage, assertions, edge cases
4. **Architecture Review** - System design, modularity, patterns
5. **Security Review** - Vulnerabilities, auth, crypto, OWASP Top 10
6. **Safety Review** - Memory safety, type safety, buffer overflows
7. **Mojo Language Review** - Mojo idioms, SIMD, ownership patterns
8. **Performance Review** - Runtime perf, algorithmic complexity, I/O
9. **Algorithm Review** - Mathematical correctness, numerical stability
10. **Data Engineering Review** - Data pipelines, preprocessing, loaders
11. **Paper Review** - Academic quality, citations, writing clarity
12. **Research Review** - Methodology, reproducibility, scientific rigor
13. **Dependency Review** - Version pinning, licenses, reproducibility

Each specialist has zero overlap with the others. The orchestrator decides which specialists to invoke based on
what files changed.

See commits [`5f3de76`](https://github.com/mvillmow/ml-odyssey/commit/5f3de76) (agents) and
[`cd1136e`](https://github.com/mvillmow/ml-odyssey/commit/cd1136e) (templates).

**Lines of code:** ~10,400 for agents, ~1,080 for templates. Yeah, I went big again.

---

## The Complete Agent Hierarchy

With the review specialists added, here's the full system:

```text
┌─────────────────────────────────────────────────────────────┐
│                    Level 0: Meta-Orchestrator                │
│                   Chief Architect Agent                      │
│         (System-wide decisions, paper selection)             │
└───────────────────────────┬─────────────────────────────────┘
                            │
        ┌───────────────────┼───────────────────┐
        ▼                   ▼                   ▼
┌──────────────────┐ ┌──────────────────┐ ┌──────────────────┐
│   Level 1:       │ │   Level 1:       │ │   Level 1:       │
│   Foundation     │ │ Shared Library   │ │    Tooling       │
│  Orchestrator    │ │  Orchestrator    │ │  Orchestrator    │
└────────┬─────────┘ └────────┬─────────┘ └────────┬─────────┘
         │                    │                     │
┌────────┴────────┐  ┌────────┴────────┐  ┌────────┴────────┐
│  Level 1: Paper │  │  Level 1: CI/CD │  │ Level 1: Agentic│
│ Implementation  │  │  Orchestrator   │  │    Workflows    │
│  Orchestrator   │  │                 │  │  Orchestrator   │
└────────┬────────┘  └────────┬────────┘  └────────┬────────┘
         │                    │                     │
         └────────────────────┼─────────────────────┘
                              ▼
            ┌─────────────────────────────────────┐
            │      Level 2: Module Design Agents  │
            ├─────────────────────────────────────┤
            │  • Architecture Design Agent        │
            │  • Integration Design Agent         │
            │  • Security Design Agent            │
            │  • Code Review Orchestrator (NEW)   │
            └──────────────────┬──────────────────┘
                               │
                               ▼
            ┌─────────────────────────────────────┐
            │   Level 3: Component Specialists     │
            ├──────────────────────────────────────┤
            │  Implementation Specialists:         │
            │  • Senior Implementation Specialist  │
            │  • Test Design Specialist            │
            │  • Documentation Specialist          │
            │  • Performance Specialist            │
            │  • Security Implementation Specialist│
            │                                      │
            │  Review Specialists (NEW):           │
            │  • Implementation Review             │
            │  • Documentation Review              │
            │  • Test Review                       │
            │  • Architecture Review               │
            │  • Security Review                   │
            │  • Safety Review                     │
            │  • Mojo Language Review              │
            │  • Performance Review                │
            │  • Algorithm Review                  │
            │  • Data Engineering Review           │
            │  • Paper Review                      │
            │  • Research Review                   │
            │  • Dependency Review                 │
            └──────────────────┬──────────────────┘
                               │
                               ▼
            ┌─────────────────────────────────────┐
            │   Level 4: Implementation Engineers  │
            ├──────────────────────────────────────┤
            │  • Senior Implementation Engineer    │
            │  • Implementation Engineer           │
            │  • Test Engineer                     │
            │  • Documentation Writer              │
            │  • Performance Engineer              │
            └──────────────────┬──────────────────┘
                               │
                               ▼
            ┌─────────────────────────────────────┐
            │      Level 5: Junior Engineers       │
            ├──────────────────────────────────────┤
            │  • Junior Implementation Engineer    │
            │  • Junior Test Engineer              │
            │  • Junior Documentation Engineer     │
            └──────────────────────────────────────┘
```

The system now has **18 specialist types at Level 3** (5 implementation + 13 review). This is getting close to
how real engineering orgs work: lots of specialists, coordinated by orchestrators.

Commit [`b0f1e8a`](https://github.com/mvillmow/ml-odyssey/commit/b0f1e8a) completed the initial 6-level
hierarchy. The review specialists came later in [`5f3de76`](https://github.com/mvillmow/ml-odyssey/commit/5f3de76).

---

## The Markdown Linting Marathon

So here's the thing about having AI agents generate all your documentation: **they don't write markdown that
lints**.

I added pre-commit hooks this morning ([`4beaced`](https://github.com/mvillmow/ml-odyssey/commit/4beaced)) to
enforce code quality. Configured `markdownlint-cli2` to check all `.md` files. Ran it.

**1,100+ errors.**

Yep. Over a thousand markdown linting violations across the entire codebase. Most of them were:

- Missing blank lines around code blocks
- Code blocks without language tags
- Missing blank lines around lists
- Lines exceeding 120 characters
- Missing blank lines around headings

### The Fix Marathon

I spent most of the afternoon fixing these. It took **6 rounds** of commits:

1. [`442d398`](https://github.com/mvillmow/ml-odyssey/commit/442d398) - Documented markdown standards, fixed
   many files
2. [`c704b38`](https://github.com/mvillmow/ml-odyssey/commit/c704b38) - Systematically fixed 1,100+ errors
3. [`ac280c8`](https://github.com/mvillmow/ml-odyssey/commit/ac280c8) - Down to 7 errors remaining
4. [`8654f59`](https://github.com/mvillmow/ml-odyssey/commit/8654f59) - Fixed final line length errors
5. [`d49b89e`](https://github.com/mvillmow/ml-odyssey/commit/d49b89e) - Fixed packaging docs
6. [`6b622fd`](https://github.com/mvillmow/ml-odyssey/commit/6b622fd) - Resolved all remaining 53 errors

By the end, I'd fixed every single markdown file in the repository. Pre-commit now runs `markdownlint-cli2` on
every commit. CI enforces it on every PR.

I also wrote comprehensive markdown standards in
[`CLAUDE.md`](https://github.com/mvillmow/ml-odyssey/blob/main/CLAUDE.md#markdown-standards) so the agents
(and I) know the rules going forward.

**Lesson learned:** Linting is non-negotiable when working with AI-generated content. Build the automation
upfront, not after you have 1,100 errors.

---

## Three Discoveries

### Discovery 1: Specialization Beats Generalization

When I built generalist agents, they tried to do everything and did nothing well. When I built 13 laser-focused
review specialists, each one became genuinely useful.

This mirrors real engineering teams. You don't have one person review code, security, performance, AND tests.
You have specialists who each bring deep expertise in their domain.

The orchestrator pattern makes this work: it routes changes to the right specialists based on file types and
scope.

### Discovery 2: AI Markdown Doesn't Lint

I knew AI output needed review. I didn't realize it would produce **1,100+ linting errors** across 200+ files.

The solution: pre-commit automation + explicit standards documentation + lots of manual fixing.

Next step: build a skill system so I can run `/precommit-fix` and have an agent automatically resolve all
linting errors. But that's future work.

### Discovery 3: Script Proliferation

I'm noticing that agents like to create temporary scripts to accomplish tasks. Sometimes these scripts are
useful tests that should be tracked. Sometimes they're one-off throwaway code that shouldn't be in the repo.

I need to establish a pattern:

- **Test scripts** → `tests/` directory, committed to repo, added to CI/CD
- **Temporary scripts** → `/tmp/` or `logs/`, NOT committed to repo

This requires agent instructions to distinguish between "write a test" and "write a temporary script to check
something."

---

## What's Next

Immediate priorities:

1. **Implement the skills system** - Build slash commands like `/precommit-fix` that trigger specialized agent
   workflows
2. **Define script storage patterns** - Clear separation between temp scripts and test scripts
3. **Auto-add tests to CI/CD** - When new tests are created, automatically integrate them into the CI pipeline
4. **Test the review system on a real PR** - See if the 13 specialists actually work as intended
5. **Document the orchestrator routing logic** - How does the orchestrator decide which specialists to invoke?

The foundation is solid now. The agent system has clear levels, specialized roles, and no-overlap review
domains. Time to actually use it for something real.

---

## Reflections

This is turning into a fascinating experiment in AI-assisted development:

1. **Specialization scales better than generalization** - 13 focused specialists beat 1 generalist
2. **Automation is mandatory for AI content** - Can't manually review 1,100 errors
3. **Iterate quickly, fix later** - Yesterday's "done" became today's "needs total rework"
4. **The right structure emerges through use** - I didn't know I needed review specialists until I tried using
   generalists

I'm basically building a software organization chart, except the "people" are AI agents. It's weird. It's
working.

Tomorrow: skills system. Let's make this thing actually productive.

---

**Status:** Agent hierarchy expanded with 14 review specialists, markdown linting fully enforced, pre-commit
automation in place

**Next:** Skills system for automated fixes and agent-triggered workflows

**Stats:**

- 25 commits made
- ~11,400 lines of agent code added
- 1,100+ markdown errors fixed
- 14 new review specialists
- 1 very tired human
