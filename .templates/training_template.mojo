# train_{{model_snake}}.mojo
"""
Training script for {{model}} on {{dataset}}.

Auto-generated by scripts/generators/generate_training_script.py
"""

from models.{{model_snake}} import {{model}}
from shared.datasets.{{dataset_snake}} import {{dataset}}Dataset
from shared.training import {{optimizer}}, {{loss}}
from shared.training.early_stopping import EarlyStopping
from shared.training.checkpoint import CheckpointManager
from shared.core import ExTensor


# Hyperparameters
alias NUM_EPOCHS = {{epochs}}
alias BATCH_SIZE = {{batch_size}}
alias LEARNING_RATE = {{learning_rate}}
alias NUM_CLASSES = {{num_classes}}


fn train_epoch(
    model: {{model}},
    dataset: {{dataset}}Dataset,
    optimizer: {{optimizer}},
    criterion: {{loss}},
    batch_size: Int
) -> Float64:
    """Train for one epoch."""
    var total_loss: Float64 = 0.0
    var num_batches = len(dataset) // batch_size

    for batch_idx in range(num_batches):
        var start = batch_idx * batch_size
        var end = min(start + batch_size, len(dataset))
        var indices = List[Int]()
        for i in range(start, end):
            indices.append(i)

        var batch = dataset.get_batch(indices)
        var inputs = batch[0]
        var targets = batch[1]

        optimizer.zero_grad()
        var outputs = model.forward(inputs)
        var loss = criterion.forward(outputs, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / Float64(num_batches)


fn validate(
    model: {{model}},
    dataset: {{dataset}}Dataset,
    criterion: {{loss}},
    batch_size: Int
) -> Tuple[Float64, Float64]:
    """Validate model."""
    var total_loss: Float64 = 0.0
    var correct: Int = 0
    var total: Int = 0
    var num_batches = len(dataset) // batch_size

    for batch_idx in range(num_batches):
        var start = batch_idx * batch_size
        var end = min(start + batch_size, len(dataset))
        var indices = List[Int]()
        for i in range(start, end):
            indices.append(i)

        var batch = dataset.get_batch(indices)
        var inputs = batch[0]
        var targets = batch[1]

        var outputs = model.forward(inputs)
        var loss = criterion.forward(outputs, targets)
        total_loss += loss.item()

        var predictions = outputs.argmax(dim=-1)
        correct += (predictions == targets).sum().item()
        total += len(indices)

    return (total_loss / Float64(num_batches), Float64(correct) / Float64(total) * 100.0)


fn main():
    """Main training function."""
    print("Training {{model}} on {{dataset}}")

    var train_dataset = {{dataset}}Dataset(train=True)
    var test_dataset = {{dataset}}Dataset(train=False)

    var model = {{model}}(num_classes=NUM_CLASSES)
    var optimizer = {{optimizer}}(model.parameters(), lr=LEARNING_RATE)
    var criterion = {{loss}}()
    var early_stopping = EarlyStopping(patience=5)
    var checkpoint_mgr = CheckpointManager("checkpoints/{{model_snake}}")

    for epoch in range(NUM_EPOCHS):
        print("Epoch", epoch + 1, "/", NUM_EPOCHS)

        var train_loss = train_epoch(model, train_dataset, optimizer, criterion, BATCH_SIZE)
        var val_result = validate(model, test_dataset, criterion, BATCH_SIZE)

        print("  Train Loss:", train_loss)
        print("  Val Loss:", val_result[0], "| Val Acc:", val_result[1], "%")

        checkpoint_mgr.save(model, optimizer, epoch, {"val_loss": val_result[0]})

        if early_stopping.step(val_result[0], epoch):
            break

    print("Training complete!")
