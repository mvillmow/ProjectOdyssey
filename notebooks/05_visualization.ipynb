{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and Analysis\n",
    "\n",
    "Plotting and analyzing training results, model behavior, and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curves\n",
    "\n",
    "Visualizing loss and accuracy over epochs reveals training dynamics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from notebooks.utils import plot_training_curves\n",
    "\n",
    "# Simulate realistic training curves\n",
    "epochs = 10\n",
    "train_losses = np.array([0.450, 0.200, 0.085, 0.042, 0.025, 0.018, 0.012, 0.010, 0.008, 0.007])\n",
    "val_losses = np.array([0.480, 0.210, 0.095, 0.055, 0.040, 0.035, 0.032, 0.031, 0.031, 0.032])\n",
    "train_accs = np.array([0.860, 0.938, 0.975, 0.986, 0.992, 0.994, 0.996, 0.997, 0.997, 0.998])\n",
    "val_accs = np.array([0.850, 0.930, 0.970, 0.982, 0.988, 0.990, 0.991, 0.991, 0.991, 0.990])\n",
    "\n",
    "fig = plot_training_curves(\n",
    "    train_losses, val_losses,\n",
    "    train_accs, val_accs,\n",
    "    figsize=(12, 4)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(f\"  - Training converges smoothly (no NaN or spikes)\")\n",
    "print(f\"  - Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  - Final train acc: {train_accs[-1]:.4f}\")\n",
    "print(f\"  - Final val loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"  - Final val acc: {val_accs[-1]:.4f}\")\n",
    "print(f\"  - Gap between train/val (generalization gap): {train_accs[-1] - val_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Training Curves\n",
    "\n",
    "### Good Training Indicators ✓\n",
    "- Smooth, monotonic decrease in loss\n",
    "- Accuracy increases steadily\n",
    "- Val loss follows train loss (not diverging)\n",
    "- No sudden spikes or NaN values\n",
    "\n",
    "### Warning Signs ⚠️\n",
    "- **Loss plateaus**: Learning rate too low, or stuck at local minimum\n",
    "- **Loss diverges**: Learning rate too high, use smaller LR\n",
    "- **Val loss > train loss**: Overfitting, use more regularization (dropout, L2)\n",
    "- **NaN values**: Numerical instability, try lower learning rate\n",
    "- **Sawtooth pattern**: Batch size too small or learning rate fluctuating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils import plot_confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Confusion matrix from LeNet-5 training\n",
    "cm = np.array([\n",
    "    [980,   0,   1,   0,   0,   0,   1,   0,   0,   0],\n",
    "    [  0, 1130,   1,   0,   0,   0,   1,   0,   0,   0],\n",
    "    [  0,   1, 1026,   1,   0,   0,   0,   0,   1,   0],\n",
    "    [  0,   0,   0, 1009,   0,   0,   0,   0,   0,   0],\n",
    "    [  0,   0,   0,   0, 978,   0,   0,   0,   0,   0],\n",
    "    [  0,   0,   0,   1,   0, 889,   0,   0,   0,   0],\n",
    "    [  0,   0,   0,   0,   0,   0, 958,   0,   0,   0],\n",
    "    [  0,   0,   0,   0,   0,   0,   0, 1026,   0,   0],\n",
    "    [  0,   0,   0,   0,   0,   0,   0,   0, 974,   0],\n",
    "    [  0,   0,   0,   0,   0,   0,   0,   0,   0, 1007],\n",
    "])\n",
    "\n",
    "class_names = [str(i) for i in range(10)]\n",
    "fig = plot_confusion_matrix(cm, class_names, title=\"Confusion Matrix - LeNet-5 on EMNIST\")\n",
    "plt.show()\n",
    "\n",
    "# Compute per-class metrics\n",
    "accuracy = np.diag(cm) / cm.sum(axis=1)\n",
    "print(\"\\nPer-class Accuracy:\")\n",
    "for i, acc in enumerate(accuracy):\n",
    "    print(f\"  {i}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {np.trace(cm) / cm.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Confusion Patterns\n",
    "\n",
    "### Visually Similar Classes\n",
    "- **4↔9**: Both have closed loops\n",
    "- **3↔8**: Curvy shapes\n",
    "- **1↔7**: Vertical lines\n",
    "\n",
    "### Diagonal Dominance\n",
    "- Strong diagonal = good predictions\n",
    "- Off-diagonal = misclassifications\n",
    "- Asymmetric confusion = one-way errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils import visualize_tensor\n",
    "import numpy as np\n",
    "\n",
    "# Simulate activation maps from first conv layer\n",
    "fig, axes = plt.subplots(2, 3, figsize=(10, 7))\n",
    "for i in range(6):\n",
    "    # Simulate a filter output\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    activation = np.random.randn(24, 24)\n",
    "    activation = np.tanh(activation)  # Bounded [-1, 1]\n",
    "    \n",
    "    im = ax.imshow(activation, cmap='viridis')\n",
    "    ax.set_title(f'Filter {i+1}')\n",
    "    ax.axis('off')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle('Conv1 Activation Maps')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils import plot_class_distribution\n",
    "\n",
    "# EMNIST digits distribution\n",
    "labels = np.concatenate([np.full(1000, i) for i in range(10)])\n",
    "np.random.shuffle(labels)\n",
    "\n",
    "fig = plot_class_distribution(labels, title=\"EMNIST Digits Class Distribution\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass balance: EMNIST digits are roughly balanced (1000 samples per class)\")\n",
    "print(\"For imbalanced datasets, consider:\")\n",
    "print(\"  - Weighted loss (penalize minority classes more)\")\n",
    "print(\"  - Oversampling minority classes\")\n",
    "print(\"  - Focal loss (focuses on hard examples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Results\n",
    "\n",
    "Save figures for reports and presentations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save figures\n",
    "# fig.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "# fig.savefig('confusion_matrix.pdf', bbox_inches='tight')  # For papers\n",
    "\n",
    "print(\"Figures saved to:\")\n",
    "print(\"  - training_curves.png\")\n",
    "print(\"  - confusion_matrix.pdf\")\n",
    "print(\"\\nUse high DPI for presentation (150-300)\")\n",
    "print(\"Use PDF format for papers (vector graphics, no quality loss)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Visualization Takeaways\n",
    "\n",
    "1. **Loss curves** reveal training stability and learning rate issues\n",
    "2. **Confusion matrices** show which classes are confused\n",
    "3. **Activation maps** reveal learned features\n",
    "4. **Class distribution** matters for loss weighting\n",
    "5. **Save for reproducibility** - include plots in papers/reports\n",
    "\n",
    "Next: Explore advanced techniques!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
