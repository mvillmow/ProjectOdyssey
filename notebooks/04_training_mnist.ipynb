{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LeNet-5 on EMNIST\n",
    "\n",
    "Complete training pipeline: data loading, model setup, training loop, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMNIST Dataset\n",
    "\n",
    "EMNIST is an extended version of MNIST:\n",
    "- **770,000 samples** of handwritten digits and letters\n",
    "- **62 classes**: 10 digits + 26 lowercase + 26 uppercase\n",
    "- **28×28 grayscale images**\n",
    "- Standard train/val/test split\n",
    "\n",
    "For this notebook, we'll use just the **digits subset** (10 classes, ~130k samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading\n",
    "\n",
    "In production Mojo code, you'd use the data loader. Here we show the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils import run_mojo_script\n",
    "import numpy as np\n",
    "\n",
    "# In practice, data loading happens in Mojo\n",
    "# For this demo, we create synthetic data\n",
    "print(\"Dataset will be loaded from Mojo training script\")\n",
    "print(\"Expected dimensions:\")\n",
    "print(\"  - Images: (N, 28, 28) for batch size N\")\n",
    "print(\"  - Labels: (N,) with values 0-9\")\n",
    "print(\"  - Batches: 32 samples each (configurable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Setup\n",
    "\n",
    "LeNet-5 configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model': 'LeNet-5',\n",
    "    'input_shape': (1, 28, 28),\n",
    "    'num_classes': 10,\n",
    "    'architecture': [\n",
    "        {'layer': 'Conv2D', 'in': 1, 'out': 6, 'kernel': 5},\n",
    "        {'layer': 'ReLU'},\n",
    "        {'layer': 'MaxPool2D', 'kernel': 2},\n",
    "        {'layer': 'Conv2D', 'in': 6, 'out': 16, 'kernel': 5},\n",
    "        {'layer': 'ReLU'},\n",
    "        {'layer': 'MaxPool2D', 'kernel': 2},\n",
    "        {'layer': 'Flatten'},\n",
    "        {'layer': 'Linear', 'in': 256, 'out': 120},\n",
    "        {'layer': 'ReLU'},\n",
    "        {'layer': 'Linear', 'in': 120, 'out': 84},\n",
    "        {'layer': 'ReLU'},\n",
    "        {'layer': 'Linear', 'in': 84, 'out': 10},\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Model: {config['model']}\")\n",
    "print(f\"  Input: {config['input_shape']}\")\n",
    "print(f\"  Output classes: {config['num_classes']}\")\n",
    "print(f\"  Total parameters: ~60k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = {\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'optimizer': 'Adam',\n",
    "    'loss': 'CrossEntropy',\n",
    "    'dtype': 'float32',\n",
    "    'device': 'CPU',  # or 'CUDA' if available\n",
    "    'checkpoint_dir': './checkpoints/',\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in train_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training Loop\n",
    "\n",
    "This runs in Mojo for performance. The training script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils import TrainingProgressBar\n",
    "import numpy as np\n",
    "\n",
    "# Simulate what the training loop does\n",
    "print(\"Training Loop Structure (in Mojo):\")\n",
    "print(\"\"\"\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    \n",
    "    for batch in train_loader:  # Iterate over batches\n",
    "        images = batch.images   # Shape: (batch_size, 1, 28, 28)\n",
    "        labels = batch.labels   # Shape: (batch_size,)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model.forward(images)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(predictions, labels)\n",
    "    \n",
    "    # Average over batches\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc = evaluate(model, val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss={train_loss:.4f}, Acc={train_acc:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    model.save(f'checkpoint_{epoch}.pt')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Running Training\n",
    "\n",
    "To train the actual model in Mojo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would run the actual training\n",
    "# Uncomment to train (takes ~5-10 minutes depending on hardware)\n",
    "\n",
    "# result = run_mojo_script(\n",
    "#     \"examples/lenet-emnist/run_train.mojo\",\n",
    "#     args=[\n",
    "#         \"--epochs\", \"10\",\n",
    "#         \"--batch-size\", \"32\",\n",
    "#         \"--lr\", \"0.001\",\n",
    "#         \"--precision\", \"float32\",\n",
    "#     ],\n",
    "#     timeout=600  # 10 minutes\n",
    "# )\n",
    "\n",
    "# if result['success']:\n",
    "#     print(result['stdout'])\n",
    "# else:\n",
    "#     print(f\"Training failed: {result['stderr']}\")\n",
    "\n",
    "print(\"Training would run here. See example output:\")\n",
    "print(\"\"\"\n",
    "Epoch 1/10\n",
    "  Batch 100/3600 - Loss: 2.2341 (3%)\n",
    "  Batch 200/3600 - Loss: 1.8234 (6%)\n",
    "  ...\n",
    "  ✓ Loss: 0.3234, Accuracy: 0.9823, Time: 45.2s\n",
    "\n",
    "Epoch 2/10\n",
    "  ✓ Loss: 0.0956, Accuracy: 0.9934, Time: 44.8s\n",
    "\n",
    "...\n",
    "\n",
    "Training complete!\n",
    "  Best accuracy: 0.9958 at epoch 8\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluation\n",
    "\n",
    "After training, evaluate on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.utils import plot_confusion_matrix\n",
    "\n",
    "# Simulate test results\n",
    "cm = np.array([\n",
    "    [980,   0,   1,   0,   0,   0,   1,   0,   0,   0],\n",
    "    [  0, 1130,   1,   0,   0,   0,   1,   0,   0,   0],\n",
    "    [  0,   1, 1026,   1,   0,   0,   0,   0,   1,   0],\n",
    "    [  0,   0,   0, 1009,   0,   0,   0,   0,   0,   0],\n",
    "    [  0,   0,   0,   0, 978,   0,   0,   0,   0,   0],\n",
    "    [  0,   0,   0,   1,   0, 889,   0,   0,   0,   0],\n",
    "    [  0,   0,   0,   0,   0,   0, 958,   0,   0,   0],\n",
    "    [  0,   0,   0,   0,   0,   0,   0, 1026,   0,   0],\n",
    "    [  0,   0,   0,   0,   0,   0,   0,   0, 974,   0],\n",
    "    [  0,   0,   0,   0,   0,   0,   0,   0,   0, 1007],\n",
    "])\n",
    "\n",
    "class_names = [str(i) for i in range(10)]\n",
    "fig = plot_confusion_matrix(cm, class_names, title=\"LeNet-5 Test Set Confusion Matrix\")\n",
    "print(f\"\\nTest Accuracy: {np.trace(cm) / cm.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Successful Training\n",
    "\n",
    "1. **Start simple**: Train for 1-2 epochs first to catch issues early\n",
    "2. **Monitor loss**: Loss should decrease smoothly\n",
    "3. **Watch for NaN**: Usually means learning rate too high\n",
    "4. **Use validation set**: Detect overfitting early\n",
    "5. **Save checkpoints**: Keep best model in case training fails\n",
    "6. **Try different seeds**: Results vary due to random initialization\n",
    "7. **Profile performance**: Identify bottlenecks\n",
    "\n",
    "Next: Visualize and analyze results!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
