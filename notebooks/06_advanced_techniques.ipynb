{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Techniques\n",
    "\n",
    "Mixed precision training, optimization, and performance techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Precision Training\n",
    "\n",
    "Use lower precision (FP16/BF16) for faster, more memory-efficient training while maintaining accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Mixed Precision?\n",
    "\n",
    "| Metric | FP32 | FP16 | BF16 | FP8 |\n",
    "|--------|------|------|------|-----|\n",
    "| Memory | 4B | 2B | 2B | 1B |\n",
    "| Speed | 1x | 2-3x | 2-3x | 4-8x |\n",
    "| Precision | Full | Half | Half | Very Low |\n",
    "| Training | ✓✓ | ✓ (w/ scaling) | ✓✓ | ✗ |\n",
    "| Inference | ✓ | ✓ | ✓ | ✓ |\n",
    "\n",
    "**Key Insight**: Use lower precision for weights/activations, higher for loss computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP16 Training Requirements\n",
    "\n",
    "To train with FP16 without numerical issues:\n",
    "\n",
    "1. **Loss Scaling**: Multiply loss by large scale factor to prevent underflow\n",
    "   ```mojo\n",
    "   scaled_loss = loss * 1024.0  # Scale up\n",
    "   scaled_loss.backward()        # Compute gradients with scaled loss\n",
    "   gradients = gradients / 1024.0 # Scale back down\n",
    "   ```\n",
    "\n",
    "2. **Gradient Clipping**: Prevent exploding gradients\n",
    "   ```mojo\n",
    "   for param in model.parameters:\n",
    "       norm = sqrt(sum(grad^2 for grad in param.grad))\n",
    "       if norm > max_norm:\n",
    "           param.grad = param.grad / norm * max_norm\n",
    "   ```\n",
    "\n",
    "3. **Keep master weights in FP32**: For accurate parameter updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation of mixed precision training\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training curves: FP32 vs FP16\n",
    "epochs = range(1, 11)\n",
    "fp32_loss = np.array([0.45, 0.20, 0.085, 0.042, 0.025, 0.018, 0.012, 0.010, 0.008, 0.007])\n",
    "fp16_loss = np.array([0.45, 0.20, 0.086, 0.043, 0.025, 0.018, 0.012, 0.010, 0.008, 0.007])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss comparison\n",
    "ax1.plot(epochs, fp32_loss, 'b-o', label='FP32', linewidth=2)\n",
    "ax1.plot(epochs, fp16_loss, 'r--s', label='FP16 (with loss scaling)', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('FP32 vs FP16 Training')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Training time comparison (simulated)\n",
    "precisions = ['FP32', 'FP16', 'BF16', 'FP8']\n",
    "times = [100, 35, 35, 18]  # Relative time\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "ax2.barh(precisions, times, color=colors, alpha=0.7)\n",
    "ax2.set_xlabel('Training Time (arbitrary units)')\n",
    "ax2.set_title('Training Speed by Precision')\n",
    "ax2.invert_yaxis()\n",
    "for i, (p, t) in enumerate(zip(precisions, times)):\n",
    "    ax2.text(t+2, i, f'{t/100:.1f}x', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Results:\")\n",
    "print(f\"  FP32 final loss: {fp32_loss[-1]:.4f}\")\n",
    "print(f\"  FP16 final loss: {fp16_loss[-1]:.4f}\")\n",
    "print(f\"  Difference: {abs(fp32_loss[-1] - fp16_loss[-1]):.6f}\")\n",
    "print(f\"\\n  FP16 training is ~3x faster with negligible accuracy loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling\n",
    "\n",
    "Adapt learning rate during training for better convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = np.arange(0, 100)\n",
    "\n",
    "# Different learning rate schedules\n",
    "lr_constant = np.ones_like(epochs) * 0.001\n",
    "lr_linear = 0.001 * (1 - epochs / 100)\n",
    "lr_exponential = 0.001 * np.exp(-epochs / 30)\n",
    "lr_cosine = 0.001 * (1 + np.cos(np.pi * epochs / 100)) / 2\n",
    "lr_step = 0.001 * (0.5 ** (epochs // 25))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(epochs, lr_constant, label='Constant', linewidth=2)\n",
    "ax.plot(epochs, lr_linear, label='Linear Decay', linewidth=2)\n",
    "ax.plot(epochs, lr_exponential, label='Exponential Decay', linewidth=2)\n",
    "ax.plot(epochs, lr_cosine, label='Cosine Annealing', linewidth=2)\n",
    "ax.plot(epochs, lr_step, label='Step Decay', linewidth=2, linestyle='--')\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedules')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Popular schedules:\")\n",
    "print(\"  - Step Decay: Reduce by factor every N epochs\")\n",
    "print(\"  - Cosine Annealing: Smooth decay following cosine curve\")\n",
    "print(\"  - Linear Decay: Linearly reduce from initial to final LR\")\n",
    "print(\"  - Exponential: Geometric decay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Algorithms\n",
    "\n",
    "Different optimizers for different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    \"SGD\": {\n",
    "        \"description\": \"Stochastic Gradient Descent\",\n",
    "        \"pros\": [\"Simple\", \"Generalizes well\"],\n",
    "        \"cons\": [\"Slow convergence\", \"Sensitive to LR\"],\n",
    "        \"best_for\": \"CNNs, strong baseline\",\n",
    "    },\n",
    "    \"SGD+Momentum\": {\n",
    "        \"description\": \"SGD with momentum term\",\n",
    "        \"pros\": [\"Faster convergence\", \"Less sensitive to LR\"],\n",
    "        \"cons\": [\"One extra hyperparameter (β)\"],\n",
    "        \"best_for\": \"Most training tasks\",\n",
    "    },\n",
    "    \"Adam\": {\n",
    "        \"description\": \"Adaptive Moment Estimation\",\n",
    "        \"pros\": [\"Adaptive per-parameter LR\", \"Few hyperparameters\"],\n",
    "        \"cons\": [\"May not generalize as well\", \"High memory\"],\n",
    "        \"best_for\": \"Transformers, NLP\",\n",
    "    },\n",
    "    \"AdamW\": {\n",
    "        \"description\": \"Adam with decoupled weight decay\",\n",
    "        \"pros\": [\"Better regularization\", \"SOTA for transformers\"],\n",
    "        \"cons\": [\"Newer, less widely tested\"],\n",
    "        \"best_for\": \"Vision Transformers, modern models\",\n",
    "    },\n",
    "}\n",
    "\n",
    "for name, info in optimizers.items():\n",
    "    print(f\"\\n{name}: {info['description']}\")\n",
    "    print(f\"  Pros: {', '.join(info['pros'])}\")\n",
    "    print(f\"  Cons: {', '.join(info['cons'])}\")\n",
    "    print(f\"  Best for: {info['best_for']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Simulate overfitting with different regularization\n",
    "epochs = np.arange(1, 21)\n",
    "\n",
    "# No regularization - severe overfitting\n",
    "train_loss_no_reg = 1.0 / (1 + 2*epochs**0.5)\n",
    "val_loss_no_reg = 1.0 / (1 + epochs**0.5) + 0.1 * epochs**0.5\n",
    "\n",
    "# L2 regularization - moderate regularization\n",
    "train_loss_l2 = 1.0 / (1 + 1.8*epochs**0.5)\n",
    "val_loss_l2 = 1.0 / (1 + 1.8*epochs**0.5) + 0.02 * epochs**0.3\n",
    "\n",
    "# Dropout - good regularization\n",
    "train_loss_dropout = 1.0 / (1 + 1.7*epochs**0.5)\n",
    "val_loss_dropout = 1.0 / (1 + 1.7*epochs**0.5) + 0.01 * epochs**0.2\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "configs = [\n",
    "    (\"No Regularization\", train_loss_no_reg, val_loss_no_reg, axes[0]),\n",
    "    (\"L2 Regularization\", train_loss_l2, val_loss_l2, axes[1]),\n",
    "    (\"Dropout\", train_loss_dropout, val_loss_dropout, axes[2]),\n",
    "]\n",
    "\n",
    "for title, train, val, ax in configs:\n",
    "    ax.plot(epochs, train, 'b-o', label='Train', linewidth=2)\n",
    "    ax.plot(epochs, val, 'r-s', label='Val', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0, 1.2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Regularization Impact:\")\n",
    "print(f\"  No Reg - Train/Val gap at epoch 20: {(val_loss_no_reg[-1] - train_loss_no_reg[-1]):.3f}\")\n",
    "print(f\"  L2 Reg - Train/Val gap at epoch 20: {(val_loss_l2[-1] - train_loss_l2[-1]):.3f}\")\n",
    "print(f\"  Dropout - Train/Val gap at epoch 20: {(val_loss_dropout[-1] - train_loss_dropout[-1]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization for Inference\n",
    "\n",
    "Reduce model size for deployment using quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_comparison = {\n",
    "    \"FP32 (baseline)\": {\n",
    "        \"model_size\": 100,  # Baseline\n",
    "        \"inference_speed\": 100,\n",
    "        \"accuracy\": 100,\n",
    "    },\n",
    "    \"FP16\": {\n",
    "        \"model_size\": 50,\n",
    "        \"inference_speed\": 200,\n",
    "        \"accuracy\": 99.9,\n",
    "    },\n",
    "    \"INT8\": {\n",
    "        \"model_size\": 25,\n",
    "        \"inference_speed\": 400,\n",
    "        \"accuracy\": 99.0,\n",
    "    },\n",
    "    \"INT4\": {\n",
    "        \"model_size\": 12,\n",
    "        \"inference_speed\": 800,\n",
    "        \"accuracy\": 98.0,\n",
    "    },\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "keys = list(quantization_comparison.keys())\n",
    "sizes = [quantization_comparison[k][\"model_size\"] for k in keys]\n",
    "speeds = [quantization_comparison[k][\"inference_speed\"] for k in keys]\n",
    "accs = [quantization_comparison[k][\"accuracy\"] for k in keys]\n",
    "\n",
    "axes[0].bar(keys, sizes, color='steelblue', alpha=0.7)\n",
    "axes[0].set_ylabel('Model Size (relative)')\n",
    "axes[0].set_title('Model Size by Quantization')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1].bar(keys, speeds, color='orange', alpha=0.7)\n",
    "axes[1].set_ylabel('Inference Speed (relative)')\n",
    "axes[1].set_title('Inference Speed by Quantization')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[2].plot(keys, accs, 'go-', linewidth=2, markersize=8)\n",
    "axes[2].set_ylabel('Accuracy (%)')\n",
    "axes[2].set_title('Accuracy vs Quantization')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].set_ylim([97, 100.5])\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nQuantization Strategy:\")\n",
    "print(\"  1. Train in FP32 with full precision\")\n",
    "print(\"  2. Evaluate accuracy at different bit widths\")\n",
    "print(\"  3. Use INT8 for 4x speedup with <1% accuracy loss\")\n",
    "print(\"  4. Use INT4 for extreme compression (edge devices)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Simulate performance profiling\n",
    "print(\"Performance Profiling Example:\\n\")\n",
    "\n",
    "operations = [\n",
    "    {\"name\": \"Conv2D 3×3\", \"time_ms\": 15.2, \"memory_mb\": 32.5},\n",
    "    {\"name\": \"Linear (256→120)\", \"time_ms\": 3.1, \"memory_mb\": 2.1},\n",
    "    {\"name\": \"BatchNorm\", \"time_ms\": 2.8, \"memory_mb\": 1.5},\n",
    "    {\"name\": \"ReLU\", \"time_ms\": 1.2, \"memory_mb\": 0.8},\n",
    "    {\"name\": \"MaxPool\", \"time_ms\": 0.9, \"memory_mb\": 0.5},\n",
    "]\n",
    "\n",
    "total_time = sum(op[\"time_ms\"] for op in operations)\n",
    "total_memory = sum(op[\"memory_mb\"] for op in operations)\n",
    "\n",
    "print(f\"{'Operation':<20} {'Time (ms)':<12} {'Memory (MB)':<15} {'% Time':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for op in operations:\n",
    "    pct = (op[\"time_ms\"] / total_time) * 100\n",
    "    print(f\"{op['name']:<20} {op['time_ms']:<12.1f} {op['memory_mb']:<15.1f} {pct:<10.1f}%\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Total':<20} {total_time:<12.1f} {total_memory:<15.1f} {'100.0%':<10}\")\n",
    "\n",
    "print(f\"\\nConclusions:\")\n",
    "print(f\"  - Conv layers are bottleneck (80% of time)\")\n",
    "print(f\"  - Consider dilated or depthwise convolutions\")\n",
    "print(f\"  - Activations/pooling are cheap (<5% each)\")\n",
    "print(f\"  - Memory dominated by activations, not parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "1. **Mixed Precision**: Use FP16 for 2-3x speedup with minimal accuracy loss\n",
    "2. **Learning Rate Scheduling**: Cosine annealing works well for most tasks\n",
    "3. **Optimizer Choice**: SGD+Momentum for vision, Adam for transformers\n",
    "4. **Regularization**: Dropout + L2 are effective and complementary\n",
    "5. **Quantization**: INT8 offers 4x speedup for inference\n",
    "6. **Profiling**: Find bottlenecks before optimizing\n",
    "\n",
    "These techniques are production-ready in ML Odyssey!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
